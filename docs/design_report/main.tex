\documentclass[twoside]{report}

% some definitions for the title page
\newcommand{\reporttitle}{Automated Theorem Proving in Dependent Type Theory}
\newcommand{\reportauthor}{Zhanrong Qiao}
\newcommand{\supervisor}{Alyssa Renata}
\newcommand{\secondmarker}{Prof. Alessio Lomuscio}
\newcommand{\reporttype}{MEng Individual Project Report}
\newcommand{\degreetype}{MEng Joint Mathematics and Computer Science} 

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

% load title page
\begin{document}
\input{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Due to their expressivity and the Curry-Howard correspondence with functional programs, dependent type theories have become the logical bases of many modern interactive theorem provers. However, such expressivity comes with complications in proof automation. For decades, efficient proof systems aimed at automation, in particular the connection method, have only been proposed and studied for simpler, proof-irrelevant, and mostly first-order logics. We demonstrated its generalisation to a fragment of the extended calculus of constructions (ECC), thereby moving towards a class of sound, complete and efficient proof search methods in the dependent type theory.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Acknowledgments}
\label{sec:acknowledgements}
\thispagestyle{empty}

First of all, I would like to thank my supervisor, Alyssa Renata, for encouraging me to pursue a topic I was interested in, for many helpful and pleasant discussions on both the overalls and specifics, for the patience to read my draft reports filled with uninteresting technical details and elementary proof methods and then still writing feedbacks, and for not disappointing when the project I proposed turned out to be scarier than I expected due to the amount of details. Without you, I would not have enough confidence in myself to complete the project.

I would also like to thank Dr. Nicolas Wu for the arrangement of this project during sabbatical, without whom the project would not have been possible.

I appreciate the initial feedback when I posted some my random thoughts to the Lean community, and the people who have informed me of the current state of machine learning provers, including Huajian Xin, and Yang Liu who has also expressed interest in further developments on this project. I would never forget Prof. Kevin Buzzard for the Xena evenings where I had a fun time learning Lean and its type theory in Year 1, and Dr. Mario Carneiro for not turning me down when I was posting trivial questions and ideas on Lean Zulip chat at that time.

On the other hand, I am indebted to my parents who have been providing financial support for my study at Imperial, as well as giving me life advices from time to time. I am grateful to my friends for their sincere emotional support: Xinyi Yang, Yunke Lin, Chengzhi Ye... You always made me feel better when I was in a low mood. I wish I could always do the same.

\vspace{1em}

My idea of making a theorem prover traces back to high school, where there was significant peer-pressure \emph{(exceeding that of Imperial)}, and me taking part in competitive programming contests for a preferential admission into top universities only made it worse. I do not feel particularly grateful for this, but being judged almost purely by my intellectual performance in solving peculiar algorithmic puzzles for a long time had made me determined to write a general-purpose program that can solve them all.\footnote{That is, if they are actually solvable, and if there is enough time and memory.} If I am not good at solving them, let my AI do it! As it seems, the first step was to make it capable of logical reasoning and mathematics, so I started learning formal logic...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% page numbering etc.
\pagenumbering{arabic}
\setcounter{page}{0}

%--- table of contents
\doublespacing
\tableofcontents
\singlespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{sec:introduction}

Historically, the mechanisation of logic progressed side-by-side with the development of computers. Nowadays, \emph{theorem provers}, or computer programs which produce and check formal proofs, have become valuable tools in various domains. These include education, software and hardware verification, and the validation of mathematical research.

There is no single definition for \emph{logic}: every theorem prover can have its own set of axioms and inference rules. In deciding which rules should be accepted, we face a trade-off between \emph{expressivity} and \emph{simplicity}. An expressive logic has more complex or intertwined rules, so that concepts can be formalised succinctly \emph{(better inside)}. A less expressive logic can have simpler rules, therefore being more amenable to proof automation \emph{(better outside)}. Such a trade-off divided theorem prover research and implementations into two lines:

\begin{itemize}
    \item \textbf{Interactive theorem provers (ITPs)}: also known as \emph{proof assistants}, ITPs like Lean \cite{moura2021lean}, Coq \cite{coq2024proof} and Agda \cite{norell2007towards} emphasise on user interaction during the construction of proofs. They typically employ expressive logics, like flavours of \emph{dependent type theories}, in order to capture `all mathematics' with proper levels of abstractions.

    \item \textbf{Automated theorem provers (ATPs)}: ATPs like E \cite{schulz2019faster}, Vampire \cite{kovacs2013first} and Z3 \cite{de2008proofs} focus on fully-automated proof search. They operate on simpler logics, like \emph{first-order predicate logics}, but have highly optimised inference rules and heuristic search procedures that are able to prove complex propositions without any user intervention.
\end{itemize}

\emph{Can we have the best from both worlds?} Existing works have investigated the integration of ATPs into ITPs in the form of \emph{hammers} \cite{bohme2010sledgehammer}. Hammers are tools that can be called from an ITP, which translate theorem statements into simpler logics and invoke external ATPs to attempt their proof. However, due to the mismatch in expressivity, such translations are usually lossy and therefore one-way, requiring a separate \emph{proof reconstruction} step from the ITP side, which involves search that may succeed or fail independently of the ATP \cite{paulsson2012three}. They may also sacrifice \emph{soundness} or \emph{completeness}, meaning that ATP invocations may give false positives, or that certain proofs cannot be found regardless of how much resource is put into the search.

This project takes another approach: to generalise the optimised inference rules used by ATPs to expressive logics employed by ITPs. Specifically, we will start with a dependent type theory, and find a set of inference rules that is \emph{sound, complete and efficient for proof search}. The finished derivations will be translated into readily checkable proof terms. To demonstrate its feasibility, we created Zenith, a prototype ITP kernel, with an interface that presents the efficient inference rules for use by some ATP search strategy. The core principles should be ultimately generalisable to mainstream ITPs, with possible features and extensions to be added in the future.

Finally, it worth mentioning that recent advancements in \emph{large language models} (LLMs) have expanded the potential for proof automation in ITPs \cite{yang2024formal}. A notable example is AlphaProof\footnote{\url{https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/}}, which successfully solved three of the six problems in the 2024 International Mathematical Olympiad by having an LLM, Gemini \cite{team2024gemini}, interact with Lean using \emph{Monte Carlo tree search} (MCTS). In contrast to \emph{hammers}, LLM provers directly search for proofs in ITPs. We argue that our approach equally benefits LLM provers by providing a \emph{streamlined action space}: for every logical inference step, our proof systems represent it as a single action instead of multiple syntactic tokens, reducing the amount of redundancy in the search tree.

% However, current machine learning provers work by directly interacting with the user interface of ITPs. The interfaces are primarily designed for humans, only later refitted for machines \cite{aniva2024pantograph}. At such, they are typically both powerful and complex, and can sometimes have rough edges or quirks that require domain expertise to navigate around easily. This makes them clearly unsuitable for simple proof search procedures, and likely increases the computational demand of the training process of machine learning models.

\section{Interactive theorem provers}
\label{sec:introduction_itp}

ITPs are like compilers. Typically, they take user proofs written in some \emph{surface language}, and transform them into proofs written in some \emph{core language}. Any unfinished parts of the proofs are reported back to the user, in the form of diagnostic information. If there are none left, they validate if the core proofs are correct. \Cref{fig:introduction_components_of_itp} gives an overview of this process.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \path (0, 0) node[align = center, fill = white](user) {User}
    (0, -3) node[draw, thick, rectangle, align = center, fill = white](elab) {Elaborator (proof compiler)}
    (0, -6) node[draw, thick, rectangle, align = center, fill = white](kernel) {Kernel (proof checker)};
    \draw[-latex, double] (user) to[bend right] node[align = left, left, xshift = -1em] {
        Proofs in surface language \\[0.3em]
        \texttt{\textcolor{blue}{example} (P : \textcolor{blue}{Prop}) :\ P $\rightarrow$ P :=} \\
        \texttt{\textcolor{blue}{by} \textcolor{blue}{intro} h; \underline{\textcolor{blue}{exact} h;}}
    } (elab);
    \draw[-latex, double] (elab) to[bend right] node[align = left, right, xshift = 1em] {
        Proof states (diagnostic information) \\[0.3em]
        \texttt{P :\ \textcolor{blue}{Prop}} \\
        \texttt{h :\ P} \\
        \texttt{\textcolor{blue}{$\vdash$} P}
    } (user);
    \draw[-latex] (elab) to[bend right] node[align = left, left, xshift = -1em] {
        Finished proofs in core language \\[0.3em] $(\lam P {(\lam h h)}) : (\ppi P {\mathcal U} {(\ppi h P P)})$
    } (kernel);
    \draw[-latex] (kernel) to[bend right] node[align = left, right, xshift = 1em] {
        Proof correct?
    } (elab);
    % For alignment
    \path (-7, 0) -- (7, 0);
    \end{tikzpicture}
\caption{Main components of an ITP}
\label{fig:introduction_components_of_itp}
\end{figure}

\subsection*{Core languages}

The core language is the logic of the ITP. It is usually clearly defined and proved to be relatively consistent with mainstream foundations of mathematics, so people can \emph{trust} a core proof once it passes the validation by the ITP.

\begin{example}
Lean \cite{moura2021lean}, Coq \cite{coq2024proof} and Agda \cite{norell2007towards} are three mainstream ITPs whose core languages are all flavours of dependent type theories. They all originate from \emph{Martin-LÃ¶f's intuitionistic type theory} (MLTT) \cite{martin1984intuitionistic} and its variants \cite{coquand1988calculus,luo1990extended}. Such languages are able to represent \emph{functions} and their \emph{types}, which are collectively called \emph{terms}. Moreover, a proof of some proposition is considered syntactically identical to a function of some type\footnote{This syntactical identification is known as the Curry-Howard correspondence.}, so proofs and propositions can be represented as well. Under such an identification, the typing rules for functions double as \emph{natural deduction} inference rules.

An example proof of the theorem, `any proposition $P$ implies $P$ itself', may have its core language term presented as:
$$\underbrace{(\lam P {(\lam h h)})}_{\text{function (proof)}} \quad : \quad \underbrace{(\ppi P {\mathcal U} {(\ppi h P P)})}_{\text{type (proposition)}}$$
Here, the form $(\lam x a)$ denotes a function that takes an argument $x$ and returns the term $a$. The form $(\ppi x A B)$ denotes the type of a function, which takes an argument $x$ of type $A$ and returns some term of type $B$. The symbol $\mathcal U$ denotes the type of propositions. The proof may be read as: `take any proposition $P$, take any evidence $h$ of $P$, we can use $h$ to prove $P$ itself'.
\end{example}

\subsection*{Surface languages}

The surface language is part of the interface between the user and the ITP. In general, there are three kinds of interface designs, in the order of increasing complexity:

\begin{itemize}
    \item \textbf{Term input.} The user directly writes down representations of core language terms and lets the system check them.

    \begin{example}
    To prove the example theorem in Lean, we may write
    $$\texttt{\textcolor{blue}{example} := (\textcolor{blue}{fun} P h $\mapsto$ h) :\ ((P : \textcolor{blue}{Prop}) $\rightarrow$ P $\rightarrow$ P);}$$
    Here, $\texttt{\textcolor{blue}{fun}}$ starts a function, and \texttt{\textcolor{blue}{Prop}} denotes the type of propositions. Note its similarity with the core language term given above.
    \end{example}
    
    Sometimes, parts of the term can be omitted, either explicitly or implicitly via the use of \emph{implicit arguments}. These omitted parts are called \emph{holes} (also known as \emph{goals}, \emph{metavariables}, \emph{existential variables}\footnote{\url{https://coq.inria.fr/doc/V8.18.0/refman/language/extensions/evars.html}}), and are inferred by the system through a number of mechanisms, including heuristic solutions to higher-order unification, and typeclass inference \cite{de2015lean,mahboubi2013canonical}.

    \begin{example}
    In Lean, the addition operator $+$ is polymorphic over types that implement the typeclass \texttt{Add}. Its full type is $$\texttt{Add.add} : (\ppi A {\mathcal U_i} {\ppi t {(\app {\texttt{Add}} A)} {\ppi l A {\ppi r A A}}})$$ which is a function type with four arguments. When we write $(1 + 2)$ or $(\app {\app {\texttt{Add.add}} 1} 2)$ in Lean, it really means $(\app {\app {\app {\app {\texttt{Add.add}} {?a}} {?t}} 1} 2)$, where the first two implicit arguments are filled by the holes ${?a}$ and ${?t}$ with types ${\mathcal U_i}$ and $(\app {\texttt{Add}} {?a})$ respectively. Both holes can be inferred:
    \begin{itemize}
        \item Since $1, 2$ have type \texttt{Nat} but are expected to have type ${?a}$, Lean infers that ${?a} \defeq \texttt{Nat}$.
        \item Since $t$ has type $(\app {\texttt{Add}} {\texttt{Nat}})$ and there is some term \texttt{Nat.instAdd} in the library \emph{(let's say)} with the same type $(\app {\texttt{Add}} {\texttt{Nat}})$, Lean infers that ${?t} \defeq \texttt{Nat.instAdd}$.
    \end{itemize}
    \end{example}
    
    \item \textbf{Commands.} When some holes cannot be inferred by the system, they will be reported back to the user, who can then fill them step-by-step using a pre-defined set of commands. This allows proofs to be carried out interactively.

    \begin{example}
    The Emacs interface of Agda \cite{coquand2006emacs} supports the \emph{refine} command, which fills the hole with a single constructor, function or theorem variable, creating new holes for its arguments or premises if any.

    To prove the example theorem in Agda, we can start by writing:
    $$
    \begin{aligned}
    &\texttt{example :\ $\forall$ (P :\ \textcolor{blue}{Set}) $\rightarrow$ P $\rightarrow$ P} \\
    &\texttt{example P = \{! !\}}        
    \end{aligned}
    $$
    where \texttt{\{! !\}} introduces a hole ${?a}$ with type $(P \rightarrow P)$. After invoking \emph{refine}, the second line becomes
    $$\texttt{example P = $\lambda$ x $\rightarrow$ \{! !\}}$$
    where the hole ${?a}$ has been filled with $(\lam x {?b})$, and the \texttt{\{! !\}} denotes the new hole ${?b}$ with type $P$. After filling in \texttt{x} and invoking \emph{refine} again, we finish the proof:
    $$\texttt{example P = $\lambda$ x $\rightarrow$ x}$$
    \end{example}
    
    \item \textbf{Tactics.} In more advanced systems, the set of commands of filling holes can be extended by custom procedures called \emph{tactics}.

    \begin{example}
    To prove the example theorem using Lean tactics, we write:
    $$\texttt{\textcolor{blue}{example} (P : \textcolor{blue}{Prop}) :\ P $\rightarrow$ P := \textcolor{blue}{by} \textcolor{blue}{intro} h; \textcolor{blue}{exact} h;}$$
    The keyword \texttt{\textcolor{blue}{by}} starts the proof by tactics. The tactic \texttt{\textcolor{blue}{intro} h} \emph{introduces} an assumption in the name \texttt{h} (which is an evidence for the inferred proposition \texttt{P}), while \texttt{\textcolor{blue}{exact} h} uses \emph{exactly} the assumption \texttt{h} to prove the goal (the proposition \texttt{P}). Or, under the hood:
    \begin{itemize}
        \item The keyword \texttt{\textcolor{blue}{by}} creates a new hole ${?a}$ with type $(P \rightarrow P)$.
        \item The tactic \texttt{\textcolor{blue}{intro} h} then fills ${?a}$ by $(\lam h {?b})$, creating a new hole ${?b}$ with type $P$.
        \item The tactic \texttt{\textcolor{blue}{exact} h} then fills ${?b}$ by $h$.
    \end{itemize}
    When combined, they construct the proof term $(\lam h h)$.

    Lean allows writing code for tactics in its own surface language. These tactics can be as simple as the \texttt{\textcolor{blue}{intro}} and \texttt{\textcolor{blue}{exact}} in the example above, or as complex as proof automation procedures \cite{clune2024duper}.
    \end{example}
\end{itemize}

\subsection*{Proof states}

ITPs can provide diagnostic information, known as \emph{proof states} or \emph{tactic states}, for unfinished parts of a proof. Such feedbacks help the user to construct a proof step-by-step.

\begin{example}
In the Lean tactic proof, right after the keyword \texttt{\textcolor{blue}{by}} is entered, Lean displays the following diagnostic:
$$
\begin{aligned}
&\texttt{P :\ \textcolor{blue}{Prop}} \\
&\texttt{\textcolor{blue}{$\vdash$} P $\rightarrow$ P}
\end{aligned}
$$
where the symbol \textcolor{blue}{$\vdash$} indicates the \emph{goal} proposition (the type of the hole) that we need to prove, and all previous lines list the \emph{assumption} propositions that we can use. This list is also called the \emph{context}. After the first tactic, \texttt{\textcolor{blue}{intro} h}, is entered, the diagnostic updates to include a new assumption $\texttt{h}$:
$$
\begin{aligned}
&\texttt{P :\ \textcolor{blue}{Prop}} \\
&\texttt{h :\ P} \\
&\texttt{\textcolor{blue}{$\vdash$} P}
\end{aligned}
$$
and after the second tactic, \texttt{\textcolor{blue}{exact} h}, is entered:
$$
\texttt{\textcolor{blue}{No goals}}
$$
which means the proof has been finished and validated by the kernel.
\end{example}

If we consider the diagnostics as \emph{states}, and the tokens in tactics as \emph{transitions} or \emph{actions}, this interactive process can be viewed as a \emph{transition system}, a particular case of \emph{Markov decision processes} (MDPs), which can be solved by \emph{reinforcement learning} (RL) methods. However, working with such an MDP is not the most efficient, which we will discuss in \cref{sec:introduction_ltp}.

% Although the ability to write tactics without having to modify the ITP's own code is important when large portions of proofs can be automated in specific ways, such cases are less common in most parts of pure mathematics, where pen-and-paper proofs are prevalent \emph{(which obviously employ little automation)}. Significant formalisation projects \cite{buzzard2020formalising} have been carried out in Lean, using only the standard set of tactics in its mathematical library\footnote{\url{https://github.com/leanprover-community/mathlib4/}}. Therefore, this project does not aim to create and \emph{support} a programming language for tactics.

\section{Automated theorem provers}
\label{sec:introduction_atp}

ATPs are like path-finding algorithms, except that they work on an implicit graph with infinitely many nodes. They take propositions written in some \emph{specification language}, possibly transform them into a different \emph{internal state}, and then perform brute-force or heuristic search, until either a proof is found, or a time/memory limit is reached. \Cref{fig:introduction_components_of_atp} gives an overview of this process. Notice that the upper halves of \cref{fig:introduction_components_of_itp,fig:introduction_components_of_atp} are somewhat similar.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \path (0, 0) node[draw, thick, rectangle, align = center, fill = white](strat) {Search strategy (path finder)}
    (0, -3) node[draw, thick, rectangle, align = center, fill = white](rules) {Proof system (implicit graph)}
    (0, -6) node[align = center, fill = white](user) {User};
    \draw[-latex] (user) to[bend right] node[align = left, right, xshift = 1em] {
        Problems in specification language \\[0.3em]
        \texttt{\textcolor{blue}{fof}(id, \textcolor{blue}{conjecture}, (p => p))}
    } (rules);
    \draw[-latex] (rules) to[bend right] node[align = left, left, xshift = -1em] {
        Proof found?
    } (user);
    \draw[-latex, double] (rules) to[bend right] node[align = left, right, xshift = 1em] {
        Internal states \\[0.3em]
        \texttt{\textcolor{blue}{refute cnf}} $\{\{p\}, \{\lnot p\}\}$
    } (strat);
    \draw[-latex, double] (strat) to[bend right] node[align = left, left, xshift = -1em] {
        Selected inference rules \\[0.3em]
        \texttt{\textcolor{blue}{resolution on}} $\{p\}$, $\{\lnot p\}$
    } (rules);
    % For alignment
    \path (-7, 0) -- (7, 0);
    \end{tikzpicture}
\caption{Main components of an ATP}
\label{fig:introduction_components_of_atp}
\end{figure}

\subsection*{Specification languages}

The specification language is the interface between the user and the ATP. Nowadays, it is highly standardised across different ATPs:

\begin{example}
The \emph{TPTP (Thousands of Problems for Theorem Provers)} \cite{sutcliffe2017tptp} dataset standardises a set of specification languages, which represent propositions from first-order logics to higher-order logics (also known as \emph{simple type theories}). An example first-order problem looks like:
$$
\begin{aligned}
& \texttt{\textcolor{blue}{fof}(pel52\_1, \textcolor{blue}{axiom},} \\
& \qquad \texttt{?[Z, W]: ![X, Y]: (big\_f(X, Y) <=> (X = Z \& Y = W))).} \\[0.3em]
& \texttt{\textcolor{blue}{fof}(pel52, \textcolor{blue}{conjecture},} \\
& \qquad \texttt{?[W]: ![Y]: (?[Z]: ![X]: (big\_f(X, Y) <=> X = Z) <=> Y = W)).}
\end{aligned}
$$
where the symbols (\texttt{\&}) (\texttt{<=>}) (\texttt{!}) (\texttt{?}) denote the connectives $(\land)$ $(\leftrightarrow)$ and quantifiers $(\forall)$ $(\exists)$, respectively. The (\texttt{=}) is the usual equality relation, which is often built-in as a part of the first-order logic. The objective is to prove the \texttt{\textcolor{blue}{conjecture}} from the \texttt{\textcolor{blue}{axiom}}.
\end{example}

\subsection*{Proof systems}

The proof system is a formal system consisting of inference rules on the internal state. Its implementation is central to the ATP: not only does it define the logic, but also heavily affects the likelihood and performance in finding a proof. Based on the overall characteristics of the inference rules, we can roughly classify ATP proof systems into three families:

\begin{itemize}
    \item \textbf{Tableaux.} Also known as \emph{backward chaining}, the \emph{method of analytic tableaux} \cite{smullyan1968first} include inference rules that `break down' propositions in the problem statement, and is reminiscent of the command-style interfaces seen in ITPs. The \texttt{leanTAP} prover \cite{beckert1995leantap} implements the tableau method for classical first-order logic in just a few lines of Prolog code.
    
    However, breaking down a proposition can be a destructive action that makes the resulting state harder to prove or no longer provable. Therefore, this approach requires \emph{deep backtracking}, which leads to relatively poor performance and is nowadays more often seen in toy provers than in practical systems.

    The tableau method uses an internal state called a \emph{tableau}, which efficiently encodes leaf sequents of a sequent calculus proof. Its generalisation to dependent type theory will be investigated in \cref{sec:sequent_calculi,sec:the_tableau_system}.
    
    \item \textbf{Resolution.} Also known as \emph{forward chaining}, the \emph{generic resolution method} \cite{mints1988gentzen,tammet1997resolution} starts from atomic facts and derive whatever derivable until reaching the goal. It never makes worsening actions, but can generate a larger search space. % The \emph{sub-formula property} of cut-free sequent calculi says that it is enough to only try deriving sub-formulas of the propositions in the problem statement, which is crucial to keep the search space in control.
    In classical first-order logic, a special form called \emph{clausal resolution} \cite{robinson1965machine} is popular. This has become the basis of state-of-the-art extensions like \emph{paramodulation} \cite{robinson1983paramodulation} and \emph{superposition} \cite{bachmair1994rewrite}, which are optimised for proofs involving the equality predicate. The E \cite{schulz2019faster} and Vampire \cite{kovacs2013first} provers are both based on clausal resolution with superposition.
    
    Clausal resolution works on a particular internal state: a \emph{conjunctive normal form} (CNF). However, transformation into CNF is not valid for intuitionistic logics and dependent type theories, making it unsuitable as a candidate method for this project.
    
    % \begin{example}
    % The \emph{Z3 theorem prover} \cite{de2008proofs} is based on classical propositional logic. However, it is able to translate first-order problems about finite objects, including fixed-width integers and bit-vectors, into propositional problems and then search for a proof or disproof using the \emph{DPLL}-family of inference rules. Such provers are more suitable for domain-specific tasks and are known as \emph{satisfiability modulo theory} (SMT) solvers.
    % \end{example}
        
    \item \textbf{Connections.} The \emph{connection method} \cite{prawitz1968proof,andrews1981theorem,bibel1981matrices} runs in two phases. The matrix expansion phase breaks down propositions in the problem statement like the tableau method, but records all choices instead of taking one, so the situation does not become worse. The connection spanning phase then incrementally builds proofs from the broken-down propositions like the resolution method. It aims to overcome redundancies in both tableaux and resolution methods, and its non-classical variants \cite{wallen1987automated} are popular in state-of-the-art ATPs for modal and intuitionistic logics, where clausal resolution does not apply in the first place. The \texttt{leanCoP} and \texttt{ileanCoP} provers \cite{otten2008leancop} implement the connection method for classical and intuitionistic first-order logics, respectively.
    
    The connection method utilises an internal state called a \emph{matrix}, which is even more efficient in encoding leaf sequents of a multi-succedent sequent calculus proof. Its generalisation to dependent type theory will be discussed in detail in \cref{sec:sequent_calculi,sec:the_connection_system}.
\end{itemize}

The implicit graph with internal states as nodes, inference rules as edges, is also a transition system and an MDP. However, they are not directly exposed to users or LLMs willing to interact with the proof search process, which may have led to their secondary role in recent LLM-based theorem proving research. Moreover, different ATPs can use different proof systems, meaning the proofs they produce are not necessarily accepted by each other and by ITPs.

\section{LLM-based theorem provers}
\label{sec:introduction_ltp}

LLMs can sit in the place of the user to ITPs, or act as intelligent search strategies for ATPs. Using RL methods, they learn to predict a score for each action from a given state in an MDP, collectively called a \emph{policy}, which then guides a search procedure, like beam search or MCTS, to maximise expected proof successful rates.

Since LLMs learn a policy, they are complementary to the formulation of MDPs themselves. As this projects focuses on the latter, we do not analyse in-depth the architectures of LLM-based provers; an overview can be found in \cite{yang2024formal}. This section mainly serves to explain how improving the formulation of the MDP might be beneficial for future research.

Most LLM-based provers interact with an ITP like Lean, which means they output scores for \emph{tokens}, roughly corresponding to `words' or `identifiers', that constitute a tactic invocation from a given proof state. We argue that this leads to additional overhead:

\begin{itemize}
    \item \textbf{Delayed feedbacks.} A typical ITP user interface only provides feedback after a tactic invocation has been written down in full, which is true even after they have been refitted for LLMs \cite{aniva2024pantograph}. As most of the token choices are dead ends leading to syntactic nonsense (\cref{fig:search_tree_wasted}), search procedures become less efficient and LLMs need more pre-training. A more streamlined formulation of the MDP (\cref{fig:search_tree_improved}) potentially allows for more efficient search, less pre-training and the use of weaker models.
    
    \begin{figure}[ht]
        \centering
        \begin{tikzpicture}[inter/.style={circle, inner sep = 0, minimum size = 4pt, fill}]
        \pgfmathsetmacro{\hs}{1.6}
        \pgfmathsetmacro{\vs}{1.2}
    
        \draw[]
            (0, 0) node[inter]{}
            -- node[auto]{\texttt{apply}}
            (\hs, 0) node[inter]{}
            -- node[auto]{\texttt{And}}
            (\hs * 2, 0) node[inter]{}
            -- node[auto]{`.'}
            (\hs * 3, 0) node[inter]{}
            -- node[auto]{\texttt{intro}}
            (\hs * 4, 0) node[inter]{}
            -- node[auto]{EOS}
            (\hs * 5, 0) node[]{};
    
        \draw[red]
            (0, 0)
            to[bend left] node[auto]{\texttt{intro}}
            (\hs * 2, \vs * 2) node[inter]{}
            -- node[auto]{$\ldots$}
            (\hs * 3, \vs * 2) node[inter]{}
            -- node[auto]{EOS}
            (\hs * 4, \vs * 2) node[]{$\times$};
    
        \draw[red]
            (\hs, 0)
            to[bend left] node[auto]{\texttt{Or}}
            (\hs * 2, \vs * 1) node[inter]{}
            -- node[auto]{$\ldots$}
            (\hs * 3, \vs * 1) node[inter]{}
            -- node[auto]{EOS}
            (\hs * 4, \vs * 1) node[]{$\times$};
    
        \draw[red]
            (\hs, 0)
            to[bend right] node[auto]{\texttt{induction}}
            (\hs * 2, -\vs * 1) node[inter]{}
            -- node[auto]{$\ldots$}
            (\hs * 3, -\vs * 1) node[inter]{}
            -- node[auto]{EOS}
            (\hs * 4, -\vs * 1) node[]{$\times$};
    
        \draw[red]
            (0, 0)
            to[bend right] node[auto]{`)'}
            (\hs * 2, -\vs * 2) node[inter]{}
            -- node[auto]{$\ldots$}
            (\hs * 3, -\vs * 2) node[inter]{}
            -- node[auto]{EOS}
            (\hs * 4, -\vs * 2) node[]{$\times$};
    
        \draw (0, 0) node[left, align = left, draw](prf_state){
            \ldots \\
            \texttt{P Q :\ Prop} \\
            \texttt{h :\ P $\land$ Q} \\
            \texttt{$\vdash$ Q $\land$ P}
        };
        \draw (prf_state.north) node[above]{Proof state};
    
        \draw (\hs * 5, 0) node[right, align = left, draw](prf_state_next){
            \ldots \\
            \texttt{P Q :\ Prop} \\
            \texttt{h :\ P $\land$ Q} \\
            \texttt{$\vdash$ Q} \\[1em]
            \ldots \\
            \texttt{P Q :\ Prop} \\
            \texttt{h :\ P $\land$ Q} \\
            \texttt{$\vdash$ P}
        };
        \draw (prf_state_next.north) node[above]{Next proof state};
        \end{tikzpicture}
    \caption{Dead ends in the search tree can waste LLM inferences $(\bullet)$. Delayed feedback means additional tokens $(\ldots)$ are produced before an error is caught $(\times)$.}
    \label{fig:search_tree_wasted}
    \end{figure}
    
    \begin{figure}[ht]
        \centering
        \begin{tikzpicture}[inter/.style={circle, inner sep = 0, minimum size = 4pt, fill}]
        \pgfmathsetmacro{\hs}{4.0}
        \pgfmathsetmacro{\vs}{2.0}
    
        \draw[]
            (0, 0) node[inter]{}
            -- node[auto, align = center]{Use $37$th entry:\\\texttt{And.intro \_ \_}}
            (\hs * 1, 0) node[inter, draw, fill = white]{}
            -- node[auto, align = center]{Automatic\\post-processing}
            (\hs * 2, 0) node[]{};
    
        \draw[red]
            (0, 0)
            to[bend left] node[auto, align = center]{Make function\\(type mismatch)}
            (\hs * 1, \vs * 1) node[]{$\times$};
    
        \draw[red]
            (0, 0)
            to[bend right] node[auto, align = center]{Use $42$nd entry:\\\texttt{Or.intro \_ \_}\\(head mismatch)}
            (\hs * 1, -\vs * 1) node[]{$\times$};
    
        \draw (0, 0) node[left, align = left, draw](prf_state){
            \ldots \\
            \texttt{P Q :\ Prop} \\
            \texttt{h :\ P $\land$ Q} \\
            \texttt{$\vdash$ Q $\land$ P}
        };
        \draw (prf_state.north) node[above]{Proof state};
    
        \draw (\hs * 2, 0) node[right, align = left, draw](prf_state_next){
            \ldots \\
            \texttt{P Q :\ Prop} \\
            \texttt{h :\ P $\land$ Q} \\
            \texttt{$\vdash$ Q} \\[1em]
            \ldots \\
            \texttt{P Q :\ Prop} \\
            \texttt{h :\ P $\land$ Q} \\
            \texttt{$\vdash$ P}
        };
        \draw (prf_state_next.north) node[above]{Next proof state};
        \end{tikzpicture}
    \caption{A more desirable search tree with minimal waste. LLM directly outputs action IDs. Every action has immediate feedback so obviously wrong/irrelevant choices are filtered out.}
    \label{fig:search_tree_improved}
    \end{figure}
    
    \item \textbf{Redundant actions.} ITPs often contain a large array of specialised tactics. They are an important feature, but can also contain redundancies. For example, to prove a conjunction in Lean, the tactic invocations \texttt{\textcolor{blue}{apply} And.intro}, \texttt{\textcolor{blue}{refine} $\langle$?\_, ?\_$\rangle$} and \texttt{\textcolor{blue}{constructor}} (with approximately 4, 6, 1 tokens each) have the same effect. Training data typically contains all styles, and LLMs will need more examples to learn their similarity. In contrast, an improved formulation of the MDP should not contain obviously redundant actions.
    
    \item \textbf{Weak relevance filtering.} ITPs often separated the list of available facts into two parts: the \emph{local context} and the \emph{top-level context}. The former contains the immediate premises of the theorem currently being proved. The latter contains all available facts from the imported libraries. By default, the user is only presented with the local context, and is expected to know which facts might be available in the top-level.
    
    However, LLM provers are typically not hinted about the current top-level context, so they can only rely on their trained memory to guess for it. Although some more recent works are towards solving such problem by supplying imported theorems in the same prompt \cite{hu2024minictx}, an in-depth analysis of the proof system gives more accurate criteria for the filtering of irrelevant facts, than simply presenting only the local context or all imported theorems. An improved formulation of the MDP may incorporate relevance filters, allowing exactly the relevant actions for each proof state (\cref{fig:search_tree_improved}).
\end{itemize}

We note that all three problems have been solved in ATPs. By formulating the MDP based on either the tableau or connection method, there is no need to tokenise since every step corresponds to a logical inference. They also contain little redundancy, especially for the connection method which was designed to eliminate redundancy in the first place. Finally, research efforts have been put into their efficient implementations, including efficient data structures for relevance filtering. It remains to adapt these ATP proof systems to the expressive logics of ITPs.

% We would also like to incorporate some of the advancements from decades of research on ATPs, for which \emph{action space pruning} has been an important objective. To my knowledge, optimisation techniques for first-order logics abound, some have been generalised to higher-order logics, but few has investigated their generalisation to dependent type theories.

\section{Report outline}
\label{sec:report_outline}

This project adapts the optimised ATP proof systems, namely the tableau and connection methods, to expressive ITP logics, namely a variant of dependent type theory. We are also working on a demo implementation, the Zenith theorem prover.\footnote{The code is available at \url{https://github.com/bridgekat/zenith/}.}

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \path (0, 0) node[align = center, fill = white](user) {External provers \\ (search procedures, LLMs, users)}
    (0, -3) node[draw, thick, rectangle, align = center, fill = white](elab) {Interface}
    (0, -6) node[draw, thick, rectangle, align = center, fill = white](kernel) {Kernel};
    \draw[-latex, double] (user) to[bend right] node[align = left, left, xshift = -1em] {Proof transitions} (elab);
    \draw[-latex, double] (elab) to[bend right] node[align = left, right, xshift = 1em] {
        Proof states \\[0.3em]
        Choices for next step
    } (user);
    \draw[-latex] (elab) to[bend right] node[align = left, left, xshift = -1em]
        {Finished proofs in the core language} (kernel);
    \draw[-latex] (kernel) to[bend right] node[align = left, right, xshift = 1em]
        {Proof correct?} (elab);
    % For alignment
    \path (-7, 0) -- (7, 0);
    \end{tikzpicture}
\caption{The components of this project}
\label{fig:introduction_components_of_zenith}
\end{figure}

\Cref{sec:logical_framework} introduces the dependent type theory $(\vdash)$, its meta-theoretic properties, and discusses the choices of primitive notions. This is a common fragment present in ITPs like Lean, Coq and Agda, and is expressive enough to frame a variety of logics and prove most theorems in undergraduate mathematics from a set of postulates.

\Cref{sec:kernel} gives an alternative formulation of the type theory $(\vdashd)$, which extends $(\vdash)$ by introducing let-expressions, and can be closer to the actual implementation of the kernel. Additional features implemented in the kernel will be discussed. This formulation will be used in the rest of the report.

\Cref{sec:ingredients} introduces the ideas of the sequent calculus and the matrix representation by an example first-order logic, with a discussion on the use of holes \emph{(placeholders for unknown subterms)} and unification in making informed choices for quantifier instantiations. The remaining part of this chapter conservatively extends our type theory $(\vdashd)$ with holes, and considers the problem of higher-order unification.

\Cref{sec:sequent_calculi} formulates two sequent calculi, a single-succedent $(\vdashf)$ and a multi-succedent $(\vdashs)$, for the purpose of proof search in $(\vdashd)$. The $(\vdashf)$-rules are the basis of the tableau method, while the $(\vdashs)$-rules are the basis of the connection method. Both include the common fragments, $(\preceq)$ and $(\cong)$, which handle type conversion via basic unification steps.

\Cref{sec:the_tableau_system} builds on the previous chapter to give a specification of the tableau method as a transition system. The transitions simultaneously handle proof search and unification search, producing only correct proofs in $(\vdashd)$, and can prove every proposition that is provable in $(\vdashd)$. The system roughly corresponds to a typical ITP user interface with a cut-down set of commands/tactics.

\Cref{sec:the_connection_system} builds on the previous chapters to give a specification of the connection method as a transition system. The system roughly corresponds to a hypothetical ITP user interface with a cut-down set of commands/tactics, where every command/tactic simultaneously applies to all applicable subgoals, leading to improved proof search efficiency in a number of ways.

\section{Related works}
\label{sec:related_works}

This project builds upon the frameworks of proof theory and ATP. We refer to \cite{harrison2009handbook,pfenning2004automated} for an introduction to first-order ATP methods, and \cite{bibel1983matings,waaler2001connections} for the connection method in particular.

The tableau method, as introduced in \cite{smullyan1968first,fitting1972tableau}, is a representation of sequent calculus proofs. The clausal resolution method was first introduced in \cite{robinson1965machine}, and was generalised in \cite{mints1988gentzen,tammet1997resolution} to non-clausal and non-classical logics. The connection method \cite{prawitz1968proof,andrews1981theorem,bibel1981matrices} appeared to be more interesting as it can be seen as a common ground for all three methods, and can simulate the others with less redundant steps \cite{bibel1982comparative}.

After \cite{wallen1987automated} generalised the connection method to intuitionistic and modal logics, \cite{waaler2001connections} gave a more succinct presentation using multi-succedent sequent calculi, \cite{schmitt1995transforming,schmitt1996converting} considered the problem of proof term assignment, and \cite{kreitz2000matrix} integrated the procedure to NuPRL. All of them still deal with first-order fragments.

For a long time, there has been relatively little ATP research for dependent type theories. An exception is \cite{pym1990proofs}, which proposed several sequent calculi for proof search in dependent type theory, similar to our $(\vdashf)$ but without proof terms. A shorter version can be found in \cite{pym1990investigations}. As a necessary ingredient, \cite{elliott1990extensions} investigated the higher-order unification problem for dependent type theory. A shorter version can be found in \cite{elliott1989higher}. Finally, a sound and complete search procedure integrating higher-order unification was proposed in \cite{dowek1993complete}.

In recent years, there has been progress in generalising first-order ATP methods to full higher-order logics \emph{(simple type theories)}. Notably, \cite{bentkamp2021superposition,vukmirovic2021making} extended superposition to simple type theory, which has been implemented in the Zipperposition prover \cite{cruanes2014logtk}, as well as the Duper tactic of Lean \cite{clune2024duper}. Due to the use of clausal superposition, all of them are classical, and there is still a gap to dependent type theory that require special handling or translations. However, this approach efficiently handles equational rewriting, a feature this project still lacks of.

Even more recently, research works similar to this project began to emerge. The Aesop tactic of Lean \cite{limperg2023aesop} performs tableau-like proof search directly on top of dependent type theory, but requires manual annotations for its rules. The Canonical tactic of Lean \cite{norman2025canonical}, with its pre-print published in the same year as this project, implements tableau proof search in a fully automated manner. We note that the proof system employed by \cite{norman2025canonical} is functionally equivalent to our tableau system presented in \cref{sec:tableau_unification_search}.

\section{Contributions}
\label{sec:contributions}

Our main contribution is the `graceful' generalisations of both tableau and connection methods to a fragment of the dependent type theory ECC. To this end, we introduced:

\begin{itemize}
    \item In \cref{sec:kernel,sec:ingredients}, an extension to the type theory with both let-expressions and holes, where terms containing holes can be reduced and typed.

    \item In \cref{sec:sequent_calculi}, the sequent calculi $(\vdashf)$ and $(\vdashs)$ as the bases for the tableau and connection methods, and the translations between their derivations.

    \item In \cref{sec:the_tableau_system}, a presentation of the tableau method as a transition system, making it a suitable target for brute-force search procedures as well as reinforcement learning methods. We have simplified the tableau method by considering proof search and higher-order unification as two sides of a single semi-decidable problem.

    \item In \cref{sec:the_connection_system}, a presentation of the connection method as a transition system. We have similarly considered proof search and higher-order unification together, which are accommodated by exclusive and shared locks that control immediate and delayed term assignment.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The logical framework}
\label{sec:logical_framework}

Observing that mainstream ITPs are generally based on variants of dependent type theories, this project starts from their common ground. To keep discussions simple, the type theory we are concerned with will be much more bare-bones compared to mainstream ITPs. Due to the expressiveness of dependent types, many of the missing features can still be stated as postulates in it \emph{(i.e. parameters or premises that appear in the very beginning)}. For example, we do not support any kind of inductive constructions, but such can be encoded using the $\mathsf W$ type of well-founded trees \cite{hugunin2021not}, which can be stated, along with its constructors and eliminator, as postulates. However, its reduction rule can only be stated as a weaker, propositional equality postulate.\footnote{New reduction rules interact with every other part of this project, which may be considered for future work.}

\section{Dependent type theory}
\label{sec:type_theory_standard}

In this report, terms are considered equal iff they are $\alpha$-equivalent, and any two variables are by default assumed to have different names through implicit $\alpha$-renaming \emph{(Barendregt's convention)}. For any term $a$, let $(\free a)$ denote the set of its free variables. The notation $\subst x a b$ means replacing all occurrences of free variable $x$ in term $a$ by term $b$, in a capture-avoiding manner via implicit $\alpha$-renamings.

Our type theory includes $\Pi$ types, $\Sigma$ types and the unit type $\unit$. This system is a fragment of the \emph{extended calculus of constructions} (ECC) \cite{luo1990extended,norell2007towards} without the impredicative universe of propositions. The ECC is in turn an extension to the \emph{Edinburgh logical framework} (LF) \cite{harper1993framework}, which is already known to be expressive enough to easily frame a variety of structural logics \cite{harper1993framework,pym1990proofs,cousineau2007embedding}, and can serve as a basis for higher-level abstraction mechanisms \cite{pollack2000dependently,sozeau2008first}.

\begin{definition}[Syntax]
\label{def:syntax}
The sets of terms $a \termo$ and contexts $\Gamma \ctxo$ are inductively defined by the rules listed in \cref{fig:syntax}.
\end{definition}

\begin{definition}[Conversion rules]
\label{def:conversion}
The relations $(\rightsquigarrow)$ $(\equiv)$ $(\leq)$ between terms are inductively defined by the rules listed in \cref{fig:syntax}.
\end{definition}

\begin{definition}[Typing rules]
\label{def:typing}
For any $\Gamma \ctxo$ and $a, A \termo$, the relations $\Gamma \vdasho \ok$ and $\Gamma \vdasho a : A$ are inductively defined by the rules listed in \cref{fig:typing}.
\end{definition}

\begin{figure}
    $\boxed{a \termo,\ \Gamma \ctxo}$

    $$
    \begin{aligned}
    A,B,a,b ::=
        & \phantom{{}\mid{}} \mathcal U_{i \in \mathbb N} \mid x \mid a : A \\
        & \mid \ppi x A B \mid \lam x b \mid \app a b \\
        & \mid \sig x A B \mid \pair a b \mid \fst a \mid \snd a \\
        & \mid \unit \mid \sstar \\
    \Gamma ::=
        & \phantom{{}\mid{}} \epsilon \mid \Gamma, (x : A) \qquad (x \notin \free \Gamma) \\
    \end{aligned}
    $$

    \vspace{1em}
    $\boxed{a \rightsquigarrow b}$
    
    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\tau)$}\UnaryInfC{$a : A \rightsquigarrow a$}
    \DisplayProof
    \qquad
    \AxiomC{}
    \RightLabel{$(\beta)$}\UnaryInfC{$\app {(\lam x b)} a \rightsquigarrow \subst x a b$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\pi\mathsf{fst})$}\UnaryInfC{$\fst {\pair a b} \rightsquigarrow a$}
    \DisplayProof
    \qquad
    \AxiomC{}
    \RightLabel{$(\pi\mathsf{snd})$}\UnaryInfC{$\snd {\pair a b} \rightsquigarrow b$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}
    
    \vspace{1em}
    \begin{framed}
        $(\mathsf{cong})$
        
        \begin{prooftree}
        \AxiomC{$a \rightsquigarrow a'$}
        \UnaryInfC{$a : A \rightsquigarrow a' : A$}
        \DisplayProof
        \qquad
        \AxiomC{$A \rightsquigarrow A'$}
        \UnaryInfC{$a : A \rightsquigarrow a : A'$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
                
        \begin{prooftree}
        \AxiomC{$A \rightsquigarrow A'$}
        \UnaryInfC{$\ppi x A B \rightsquigarrow \ppi x {A'} B$}
        \DisplayProof
        \qquad
        \AxiomC{$B \rightsquigarrow B'$}
        \UnaryInfC{$\ppi x A B \rightsquigarrow \ppi x A {B'}$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
        
        \begin{prooftree}
        \AxiomC{$b \rightsquigarrow b'$}
        \UnaryInfC{$\lam x b \rightsquigarrow \lam x {b'}$}
        \DisplayProof
        \qquad
        \AxiomC{$a \rightsquigarrow a'$}
        \UnaryInfC{$\app a b \rightsquigarrow \app {a'} b$}
        \DisplayProof
        \qquad
        \AxiomC{$b \rightsquigarrow b'$}
        \UnaryInfC{$\app a b \rightsquigarrow \app a {b'}$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
        
        \begin{prooftree}
        \AxiomC{$A \rightsquigarrow A'$}
        \UnaryInfC{$\sig x A B \rightsquigarrow \sig x {A'} B$}
        \DisplayProof
        \qquad
        \AxiomC{$B \rightsquigarrow B'$}
        \UnaryInfC{$\sig x A B \rightsquigarrow \sig x A {B'}$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
        
        \begin{prooftree}
        \AxiomC{$a \rightsquigarrow a'$}
        \UnaryInfC{$\pair a b \rightsquigarrow \pair {a'} b$}
        \DisplayProof
        \qquad
        \AxiomC{$b \rightsquigarrow b'$}
        \UnaryInfC{$\pair a b \rightsquigarrow \pair a {b'}$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
        
        \begin{prooftree}
        \AxiomC{$a \rightsquigarrow a'$}
        \UnaryInfC{$\fst a \rightsquigarrow \fst {a'}$}
        \DisplayProof
        \qquad
        \AxiomC{$a \rightsquigarrow a'$}
        \UnaryInfC{$\snd a \rightsquigarrow \snd {a'}$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
    \end{framed}

    \vspace{1em}
    $\boxed{a \equiv b}$

    \begin{prooftree}
    \AxiomC{$\vphantom{a}$}
    \RightLabel{$(\mathsf{refl})$}\UnaryInfC{$a \equiv a$}
    \DisplayProof
    \qquad
    \AxiomC{$a \rightsquigarrow a'$}
    \RightLabel{$(\mathsf{step})$}\UnaryInfC{$a \equiv a'$}
    \DisplayProof
    \qquad
    \AxiomC{$a \equiv b$}
    \RightLabel{$(\mathsf{symm})$}\UnaryInfC{$b \equiv a$}
    \DisplayProof
    \qquad
    \AxiomC{$a \equiv b$}
    \AxiomC{$b \equiv c$}
    \RightLabel{$(\mathsf{trans})$}\BinaryInfC{$a \equiv c$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \vspace{1em}
    $\boxed{a \leq b}$

    \begin{prooftree}
    \AxiomC{$a \equiv b$}
    \RightLabel{$(\mathsf{refl})$}\UnaryInfC{$a \leq b$}
    \DisplayProof
    \qquad
    \AxiomC{$\vphantom{i}$}
    \RightLabel{$(\mathsf{cuml})$}\UnaryInfC{$\mathcal U_i \leq \mathcal U_{i + 1}$}
    \DisplayProof
    \qquad
    \AxiomC{$a \leq b$}
    \AxiomC{$b \leq c$}
    \RightLabel{$(\mathsf{trans})$}\BinaryInfC{$a \leq c$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

\caption{Syntax and reduction rules (standard formulation)}
\label{fig:syntax}
\end{figure}

\begin{figure}
    $\boxed{\Gamma \vdasho \ok,\ \Gamma \vdasho a : A}$

    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\mathsf{empty})$}\UnaryInfC{$\epsilon \vdasho \ok$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdasho A : \mathcal U_i$}
    \RightLabel{$(\mathsf{assume})$}\UnaryInfC{$\Gamma, (x : A) \vdasho \ok$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdasho \ok$}
    \RightLabel{$(\mathsf{univ})$}\UnaryInfC{$\Gamma \vdasho \mathcal U_i: \mathcal U_{i + 1}$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdasho \ok$}
    \AxiomC{$(x : A) \in \Gamma$}
    \RightLabel{$(\mathsf{var})$}\BinaryInfC{$\Gamma \vdasho x : A$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdasho A : \mathcal U_i$}
    \AxiomC{$\Gamma \vdasho a : A$}
    \RightLabel{$(\mathsf{ann})$}\BinaryInfC{$\Gamma \vdasho (a : A) : A$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdasho A : \mathcal U_i$}
    \AxiomC{$\Gamma, (x : A) \vdasho B : \mathcal U_i$}
    \RightLabel{$(\Pi\mathsf{form})$}\BinaryInfC{$\Gamma \vdasho (\ppi x A B) : \mathcal U_i$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma, (x : A) \vdasho b : B$}
    \AxiomC{$\Gamma \vdasho (\ppi x A B) : \mathcal U_i$}
    \RightLabel{$(\Pi\mathsf{intro})$}\BinaryInfC{$\Gamma \vdasho (\lam x b) : (\ppi x A B)$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdasho b : (\ppi x A B)$}
    \AxiomC{$\Gamma \vdasho a : A$}
    \RightLabel{$(\Pi\mathsf{elim})$}\BinaryInfC{$\Gamma \vdasho \app b a : \subst x a B$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdasho A : \mathcal U_i$}
    \AxiomC{$\Gamma, (x : A) \vdasho B : \mathcal U_i$}
    \RightLabel{$(\Sigma\mathsf{form})$}\BinaryInfC{$\Gamma \vdasho (\sig x A B) : \mathcal U_i$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdasho a : A$}
    \AxiomC{$\Gamma \vdasho b : \subst x a B$}
    \AxiomC{$\Gamma \vdasho (\sig x A B) : \mathcal U_i$}
    \RightLabel{$(\Sigma\mathsf{intro})$}\TrinaryInfC{$\Gamma \vdasho \pair a b : (\sig x A B)$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdasho a : (\sig x A B)$}
    \RightLabel{$(\Sigma\mathsf{fst})$}\UnaryInfC{$\Gamma \vdasho \fst a : A$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma \vdasho a : (\sig x A B)$}
    \RightLabel{$(\Sigma\mathsf{snd})$}\UnaryInfC{$\Gamma \vdasho \snd a : \subst x {\fst a} B$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdasho \ok$}
    \RightLabel{$(\unit\mathsf{form})$}\UnaryInfC{$\Gamma \vdasho \unit : \mathcal U_0$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma \vdasho \ok$}
    \RightLabel{$(\unit\mathsf{intro})$}\UnaryInfC{$\Gamma \vdasho \sstar : \unit$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdasho a : A$}
    \AxiomC{$\Gamma \vdasho B : \mathcal U_i$}
    \AxiomC{$A \leq B$}
    \RightLabel{$(\mathsf{conv})$}\TrinaryInfC{$\Gamma \vdasho a : B$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

\caption{Type assignment rules (standard formulation)}
\label{fig:typing}
\end{figure}

Detailed and intuitive explanations of these rules may be found in \cite{program2013homotopy} as well. Here, we briefly summarise their intended meanings:

\begin{itemize}
    \item The rule $(\Pi\mathsf{elim})$ captures the common essence of \emph{function application}, \emph{modus ponens} and \emph{specialisation} in logic. Similarly, $(\Pi\mathsf{intro})$ captures \emph{function definition}, \emph{assumption} and \emph{generalisation}. The \emph{context} is a list of assumptions and variables in scope, introduced by $(\Pi\mathsf{intro})$ and are referenced via $(\mathsf{var})$. When appending new variables to the context, we need to check that the new variable is well-typed, which is the intended meaning of $(\mathsf{assume})$. These are the core rules of logical inference in \emph{natural deduction} proof systems \cite{gentzen1964investigations}.

    \item When iterated, $(\Sigma\mathsf{intro})$ allows us to `package' multiple assumptions, or multiple conclusions from the same assumption. Similarly, $(\Sigma\mathsf{fst})$ and $(\Sigma\mathsf{snd})$ allow us to take stuff from packages. The rule $(\unit\mathsf{intro})$ creates an empty package. Later entries in a package may reference earlier ones in their type via $(\mathsf{var})$. Packages are isomorphic to contexts, and are essential in representing complex theories which arise in practice. Even though they can be postulated, they are included in the core syntax for better type checking performance in Zenith.

    \item The reduction rules $(\beta)$ $(\pi\mathsf{fst})$ $(\pi\mathsf{snd})$ specify how terms can be rewritten (or \emph{reduced}) without being changed in essential ways. For example, $(\beta)$ substitutes an argument into a function, and $(\pi\mathsf{fst})$ says if we pack things together and then take one out, it is still the same thing. Congruence rules $(\mathsf{cong})$ allow reduction to happen anywhere inside a term. For terms being proofs, these rules correspond to \emph{cut elimination} \cite{gentzen1964investigations}. This may simplify terms, but combinatorial explosions are possible. Since we want to treat the objects as the same before and after such reduction, their reflexive, symmetric and transitive closure are taken, forming the \emph{definitional equality} relation $(\equiv)$, which is respected by typing via the typed \emph{conversion} rule $(\mathsf{conv})$.

    \item The universes $\mathcal U_i$, as well as type \emph{formation} rules $(\Pi\mathsf{form})$ $(\Sigma\mathsf{form})$ $(\unit\mathsf{form})$, are an unfortunate technical artifact in order to assign every well-formed term with a type, without causing Girard's paradox \cite{girard1972interpretation}. As we will see, a term represents a type iff it has type $\mathcal U_i$ for some $i\in\mathbb N$. In practice, most of the types have type $\mathcal U_0$, but in order to \emph{postulate} types and type constructors, we need to be able to state their types, which have type $\mathcal U_1$.

    \item The universes form an \emph{infinite hierarchy} $\mathcal U_0 : \mathcal U_1 : \mathcal U_2 : \ldots$ which is \emph{fully cumulative}: the subtyping relation $(\leq)$ and the $(\mathsf{conv})$ rule allow us to deduce that if some term $A : \mathcal U_i$, then $A : \mathcal U_j$ for all $j \geq i$. By implicitly lifting types inside `smaller' universes into `larger' ones, they make up for some inconvenience caused by the introduction of universe levels, removing one source of combinatorial explosion while preserving consistency.
\end{itemize}

\section{Meta-theoretic properties}
\label{sec:type_theory_standard_properties}

A dependent type theory necessarily blurs the distinction between terms and types. Since terms are annotated with types, types are part of some terms. Since types can be parametric on terms, terms are part of some types. For convenience, in standard presentations, terms and types are merged into a single syntactic category, collectively called \emph{terms}. Even so, there is still a distinction between them (\cref{thm:classification}).

Below are standard results which can be established for almost every flavour of dependent type theory \cite{harper1993framework,pym1990proofs,elliott1990extensions}, and for the particular case of ECC we refer to \cite{luo1990extended}. Full proofs are likely to be long and mostly uninteresting, so we only describe the outlines.

\begin{definition}[One-step parallel reduction]
\label{def:red1}
The relation $(\rightsquigarrow^1)$ between terms is inductively defined by:
\begin{prooftree}
\AxiomC{$a \rightsquigarrow^1 a'$}
\RightLabel{$(\tau)$}\UnaryInfC{$a : A \rightsquigarrow^1 a'$}
\DisplayProof
\qquad
\AxiomC{$a \rightsquigarrow^1 a'$}
\AxiomC{$b \rightsquigarrow^1 b'$}
\RightLabel{$(\beta)$}\BinaryInfC{$\app {(\lam x b)} a \rightsquigarrow^1 \subst x {a'} {b'}$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$a \rightsquigarrow^1 a'$}
\RightLabel{$(\pi\mathsf{fst})$}\UnaryInfC{$\fst {\pair a b} \rightsquigarrow^1 {a'}$}
\DisplayProof
\qquad
\AxiomC{$b \rightsquigarrow^1 b'$}
\RightLabel{$(\pi\mathsf{snd})$}\UnaryInfC{$\snd {\pair a b} \rightsquigarrow^1 {b'}$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\mathcal U_i \rightsquigarrow^1 \mathcal U_i$}
\DisplayProof
\qquad
\AxiomC{}
\UnaryInfC{$x \rightsquigarrow^1 x$}
\DisplayProof
\qquad
\AxiomC{$A \rightsquigarrow^1 A'$}
\AxiomC{$a \rightsquigarrow^1 a'$}
\BinaryInfC{$a : A \rightsquigarrow^1 a' : A'$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$A \rightsquigarrow^1 A'$}
\AxiomC{$B \rightsquigarrow^1 B'$}
\BinaryInfC{$\ppi x A B \rightsquigarrow^1 \ppi x {A'} {B'}$}
\DisplayProof
\qquad
\AxiomC{$b \rightsquigarrow^1 b'$}
\UnaryInfC{$\lam x b \rightsquigarrow^1 \lam x {b'}$}
\DisplayProof
\qquad
\AxiomC{$a \rightsquigarrow^1 a'$}
\AxiomC{$b \rightsquigarrow^1 b'$}
\BinaryInfC{$\app a b \rightsquigarrow^1 \app {a'} {b'}$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$A \rightsquigarrow^1 A'$}
\AxiomC{$B \rightsquigarrow^1 B'$}
\BinaryInfC{$\sig x A B \rightsquigarrow^1 \sig x {A'} {B'}$}
\DisplayProof
\quad
\AxiomC{$a \rightsquigarrow^1 a'$}
\AxiomC{$b \rightsquigarrow^1 b'$}
\BinaryInfC{$\pair a b \rightsquigarrow^1 \pair {a'} {b'}$}
\DisplayProof
\quad
\AxiomC{$a \rightsquigarrow^1 a'$}
\UnaryInfC{$\fst a \rightsquigarrow^1 \fst {a'}$}
\DisplayProof
\quad
\AxiomC{$a \rightsquigarrow^1 a'$}
\UnaryInfC{$\snd a \rightsquigarrow^1 \snd {a'}$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\unit \rightsquigarrow^1 \unit$}
\DisplayProof
\qquad
\AxiomC{}
\UnaryInfC{$\sstar \rightsquigarrow^1 \sstar$}
\DisplayProof
\AxiomC{}
\end{prooftree}
In other words, $a \rightsquigarrow^1 b$ iff $b$ is obtained by contracting zero or more \emph{immediate} redices in $a$.
\end{definition}

\begin{proposition}[Substitution preserves $\rightsquigarrow^1$]
\label{thm:red1_substitution}
For any $a, a', b, b' \termo$ such that $a \rightsquigarrow^1 a'$ and $b \rightsquigarrow^1 b'$, we have $\subst x a b \rightsquigarrow^1 \subst x {a'} {b'}$.
\end{proposition}

\begin{proof}
By strong induction on the structure of $a$, using definition of $(\rightsquigarrow^1)$.
\end{proof}

\begin{proposition}[Confluence of $\rightsquigarrow^1$]
\label{thm:red1_confluence}
For any $a,b,c \termo$ such that $a\rightsquigarrow^1 b$ and $a\rightsquigarrow^1 c$, exists $d \termo$ such that $b\rightsquigarrow^1 d$ and $c\rightsquigarrow^1 d$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\rightsquigarrow^1)$, using \cref{thm:red1_substitution} for case $(\beta)$.
\end{proof}

\begin{proposition}[Substitution preserves $\rightsquigarrow^\ast$]
\label{thm:red_substitution}
For any $a, a', b, b' \termo$ such that $a \rightsquigarrow^\ast a'$ and $b \rightsquigarrow^\ast b'$, we have $\subst x a b \rightsquigarrow^\ast \subst x {a'} {b'}$.
\end{proposition}

\begin{proof}
Clearly, every $(\rightsquigarrow^\ast)$ consists of finitely many applications of $(\rightsquigarrow)$ and therefore $(\rightsquigarrow^1)$. Let $m$ and $n$ be the numbers of reductions that $a\rightsquigarrow^\ast a'$ and $b\rightsquigarrow^\ast b'$ contain, respectively. The lemma can be shown by induction on $m$ and then on $n$, using \cref{thm:red1_substitution} in the inductive step.
\end{proof}

\begin{proposition}[Confluence of $\rightsquigarrow^\ast$]
\label{thm:red_confluence}
For any $a,b,c \termo$ such that $a\rightsquigarrow^\ast b$ and $a\rightsquigarrow^\ast c$, exists $d \termo$ such that $b\rightsquigarrow^\ast d$ and $c\rightsquigarrow^\ast d$.
\end{proposition}

\begin{proof}
Let $m$ and $n$ be the numbers of reductions that $a\rightsquigarrow^\ast b$ and $a\rightsquigarrow^\ast c$ contain, respectively. We can show that $b, c$ reduce to some common term $d$ by induction on $(m + n)$, using \cref{thm:red1_confluence} in the inductive step.
\end{proof}

This proof was first given in \cite{martin1998intuitionistic}. Confluence implies if a term reduces to some \emph{normal form} (where no further reductions are possible), it has a \emph{unique} normal form. So definitional equality can be checked by simply reducing two terms to normal forms and comparing them, given that no term has an infinite reduction sequence.

\begin{proposition}[Characterisation of $\equiv$]
\label{thm:characterisation_of_defeq}
For any $a, b \termo$, we have $a \equiv b$ iff exists $c \termo$ such that $a\rightsquigarrow^\ast c$ and $b\rightsquigarrow^\ast c$.
\end{proposition}

\begin{proof}
$(\leftarrow)$: By definition of $(\equiv)$, we have $a \equiv c$ and $b \equiv c$ so $a \equiv b$.

$(\rightarrow)$: By induction on the structure of $(\equiv)$, using \cref{thm:red_confluence} for the $(\mathsf{trans})$ case.
\end{proof}

\begin{proposition}[Injectivity of type formers]
\label{thm:injectivity_of_type_formers}
For any $A, A', B, B' \termo$:
\begin{itemize}[noitemsep]
    \item If $\mathcal U_i \equiv \mathcal U_j$, then $i = j$.
    \item If $\mathcal U_i \leq \mathcal U_j$, then $i \leq j$.
    \item If $(\ppi x A B) \leq (\ppi x {A'} {B'})$, then $A \equiv A'$ and $B \equiv B'$.
    \item If $(\sig x A B) \leq (\sig x {A'} {B'})$, then $A \equiv A'$ and $B \equiv B'$.
\end{itemize}
\end{proposition}

\begin{proof}
First show all results with $(\leq)$ replaced with $(\equiv)$. This can be done by \cref{thm:characterisation_of_defeq} and the fact that the only reduction rules applicable to $\mathcal U, \Pi, \Sigma$ forms are the congruence rules.

Then by induction on the number of $(\mathsf{refl})$ or $(\mathsf{cuml})$ steps in $(\leq)$.
\end{proof}

\begin{proposition}[Context]
\label{thm:context}
For any $\Gamma \ctxo$ and $a, A \termo$ such that $\Gamma \vdasho a : A$, we have $\Gamma \vdasho \ok$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdasho)$ in $\Gamma \vdasho a : A$.
\end{proof}

\begin{proposition}[Weakening]
\label{thm:weakening}
For any $\Gamma, \Gamma' \ctxo$ and $b, A, B \termo$ such that $\Gamma \vdasho A : \mathcal U_i$ and $(\Gamma, \Gamma') \vdasho b : B$, we have $(\Gamma, x : A, \Gamma') \vdasho b : B$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdasho)$ in $(\Gamma, \Gamma') \vdasho b : B$ (throughout this report, this means applying induction on $\Gamma^\cup \vdasho \ok$ and $\Gamma^\cup \vdasho b : B$ with the extra hypothesis $\Gamma^\cup = (\Gamma, \Gamma')$ for some $\Gamma' \ctxo$). For the case $(\mathsf{assume})$, we consider two subcases: if $\Gamma' = \epsilon$, use the side condition $\Gamma \vdasho A : \mathcal U_i$; otherwise, use the induction hypothesis.
\end{proof}

\begin{proposition}[Substitution principle]
\label{thm:substitution_principle}
For any $\Gamma, \Gamma' \ctxo$ and $a, b, A, B \termo$ such that $\Gamma \vdasho a : A$ and $(\Gamma, x : A, \Gamma') \vdasho b : B$, we have $(\Gamma, \subst x a {\Gamma'}) \vdasho \subst x a b : \subst x a B$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdasho)$ in $(\Gamma, x : A, \Gamma') \vdasho b : B$ (again, this involves introducing a new variable $\Gamma^\cup$ and the extra hypothesis $\Gamma^\cup = (\Gamma, x : A, \Gamma')$ for some $\Gamma' \ctxo$). The case $(\mathsf{assume})$ is similar as above. For the case $(\mathsf{var})$, we consider two subcases: if the variable is $x$, use the side condition $\Gamma \vdasho a : A$; otherwise, use the induction hypothesis $(\Gamma, \subst x a {\Gamma'}) \vdasho \ok$.
\end{proof}

\begin{definition}
\label{def:context_conv}
The relation $(\equiv)$ between contexts is defined by:
\begin{center}
    \AxiomC{$\vphantom{\Gamma}$}
    \UnaryInfC{$\epsilon \equiv \epsilon$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma \equiv \Gamma'$}
    \AxiomC{$A \equiv A'$}
    \BinaryInfC{$(\Gamma, x : A) \equiv (\Gamma', x : A')$}
    \DisplayProof
\end{center}
In other words, $\Gamma \equiv \Gamma'$ iff they contain definitionally equal entries.
\end{definition}

\begin{proposition}[Context conversion]
\label{thm:context_conv}
For any $\Gamma, \Gamma' \ctxo$ and $a, A \termo$ such that $\Gamma \vdasho a : A$ and $\Gamma \equiv \Gamma'$ and $\Gamma' \vdasho \ok$, we have $\Gamma' \vdasho a : A$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdasho)$ in $\Gamma \vdasho a : A$. Most cases are straightforward, the only interesting case is $(\mathsf{var})$, where we use the conversion rule.
\end{proof}

\begin{proposition}[Classification]
\label{thm:classification}
For any $\Gamma \ctxo$ and $a, A \termo$ such that $\Gamma \vdasho a : A$, we have $\Gamma \vdasho A : \mathcal U_i$ for some $i \in \mathbb N$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdasho)$ in $\Gamma \vdasho a : A$, show that (i) $\Gamma \vdasho A : \mathcal U_i$ for some $i$, and (ii) for all $(x : B) \in \Gamma$, we have $\Gamma \vdasho B : \mathcal U_i$ for some $i$. The second condition is added so that the case $(\mathsf{var})$ goes through. For the cases $(\Pi\mathsf{elim})$ $(\Sigma\mathsf{fst})$ $(\Sigma\mathsf{snd})$, use the induction hypothesis, inversion on $\Gamma \vdasho (\ppi x A B) : \mathcal U_i$ or $\Gamma \vdasho (\sig x A B) : \mathcal U_i$, and then \cref{thm:substitution_principle}.
\end{proof}

In other words, a term can appear to the right of a colon only if it has type $\mathcal U_i$ for some $i \in \mathbb N$. We can understand the situation as a \emph{three-layer hierarchy} (\cref{fig:three_layer_hierarchy}): terms which are themselves $\mathcal U_i$ are called \emph{universes}, terms which have type $\mathcal U_i$ but are not themselves $\mathcal U_i$ are called \emph{types}, and terms which have those as types are individuals. No terms can have individuals as their types, so the hierarchy stops here.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \node[] (u0) {$\mathcal U_0$};
    \node[right = of u0] (u1) {$\mathcal U_1$};
    \node[right = of u1] (u2) {$\mathcal U_2$};
    \node[right = of u2] (u3) {$\ldots$};
    \node[below = of u0] (a0) {$A_0$};
    \node[below = of u1] (a1) {$A_1$};
    \node[below = of u2] (a2) {$A_2$};
    \node[below = of a0] (t0) {$a_0$};
    \node[below = of a1] (t1) {$a_1$};
    \node[below = of a2] (t2) {$a_2$};
    \draw[-latex] (t0) -- node[sloped, fill = white]{:} (a0);
    \draw[-latex] (t1) -- node[sloped, fill = white]{:} (a1);
    \draw[-latex] (t2) -- node[sloped, fill = white]{:} (a2);
    \draw[-latex] (a0) -- node[sloped, fill = white]{:} (u0);
    \draw[-latex] (a1) -- node[sloped, fill = white]{:} (u1);
    \draw[-latex] (a2) -- node[sloped, fill = white]{:} (u2);
    \draw[-latex] (u0) -- node[sloped, fill = white]{:} (u1);
    \draw[-latex] (u1) -- node[sloped, fill = white]{:} (u2);
    \draw[-latex] (u2) -- node[sloped, fill = white]{:} (u3);
    \node[left = of u0, draw] (universes) {Universes};
    \node[left = of a0, draw] (types) {Types};
    \node[left = of t0, draw] (terms) {Terms};
    \end{tikzpicture}
\caption{Three-layer hierarchy}
\label{fig:three_layer_hierarchy}
\end{figure}

\begin{proposition}[Subject reduction]
\label{thm:subject_reduction}
For any $\Gamma \ctxo$ and $a, a', A \termo$ such that $\Gamma \vdasho a : A$ and $a \rightsquigarrow a'$, we have $\Gamma \vdasho a' : A$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdasho)$ in $\Gamma \vdasho a : A$. The case $(\mathsf{conv})$ follows from induction hypothesis. For every other case, the top-level form of $a$ is uniquely determined, and we proceed by inversion on \emph{(i.e. finding all relevant cases for)} $(\rightsquigarrow)$ in $a \rightsquigarrow a'$:
\begin{itemize}[noitemsep]
    \item For $(\mathsf{ann})$-$(\tau)$, use the induction hypothesis.
    \item For $(\Pi\mathsf{elim})$-$(\mathsf{\beta})$, apply inversion on $(\vdasho)$ in $\Gamma \vdasho (\lam x b) : (\ppi x A B)$, use \cref{thm:injectivity_of_type_formers,thm:substitution_principle}.
    \item For $(\Sigma\mathsf{fst})$-$(\mathsf{\pi{fst}})$ and $(\Sigma\mathsf{snd})$-$(\mathsf{\pi{snd}})$, apply inversion on $(\vdasho)$ in $\Gamma \vdasho \pair a b : (\sig x A B)$, use \cref{thm:injectivity_of_type_formers,thm:substitution_principle}.
    \item The rest are $(\mathsf{cong})$ cases, most of which are straightforward from their respective induction hypotheses, and congruence on bound variable types can be handled with \cref{thm:context_conv}. To drive the conversion rule, \cref{thm:classification} is sometimes needed.
\end{itemize}
As a corollary, reducing an entry in a well-typed context preserves well-typedness (by induction on $\Gamma \vdasho \ok$ and using \cref{thm:context_conv,thm:subject_reduction}), which implies for $\Gamma \vdasho a : A$ and $\Gamma \rightsquigarrow \Gamma'$ we have $\Gamma' \vdasho a : A$. In addition, with \cref{thm:classification,thm:subject_reduction}, for $\Gamma \vdasho a : A$ and $A \rightsquigarrow A'$ we also have $\Gamma \vdasho a : A'$. In other words, $\Gamma \vdasho a : A$ is preserved by reduction on either of $\Gamma, a, A$.
\end{proof}

\begin{proposition}[Strong normalisation]
\label{thm:red_termination}
For any $\Gamma \ctxo$ and $a,A \termo$ such that $\Gamma \vdasho a : A$, there exists no infinite reduction sequence of $(\rightsquigarrow)$ which starts at $a$.
\end{proposition}

The last proof is beyond the scope of this project. Since the system presented here is a fragment of ECC, the proof in \cite{luo1990extended} is applicable.

\section{Notions of equality}
\label{sec:notions_of_equality}

Based on the Curry-Howard correspondence between propositions and types, programs and proofs, some argue that reduction rules, embodying the notion of \emph{computation}, are the most prominent feature of type theories, and extend them as much as possible. The equivalence relation $(\equiv)$ generated by reduction rules is called \emph{definitional equality}. On the other hand, one can introduce a type family \texttt{Eq}, called the \emph{identity type}, which acts like a provable \emph{propositional equality} between terms, characterised by the Leibniz substitution law \cite{program2013homotopy}. This gives more flexibility in that we can now introduce assumptions of equalities, and that equalities between two terms are not required to be decidable, both necessary for a general \emph{mathematical} notion of equality.

In most ITPs, both notions of equality are used, and different systems often support different forms of definitional equalities. They cannot be expressed as postulates, and are usually hard-coded inside specific systems. Remarkable works have been done on unifying or extending the two notions of equality, a few of them summarised below.

At one end of the spectrum, completely removing reduction rules would result in an extreme example of \emph{weak type theories} (WTTs) \cite{berg2021quadratic}. They require rather explicit rewrites and type conversions using propositional equality axiom schemas for even a basic rule like $(\beta)$. At the other end, enhancing them with provable propositional equalities via the \emph{reflection} rule would result in \emph{extensional type theories} (ETTs) \cite{constable1986implementing}, which are undecidable in type checking. % \emph{Observational type theory} (OTT) \cite{altenkirch2006towards} uses a heterogeneous equality as well as an extensionality axiom for each data constructor, and is said to provide some benefits of ETT without its cost.

Another idea is allowing users to specify certain propositional equalities to be treated as reduction rules, thus supporting customised notions of definitional equality and computation. This approach is being used in Dedukti \cite{assaf2023dedukti} and Agda \cite{norell2007towards}, under the names \emph{$\lambda\Pi$-calculus modulo theory} \cite{boespflug2012lambdapi} and \emph{rewriting type theory} (RTT) \cite{cockx2021taming}, respectively. This is appealing for an intermediate representation of theorems and proofs across ITP systems. However, for a proper implementation, care must be taken in order to ensure custom rules do not break confluence (\cref{thm:red_confluence}) and therefore subject reduction (\cref{thm:subject_reduction}). Even so, they should be considered as a kind of global axiom, whose effects remain after the relevant variables have gone out of scope.

For this project, we included only the reduction rules given in \cref{fig:syntax}. Apart from performance reasons, under specific contexts they should guarantee to reduce proofs into simpler systems like first-order logics\footnote{With an `axiomatic' embedding, this can be shown by induction on the structure of normal forms, using the classification in \cref{sec:higher_order_unification}.}, enabling Zenith to work with them.

\section{Type universes}
\label{sec:universes}

In the system LF, there are only two universes $\mathsf{Type}$ and $\mathsf{Kind}$. In contrast, ECC has an \emph{infinite hierarchy} of universes. The original ECC also had \emph{covariance} rules for $\Pi$ and $\Sigma$ types based on the subsumptive subtyping relation $(\leq)$, which are not included here for their minor importance \emph{(such coercions may be easily achieved by $(\eta)$-expanding terms)} and non-generalisability to more complex types \cite{luo2012notes}.

However, mainstream ITPs also support \emph{universe-polymorphic} type declarations which cannot be simply postulated, since it requires postulating infinitely many \emph{(or at least exponentially many)} copies of the same type. But having universe polymorphism in Zenith seems too complicated.\footnote{Relatively simple approaches exist, which may be explored later. An example is \url{https://mazzo.li/epilogue/index.html\%3Fp=857&cpage=1.html}.}

Another problem is that this hierarchy can still be extended in different ways: Lean \cite{moura2021lean} and Coq \cite{gilbert2019definitional}, for example, both include a universe of \emph{definitionally proof-irrelevant} propositions at the bottom of the hierarchy, which also supports the \emph{(sub)singleton elimination} rule \cite{gilbert2019definitional}. It was later shown that this configuration breaks normalisation, which makes type checking undecidable in theory \cite{carneiro2019type,abel2020failure}. Recent versions of Agda have two infinite hierarchies, one on top of another\footnote{\url{https://agda.readthedocs.io/en/v2.7.0/language/universe-levels.html}}.

In Dedukti \cite{assaf2023dedukti}, systems with more complex universe hierarchies and polymorphisms are framed via \emph{Tarski-style universes} \cite{martin1984intuitionistic} with rewrite axioms, as detailed in \cite{cousineau2007embedding}. However, we currently do not support rewrite axioms, so Tarski encoding creates additional notational overhead that cannot be simplified. This has a negative impact on the performance of proof search.

As a temporary solution, we allow an \emph{inconsistent} `type-in-type' mode to be turned on in Zenith, which modifies the $(\mathsf{univ})$ rule as:

\begin{center}
    \AxiomC{$\Gamma \vdashd \ok$}
    \RightLabel{$(\mathsf{univ})$}\UnaryInfC{$\Gamma \vdashd \mathcal U_i: \mathcal U_i$}
    \DisplayProof
\end{center}

In this way, every type can be in the same universe $\mathcal U_0$. We conjecture this will not result in significant problem, as universe inconsistency is rare in mathematics, and the construction of the Girard's paradox \cite{girard1972interpretation} seems not easily discovered by either brute-force provers or machine learning models\footnote{For the latter, it depends on whether this is included in the training data. It still seems too early to worry about this, and the main focus of the project is on the proof theory instead of variations on the type theory itself...}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The standalone kernel}
\label{sec:kernel}

Even not necessarily optimised for the best performance, a type checker should still be fast enough to verify potentially large bodies of formal theories within reasonable time spans. This chapter will reformulate the type theory, in order to represent the optimisations that we included in the kernel of Zenith, our ITP implementation. Many of the ideas were inspired by the \texttt{smalltt} project.\footnote{\url{https://github.com/AndrasKovacs/smalltt/}}

\section{Normalisation by evaluation}
\label{sec:type_theory_def}

A naive implementation of the term substitution operation, $\subst x a b$, requires traversing through term $a$ and finding all variables. If a substitution does occur, the whole path from the replaced variable to the root of the term must be copied. For a function with multiple arguments and a very large body (e.g. an entire `parametrised module' as described in \cref{sec:from_pairs_to_tuples}), multiple traversals will be performed on the same large term, which may be detrimental to performance.

The standard approach of avoiding this is to use \emph{normalisation by evaluation} (NbE), or Coquand's type checking algorithm \cite{coquand1996algorithm}. Its soundness was proved in \cite{coquand1996algorithm} using a denotational semantics based on a class of models for the $\lambda$-calculus, whose existence was proved elsewhere. This section will give an operational semantics based on a formulation of \emph{let-expressions} \cite{severi1994pure}, which was also used in \cite{mcbride2000dependently}. This extends the syntax with the form $\llet x a b$, or more commonly written as $(\mathsf{let}\ x \defeq a\ \mathsf{in}\ b)$.

The internal representation of terms in an NbE type checker may be described by replacing term substitutions with let-expressions. Whenever a let-expression $\llet x a b$ is encountered, a new \emph{definition} $(x \defeq a)$ is added to a set\footnote{Even if \emph{higher-order abstract syntax} (HOAS) is used, the defunctionalised code will generate (meta-language level) closures that capture partially-applied arguments in a similar way.} when inspecting $b$, which locally extends the definitional equality relation by $(x \equiv a)$. Substitution is delayed until an actual variable $x$ is accessed, when it can be reduced to $a$. In this way, useless traversals across terms are totally avoided.

Throughout this report, a set of definitions $\Delta$ is called an \emph{environment}. A context $\Gamma$ may also contain definitions, which can be extracted as $(\defn \Gamma)$. We will sometimes omit $(\defn)$ in places where there is no ambiguity that an environment is expected but a context is provided. Specifically, we shall abbreviate $(\equiv_{\defn \Gamma})$ and $(\leq_{\defn \Gamma})$ as $(\equiv_\Gamma)$ and $(\leq_\Gamma)$.

\begin{definition}[Syntax]
\label{def:def_syntax}
The sets of terms $a \termd$, contexts $\Gamma \ctxd$ and environments $\Delta \envd$ are inductively defined by the rules listed in \cref{fig:def_syntax}.
\end{definition}

\begin{definition}[Conversion rules]
\label{def:def_conversion}
For any $\Delta \envd$, the relations $(\rightsquigarrow_\Delta)$ $(\equiv_\Delta)$ $(\leq_\Delta)$ between terms are inductively defined by the rules listed in \cref{fig:def_syntax}.
\end{definition}

\begin{definition}[Typing rules]
\label{def:def_typing}
For any $\Gamma \ctxd$ and $a, A \termd$, the relations $\Gamma \vdashd \ok$ and $\Gamma \vdashd a : A$ are inductively defined by the rules listed in \cref{fig:def_typing}.
\end{definition}

\begin{figure}
    $\boxed{a \termd,\ \Gamma \ctxd,\ \Delta \envd}$

    $$
    \begin{aligned}
    A,B,a,b ::=
        & \phantom{{}\mid{}} \mathcal U_{i \in \mathbb N} \mid x \mid a : A \mid \textcolor{blue}{\llet x a b} \\
        & \mid \ppi x A B \mid \lam x b \mid \app a b \\
        & \mid \sig x A B \mid \pair a b \mid \fst a \mid \snd a \\
        & \mid \unit \mid \sstar \\
    \Gamma ::=
        & \phantom{{}\mid{}} \epsilon \mid \Gamma, (x : A) \mid \textcolor{blue}{\Gamma, (x \defeq a : A)} & (x \notin \free \Gamma) \\
    \Delta ::=
        & \phantom{{}\mid{}} \epsilon \mid \textcolor{blue}{\Delta, (x \defeq a)} & (x \notin \free \Delta) \\
    \end{aligned}
    $$

    \vspace{1em}
    $\boxed{a \rightsquigarrow_\Delta b}$

    \begin{prooftree}
    \AxiomC{$\vphantom{()}$}
    \RightLabel{$(\tau)$}\UnaryInfC{$a : A \rightsquigarrow_\Delta a$}
    \DisplayProof
    \qquad
    \AxiomC{$(x \defeq a) \in \Delta$}
    \RightLabel{$(\delta)$}\UnaryInfC{$x \rightsquigarrow_\Delta a \vphantom{()}$}
    \textcolor{blue}\DisplayProof
    \qquad
    \AxiomC{$x \notin \free b$}
    \RightLabel{$(\zeta)$}\UnaryInfC{$\llet x a b \rightsquigarrow_\Delta b$}
    \textcolor{blue}\DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\beta)$}\UnaryInfC{$\app {(\lam x b)} a \rightsquigarrow_\Delta \llet x a b$}
    \textcolor{blue}\DisplayProof
    \qquad
    \AxiomC{}
    \RightLabel{$(\pi\mathsf{fst})$}\UnaryInfC{$\fst {\pair a b} \rightsquigarrow_\Delta a$}
    \DisplayProof
    \qquad
    \AxiomC{}
    \RightLabel{$(\pi\mathsf{snd})$}\UnaryInfC{$\snd {\pair a b} \rightsquigarrow_\Delta b$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \vspace{1em}
    \begin{framed}
        $(\mathsf{cong})$
        
        \begin{prooftree}
        \AxiomC{$a \rightsquigarrow_\Delta a'$}
        \UnaryInfC{$a : A \rightsquigarrow_\Delta a' : A$}
        \DisplayProof
        \qquad
        \AxiomC{$A \rightsquigarrow_\Delta A'$}
        \UnaryInfC{$a : A \rightsquigarrow_\Delta a : A'$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
        
        \begin{prooftree}
        \AxiomC{$a \rightsquigarrow_\Delta a'$}
        \UnaryInfC{$\llet x a b \rightsquigarrow_\Delta \llet x {a'} b$}
        \textcolor{blue}\DisplayProof
        \qquad
        \AxiomC{$b \rightsquigarrow_{\Delta, (x \defeq a)} b'$}
        \UnaryInfC{$\llet x a b \rightsquigarrow_\Delta \llet x a {b'}$}
        \textcolor{blue}\DisplayProof
        \AxiomC{}
        \end{prooftree}
        
        \begin{prooftree}
        \AxiomC{$A \rightsquigarrow_\Delta A'$}
        \UnaryInfC{$\ppi x A B \rightsquigarrow_\Delta \ppi x {A'} B$}
        \DisplayProof
        \qquad
        \AxiomC{$B \rightsquigarrow_\Delta B'$}
        \UnaryInfC{$\ppi x A B \rightsquigarrow_\Delta \ppi x A {B'}$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
        
        \begin{prooftree}
        \AxiomC{$b \rightsquigarrow_\Delta b'$}
        \UnaryInfC{$\lam x b \rightsquigarrow_\Delta \lam x {b'}$}
        \DisplayProof
        \qquad
        \AxiomC{$a \rightsquigarrow_\Delta a'$}
        \UnaryInfC{$\app a b \rightsquigarrow_\Delta \app {a'} b$}
        \DisplayProof
        \qquad
        \AxiomC{$b \rightsquigarrow_\Delta b'$}
        \UnaryInfC{$\app a b \rightsquigarrow_\Delta \app a {b'}$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
        
        \begin{prooftree}
        \AxiomC{$A \rightsquigarrow_\Delta A'$}
        \UnaryInfC{$\sig x A B \rightsquigarrow_\Delta \sig x {A'} B$}
        \DisplayProof
        \qquad
        \AxiomC{$B \rightsquigarrow_\Delta B'$}
        \UnaryInfC{$\sig x A B \rightsquigarrow_\Delta \sig x A {B'}$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
        
        \begin{prooftree}
        \AxiomC{$a \rightsquigarrow_\Delta a'$}
        \UnaryInfC{$\pair a b \rightsquigarrow_\Delta \pair {a'} b$}
        \DisplayProof
        \qquad
        \AxiomC{$b \rightsquigarrow_\Delta b'$}
        \UnaryInfC{$\pair a b \rightsquigarrow_\Delta \pair a {b'}$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
        
        \begin{prooftree}
        \AxiomC{$a \rightsquigarrow_\Delta a'$}
        \UnaryInfC{$\fst a \rightsquigarrow_\Delta \fst {a'}$}
        \DisplayProof
        \qquad
        \AxiomC{$a \rightsquigarrow_\Delta a'$}
        \UnaryInfC{$\snd a \rightsquigarrow_\Delta \snd {a'}$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
    \end{framed}

    \vspace{1em}
    $\boxed{a \equiv_\Delta b}$

    \begin{prooftree}
    \AxiomC{$\vphantom{a}$}
    \RightLabel{$(\mathsf{refl})$}\UnaryInfC{$a \equiv_\Delta a$}
    \DisplayProof
    \qquad
    \AxiomC{$a \rightsquigarrow_\Delta a'$}
    \RightLabel{$(\mathsf{step})$}\UnaryInfC{$a \equiv_\Delta a'$}
    \DisplayProof
    \qquad
    \AxiomC{$a \equiv_\Delta b$}
    \RightLabel{$(\mathsf{symm})$}\UnaryInfC{$b \equiv_\Delta a$}
    \DisplayProof
    \qquad
    \AxiomC{$a \equiv_\Delta b$}
    \AxiomC{$b \equiv_\Delta c$}
    \RightLabel{$(\mathsf{trans})$}\BinaryInfC{$a \equiv_\Delta c$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \vspace{1em}
    $\boxed{a \leq_\Delta b}$

    \begin{prooftree}
    \AxiomC{$a \equiv_\Delta b$}
    \RightLabel{$(\mathsf{refl})$}\UnaryInfC{$a \leq_\Delta b$}
    \DisplayProof
    \qquad
    \AxiomC{$\vphantom{i}$}
    \RightLabel{$(\mathsf{cuml})$}\UnaryInfC{$\mathcal U_i \leq_\Delta \mathcal U_{i + 1}$}
    \DisplayProof
    \qquad
    \AxiomC{$a \leq_\Delta b$}
    \AxiomC{$b \leq_\Delta c$}
    \RightLabel{$(\mathsf{trans})$}\BinaryInfC{$a \leq_\Delta c$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

\caption{Syntax and reduction rules}
\label{fig:def_syntax}
\end{figure}

\begin{figure}
    $\boxed{\Gamma \vdashd \ok,\ \Gamma \vdashd a : A}$

    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\mathsf{empty})$}\UnaryInfC{$\epsilon \vdashd \ok$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd A : \mathcal U_i$}
    \RightLabel{$(\mathsf{assume})$}\UnaryInfC{$\Gamma, (x : A) \vdashd \ok$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd a : A$}
    \RightLabel{$(\mathsf{define})$}\UnaryInfC{$\Gamma, (x \defeq a : A) \vdashd \ok$}
    \textcolor{blue}\DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd \ok$}
    \RightLabel{$(\mathsf{univ})$}\UnaryInfC{$\Gamma \vdashd \mathcal U_i: \mathcal U_{i + 1}$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd \ok$}
    \AxiomC{$(x : A) \in \Gamma$}
    \RightLabel{$(\mathsf{var})$}\BinaryInfC{$\Gamma \vdashd x : A$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd A : \mathcal U_i$}
    \AxiomC{$\Gamma \vdashd a : A$}
    \RightLabel{$(\mathsf{ann})$}\BinaryInfC{$\Gamma \vdashd (a : A) : A$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma, (x \defeq a : A) \vdashd b : B$}
    \RightLabel{$(\mathsf{let})$}\UnaryInfC{$\Gamma \vdashd \llet x a b : \llet x a B$}
    \textcolor{blue}\DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd A : \mathcal U_i$}
    \AxiomC{$\Gamma, (x : A) \vdashd B : \mathcal U_i$}
    \RightLabel{$(\Pi\mathsf{form})$}\BinaryInfC{$\Gamma \vdashd (\ppi x A B) : \mathcal U_i$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma, (x : A) \vdashd b : B$}
    \AxiomC{$\Gamma \vdashd (\ppi x A B) : \mathcal U_i$}
    \RightLabel{$(\Pi\mathsf{intro})$}\BinaryInfC{$\Gamma \vdashd (\lam x b) : (\ppi x A B)$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd b : (\ppi x A B)$}
    \AxiomC{$\Gamma \vdashd a : A$}
    \RightLabel{$(\Pi\mathsf{elim})$}\BinaryInfC{$\Gamma \vdashd \app b a : \llet x a B$}
    \textcolor{blue}\DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd A : \mathcal U_i$}
    \AxiomC{$\Gamma, (x : A) \vdashd B : \mathcal U_i$}
    \RightLabel{$(\Sigma\mathsf{form})$}\BinaryInfC{$\Gamma \vdashd (\sig x A B) : \mathcal U_i$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd a : A$}
    \AxiomC{$\Gamma \vdashd b : \llet x a B$}
    \AxiomC{$\Gamma \vdashd (\sig x A B) : \mathcal U_i$}
    \RightLabel{$(\Sigma\mathsf{intro})$}\TrinaryInfC{$\Gamma \vdashd \pair a b : (\sig x A B)$}
    \textcolor{blue}\DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd a : (\sig x A B)$}
    \RightLabel{$(\Sigma\mathsf{fst})$}\UnaryInfC{$\Gamma \vdashd \fst a : A$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma \vdashd a : (\sig x A B)$}
    \RightLabel{$(\Sigma\mathsf{snd})$}\UnaryInfC{$\Gamma \vdashd \snd a : \llet x {\fst a} B$}
    \textcolor{blue}\DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd \ok$}
    \RightLabel{$(\unit\mathsf{form})$}\UnaryInfC{$\Gamma \vdashd \unit : \mathcal U_0$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma \vdashd \ok$}
    \RightLabel{$(\unit\mathsf{intro})$}\UnaryInfC{$\Gamma \vdashd \sstar : \unit$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd a : A$}
    \AxiomC{$\Gamma \vdashd B : \mathcal U_i$}
    \AxiomC{$A \leq_{\Gamma} B$}
    \RightLabel{$(\mathsf{conv})$}\TrinaryInfC{$\Gamma \vdashd a : B$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

\caption{Type assignment rules}
\label{fig:def_typing}
\end{figure}

Notice how the term substitution operation, $\subst x a b$, is no longer involved in the rules.

\begin{itemize}
    \item The $(\delta)$ rule expands definitions as just mentioned. $(\zeta)$ removes a let-binder when its variable is no longer being referenced inside its body. $(\beta)$ uses let-expressions $\llet x a b$ instead of term substitution operations $\subst x a b$. With $(\delta)$ and $(\zeta)$, we now have $\llet x a b \rightsquigarrow_\Delta^\ast \subst x a b$.

    \item The new $(\mathsf{cong})$ rule for reducing inside let-expressions makes use of the definition by adding it to the environment. The environment acts like the \emph{stack} in programming languages.

    \item Finally, $(\mathsf{define})$ governs the addition of a well-typed definition to the context during type checking, in a similar way as how $(\mathsf{assume})$ handles the addition of a new variable.
\end{itemize}

The new reduction relation $(\rightsquigarrow_\Delta)$ is still \emph{nondeterministic}, in that they still allow contraction of redices anywhere throughout the term, via the full set of $(\mathsf{cong})$ rules. In contrast, the Zenith kernel implements \emph{weak reduction} following the \emph{call-by-value} reduction order. Weak reduction $(\rightsquigarrow^\mathsf{weak}_\Delta)$ refers to $(\rightsquigarrow_\Delta)$ with a smaller set of $(\mathsf{cong})$ rules, so that reduction never occur inside $\lambda$-, $\Pi$- or $\Sigma$-binders. A term in weak normal form is also called a \emph{value}. Call-by-value reduction then means that an argument term is always reduced to a value before being supplied to a $\lambda$-, $\Pi$- or $\Sigma$-binder.

Implementing weak reduction requires one more rule:
\begin{prooftree}
    \AxiomC{$\Delta = (x_0 \defeq a_0, \ldots, x_{n-1} \defeq a_{n-1})$}
    \RightLabel{$(\mathsf{attach})$}\UnaryInfC{$b \equiv_\Delta \llet {x_0} {a_0} {\ldots \llet {x_{n-1}} {a_{n-1}} b}$}
\end{prooftree}

This is admissible: both sides can be unfolded under $\Delta$ to $\subst {x_0} {a_0} {\ldots \subst {x_{n-1}} {a_{n-1}} b}$ using the $(\delta)$ $(\zeta)$ $(\mathsf{cong})$ rules. During reduction, we are slightly more liberal to allow rewriting from left to right as if the definitional equality $(\mathsf{attach})$ is a reduction rule, which does not strengthen conversion checking.\footnote{But it obviously breaks strong normalisation, so we did not include it as a part of the theory. The type checker can still be terminating with a careful application of this rule.} The $(\mathsf{attach})$ rule then means that the whole environment can be attached around any term. A term with entire environment attached around it is called a \emph{thunk}, which admit the same normal form when reduced in an empty environment. A term which begins with a $\lambda$-, $\Pi$- or $\Sigma$-binder and has a thunk as its body is also called a \emph{closure}. Both thunks and closures are closed terms. In addition, closures are values.

The Zenith kernel does not create thunks except for creating closures. To be able to quickly attach the whole environment during closure creation, environments are implemented as linked lists to allow structural sharing. This means variable lookup in environments can take linear time. However, benchmarks have shown this performance impact is acceptable.

Whenever a rule requires a let-binding to be applied to the thunk body of a closure, like in $(\mathsf{\beta})$ and $(\Pi\mathsf{elim})$, the new let-binder is first swapped across the attached environment (which is similarly admissible, since there are no dependencies between swapped let-bindings here), and then the thunk is reduced. Such ordering elides the need for variable renaming inside the kernel, which represents variables using a \emph{locally nameless} scheme: % \cite{mcbride2004functional}:

\begin{itemize}
    \item Variables inside unnormalised terms, including inside closures, are in \emph{de Bruijn indices} \cite{de1972lambda}. This means a bound variable is represented as a natural number indicating how many binders (i.e. $\Pi$, $\lambda$, $\Sigma$ or let) it `goes up' in the expression tree. For example, a variable numbered 0 is bound to the nearest binder on its path to the root node of the term, a variable numbered 1 passes through one binder and is bound to the second-nearest one, etc. If the de Bruijn index is not less than the total number of binders, the variable is instead free, with the amount of `overflow' indicating the right-to-left position inside the context. For example, an `overflow' of 0 is free and refers to the rightmost entry in $\Gamma$, an `overflow' of 1 is free and refers to the second rightmost entry in $\Gamma$, etc.

    \item Variables inside values, except inside closures, are in \emph{de Bruijn levels} \cite{de1972lambda}. This means a free variable is represented as a natural number indicating the left-to-right position inside the context. For example, a variable numbered 0 is free and refers to the leftmost entry in $\Gamma$, a variable numbered 1 is free and refers to the second leftmost entry in $\Gamma$, etc.
\end{itemize}

During $(\delta)$ reduction, a value $a$ is pulled into a larger context. If de Bruijn indices are used throughout, the free variables inside $a$ will require re-indexing; if de Bruijn levels are used throught, the bound variables inside $a$ will require re-indexing. Both potentially require a full copy of $a$. However, by interleaving de Bruijn indices and levels, re-indexing becomes unnecessary. Since $(\delta)$ reductions can be frequent, the ability to elide variable renamings is important for good performance.

\section{Meta-theoretic properties}
\label{sec:type_theory_def_properties}

This section describes some useful properties for the modified system, and gives proof outlines that the modified system (\cref{fig:def_syntax,fig:def_typing}) is `equivalent' to the standard formulation without let-expressions (\cref{fig:syntax,fig:typing}).

\begin{definition}[One-step parallel reduction]
\label{def:def_red1}
The relation $(\rightsquigarrow_\Delta^1)$ between terms is inductively defined by the same rules in \cref{def:red1} with $(\beta)$ replaced by:
\begin{prooftree}
    \AxiomC{$(x \defeq a) \in \Delta$}
    \RightLabel{$(\delta)$}\UnaryInfC{$x \rightsquigarrow_\Delta^1 a$}
    \DisplayProof
    \qquad
    \AxiomC{$x \notin \free b$}
    \AxiomC{$b \rightsquigarrow_\Delta^1 b'$}
    \RightLabel{$(\zeta)$}\BinaryInfC{$\llet x a b \rightsquigarrow_\Delta^1 b'$}
    \DisplayProof
    \AxiomC{}
\end{prooftree}
\begin{prooftree}
    \AxiomC{$a \rightsquigarrow_\Delta^1 a'$}
    \AxiomC{$b \rightsquigarrow_\Delta^1 b'$}
    \RightLabel{$(\beta)$}\BinaryInfC{$\app {(\lam x b)} a \rightsquigarrow_\Delta^1 \llet x {a'} {b'}$}
    \DisplayProof
    \qquad
    \AxiomC{$a \rightsquigarrow_\Delta^1 a'$}
    \AxiomC{$b \rightsquigarrow_{\Delta, (x \defeq a)}^1 b'$}
    \BinaryInfC{$\llet x a b \rightsquigarrow_\Delta^1 \llet x {a'} {b'}$}
    \DisplayProof
    \AxiomC{}
\end{prooftree}
\end{definition}

\begin{proposition}[Confluence of $\rightsquigarrow_\Delta^1$]
\label{thm:def_red1_confluence}
For any $\Delta \envd$ and $a,b,c \termd$ such that $a\rightsquigarrow_\Delta^1 b$ and $a\rightsquigarrow_\Delta^1 c$, exists $d \termd$ such that $b\rightsquigarrow_\Delta^1 d$ and $c\rightsquigarrow_\Delta^1 d$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\rightsquigarrow_\Delta^1)$.
\end{proof}

\begin{proposition}[Confluence of $\rightsquigarrow_\Delta^\ast$]
\label{thm:def_red_confluence}
For any $\Delta \envd$ and $a,b,c \termd$ such that $a\rightsquigarrow_\Delta^\ast b$ and $a\rightsquigarrow_\Delta^\ast c$, exists $d \termd$ such that $b\rightsquigarrow_\Delta^\ast d$ and $c\rightsquigarrow_\Delta^\ast d$.
\end{proposition}

\begin{proof}
Apply similar reasoning as in \cref{thm:red_confluence}.
\end{proof}

\begin{proposition}[Strong normalisation of $\delta$ and $\zeta$]
\label{thm:def_delta_zeta_termination}
For any $\Delta \envd$ and $a \termd$, there exists no infinite reduction sequence of $(\rightsquigarrow_\Delta)$ which starts at $a$ using only $(\delta)$ $(\zeta)$ $(\mathsf{cong})$ rules.
\end{proposition}

\begin{proof}
Let $|a|$ denote the size of the term $a$, and $|\Delta|$ be the sum of sizes of terms in $\Delta$. Define the `unfolding potential' function $\phi(\Delta, a)$ by
$$
\begin{aligned}
\phi(\Delta, (\llet x a b)) &\defeq \phi(\Delta, a) + \phi((\Delta, x \defeq a), b) + 1 \\
\phi(\Delta, x) &\defeq \phi(\Delta, a) + 1 & (x \defeq a \in \Delta) \\
\phi(\Delta, x) &\defeq 0 & (x\notin\dom\Delta) \\
&\ldots
\end{aligned}
$$
and all other cases by taking the sum of $\phi$ over subterms, e.g. $\phi(\Delta, (\ppi x A B)) \defeq \phi(\Delta, A) + \phi(\Delta, B)$. The recursive definition itself is terminating since $(|\Delta| + |a|)$ is strictly decreasing.

By induction on $(|\Delta| + |a|)$, we can show for any $\Delta, \Delta' \envd$ and $a \termd$ such that $\Delta\subseteq\Delta'$ but $\free a \cap (\operatorname{dom} \Delta' \setminus \operatorname{dom} \Delta) = \emptyset$, we have $\phi(\Delta, a) = \phi(\Delta', a)$. This means $\phi((\Delta, x \defeq a, \Delta'), x) > \phi(\Delta, a) = \phi((\Delta, x \defeq a, \Delta'), a)$ and if $x \notin \free b$, $\phi(\Delta, (\llet x a b)) > \phi((\Delta, x \defeq a), b) = \phi(\Delta, b)$, which implies if $a \rightsquigarrow_\Delta a'$ via $(\delta)$ or $(\zeta)$, then $\phi(\Delta, a') < \phi(\Delta, a)$.

Since every $(\rightsquigarrow_\Delta)$ using only $(\delta)$ $(\zeta)$ $(\mathsf{cong})$ rules involves exactly one application of $(\delta)$ or $(\zeta)$, for any reduction sequence $a \rightsquigarrow_\Delta a_1 \rightsquigarrow_\Delta \ldots$ we have $\phi(\Delta, a) > \phi(\Delta, a_1) > \ldots$ which must be terminating.
\end{proof}

By similar reasoning as in \cref{thm:def_red1_confluence,thm:def_red_confluence}, reductions involving only $(\delta)$ $(\zeta)$ $(\mathsf{cong})$ are confluent on their own. This implies every term has a unique $(\delta\zeta)$-normal form.

\begin{proposition}[Projection]
\label{thm:def_projection}
For any $\Delta \envd$ and $a \termd$, denote by $\inter{a}_\Delta$ the unique $(\delta\zeta)$-normal form of $a$ under $\Delta$. Then we have $\inter{a}_\Delta \termo$ and $(\free{\inter{a}_\Delta} \cap \dom \Delta) = \emptyset$.
\end{proposition}

\begin{proof}
Assume otherwise $\inter{a}_\Delta$ contains a let-expression $\llet x c b$. If $x \in \free b$, the occurrence of $x$ in $b$ can be reduced by the $(\delta)$ rule. If $x \notin \free b$, we have $\llet x c b \rightsquigarrow_\Delta b$ by the $(\zeta)$ rule. In either case, $\inter{a}_\Delta$ is not in $(\delta\zeta)$-normal form, contradiction. This means $\inter{a}_\Delta \termo$.

For any $x \in \dom \Delta$, assume otherwise $x \in \free{\inter{a}_\Delta}$, the occurrence of $x$ in $\inter{a}_\Delta$ may be reduced by the $(\delta)$ rule, which is a contradiction. Therefore $(\free{\inter{a}_\Delta} \cap \dom \Delta) = \emptyset$.
\end{proof}

\begin{proposition}[Equivalence]
\label{thm:def_defeq_equivalence}
For any $\Delta \envd$ and $a, b \termd$:
\begin{itemize}[noitemsep]
    \item If $a \rightsquigarrow_\Delta b$, then $\inter{a}_\Delta \rightsquigarrow^1 \inter{b}_\Delta$.
    \item If $\inter{a}_\Delta \rightsquigarrow \inter{b}_\Delta$, then $a \equiv_\Delta b$.
\end{itemize}
\end{proposition}

\begin{proof}
(1) By induction on the structure of $(\rightsquigarrow_\Delta)$. The cases which require checking are:

\begin{itemize}[noitemsep]
    \item For $(\delta)$, given $(x \defeq a) \in \Delta$, we have $\inter{x}_\Delta = \inter{a}_\Delta$.

    \item For $(\zeta)$, given $x \notin \free b$, we have $\inter{\llet x a b}_\Delta = \inter{b}_\Delta$.

    \item For $(\beta)$, we have $\inter{\app {(\lam x b)} a}_\Delta = \app {(\lam x {\inter{b}_\Delta})} {\inter{a}_\Delta} \rightsquigarrow \subst x {\inter{a}_\Delta} {\inter{b}_\Delta} = \inter{\llet x a b}_\Delta$.

    \item For $(\mathsf{cong})$ over let-binders, given the induction hypothesis $\inter{a}_\Delta \rightsquigarrow^1 \inter{a'}_\Delta$ we have $\inter{\llet x a b}_\Delta = \subst x {\inter{a}_\Delta} {\inter{b}_\Delta} \rightsquigarrow^1 \subst x {\inter{a'}_\Delta} {\inter{b}_\Delta} = \inter{\llet x {a'} b}_\Delta$.

    \item For $(\mathsf{cong})$ over let-bodies, given the induction hypothesis $\inter{b}_{\Delta, (x \defeq a)} \rightsquigarrow^1 \inter{b'}_{\Delta, (x \defeq a)}$ we have $\inter{\llet x a b}_\Delta = \inter{b}_{\Delta, (x \defeq a)} \rightsquigarrow^1 \inter{b'}_{\Delta, (x \defeq a)} = \inter{\llet x a {b'}}_\Delta$.
\end{itemize}

(2) Since $a \equiv_\Delta \inter{a}_\Delta$ and $b \equiv_\Delta \inter{b}_\Delta$, it suffices to show for any $a, b \termo$ such that $a \rightsquigarrow b$ we have $a \equiv_\epsilon b$. This is by induction on the structure of $(\rightsquigarrow)$. The only non-trivial case is $(\beta)$, where we have $\app {(\lam x b)} a \rightsquigarrow_\epsilon \llet x a b \rightsquigarrow_\epsilon^\ast \subst x a b$.
\end{proof}

As a corollary, we have $a \equiv_\Delta b$ iff $\inter{a}_\Delta \equiv \inter{b}_\Delta$, and $a \leq_\Delta b$ iff $\inter{a}_\Delta \leq \inter{b}_\Delta$.

\begin{proposition}[Context projection]
\label{thm:def_ctx_projection}
For any $\Gamma \ctxd$, define $\inter{\Gamma}$ inductively by:
$$
\begin{aligned}
    \inter{\epsilon} &\defeq \epsilon \\
    \inter{\Gamma, x : A} &\defeq \inter{\Gamma}, (x : \inter{A}_\Gamma) \\
    \inter{\Gamma, x \defeq a : A} &\defeq \inter{\Gamma} \\
\end{aligned}
$$
In other words, $\inter{\Gamma}$ unfolds all definitions in all types $A$ in $\Gamma$, and then removes all defined variables $x \in (\dom \defn \Gamma)$ from $\Gamma$. Then we have $\inter{\Gamma} \ctxo$.
\end{proposition}

\begin{proof}
By induction on the structure of $\Gamma$, using \cref{thm:def_projection}.
\end{proof}

\begin{proposition}[Soundness]
\label{thm:def_soundness}
For any $\Gamma \ctxd$ and $a, A \termd$ such that $\Gamma \vdashd a : A$, we have $\inter{\Gamma} \vdasho \inter{a}_\Gamma : \inter{A}_\Gamma$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdashd)$ in $\Gamma \vdashd a : A$, show that (i) $\inter{\Gamma} \vdasho \inter{a}_\Gamma : \inter{A}_\Gamma$, and (ii) for all $(x \defeq b : B) \in \Gamma$, we have $\inter{\Gamma} \vdasho \inter{b}_\Gamma : \inter{B}_\Gamma$. The second condition is added so that $(\mathsf{define})$ and $(\mathsf{var})$ go through. All other cases can be derived via the conversion rule, \cref{thm:classification}, and the equation $\inter{\llet x a b}_\Delta = \subst x {\inter{a}_\Delta} {\inter{b}_\Delta}$.
\end{proof}

Unfortunately, soundness alone does not imply nice meta-theoretic properties like subject reduction, since there is still a possibility that terms may fail to be well-typed after being reduced. This has to be proved again for the modified system.

\begin{proposition}[Characterisation of $\equiv_\Delta$]
\label{thm:def_characterisation_of_defeq}
For any $\Delta \envd$ and $a, b \termd$, we have $a \equiv_\Delta b$ iff exists $c \termd$ such that $a\rightsquigarrow_\Delta^\ast c$ and $b\rightsquigarrow_\Delta^\ast c$.
\end{proposition}

\begin{proof}
Apply similar reasoning as in \cref{thm:characterisation_of_defeq}.
\end{proof}

\begin{proposition}[Injectivity of type formers]
\label{thm:def_injectivity_of_type_formers}
For any $\Delta \envd$ and $A, A', B, B' \termd$:
\begin{itemize}[noitemsep]
    \item If $\mathcal U_i \equiv_\Delta \mathcal U_j$, then $i = j$.
    \item If $\mathcal U_i \leq_\Delta \mathcal U_j$, then $i \leq j$.
    \item If $(\ppi x A B) \leq_\Delta (\ppi x {A'} {B'})$, then $A \equiv_\Delta A'$ and $B \equiv_\Delta B'$.
    \item If $(\sig x A B) \leq_\Delta (\sig x {A'} {B'})$, then $A \equiv_\Delta A'$ and $B \equiv_\Delta B'$.
\end{itemize}
\end{proposition}

\begin{proof}
Apply similar reasoning as in \cref{thm:injectivity_of_type_formers}.
\end{proof}

\begin{proposition}[Context]
\label{thm:def_context}
For any $\Gamma \ctxd$ and $a, A \termd$ such that $\Gamma \vdashd a : A$, we have $\Gamma \vdashd \ok$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdashd)$ in $\Gamma \vdashd a : A$.
\end{proof}

\begin{proposition}[Weakening]
\label{thm:def_weakening}
For any $\Gamma, \Gamma' \ctxd$ and $a, b, A, B \termd$:
\begin{itemize}[noitemsep]
    \item If $\Gamma \vdashd A : \mathcal U_i$ and $(\Gamma, \Gamma') \vdashd b : B$, we have $(\Gamma, x : A, \Gamma') \vdashd b : B$.
    \item If $\Gamma \vdashd a : A$ and $(\Gamma, x : A, \Gamma') \vdashd b : B$, we have $(\Gamma, x \defeq a : A, \Gamma') \vdashd b : B$.
\end{itemize}
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdashd)$.
\end{proof}

\begin{proposition}[Strengthening]
\label{thm:def_strengthening}
For any $\Gamma, \Gamma' \ctxd$ and $a, b, A, B \termd$:
\begin{itemize}[noitemsep]
    \item If $x \notin \free b \cup \free B \cup \free{\Gamma'}$ and $(\Gamma, x : A, \Gamma') \vdashd b : B$, we have $(\Gamma, \Gamma') \vdashd b : B$.
    \item If $x \notin \free b \cup \free B \cup \free{\Gamma'}$ and $(\Gamma, x \defeq a : A, \Gamma') \vdashd b : B$, we have $(\Gamma, \Gamma') \vdashd b : B$.
\end{itemize}
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdashd)$.
\end{proof}

\begin{definition}
\label{def:def_context_conv}
The relation $(\equiv)$ between contexts is defined by:
\begin{center}
    \AxiomC{}
    \UnaryInfC{$\epsilon \equiv \epsilon$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma \equiv \Gamma'$}
    \AxiomC{$A \equiv_{\Gamma} A'$}
    \BinaryInfC{$(\Gamma, x : A) \equiv (\Gamma', x : A')$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma \equiv \Gamma'$}
    \AxiomC{$A \equiv_{\Gamma} A'$}
    \AxiomC{$a \equiv_{\Gamma} a'$}
    \TrinaryInfC{$(\Gamma, x \defeq a : A) \equiv (\Gamma', x \defeq a' : A')$}
    \DisplayProof
\end{center}
In other words, $\Gamma \equiv \Gamma'$ iff they contain definitionally equal entries.
\end{definition}

If $\Gamma \equiv \Gamma'$, then $(\equiv_\Gamma)$ and $(\equiv_{\Gamma'})$ are the same relation, so $(\equiv)$ between contexts is symmetric and transitive, despite the definition uses $(\equiv_\Gamma)$ but not $(\equiv_{\Gamma'})$.

\begin{proposition}[Context conversion]
\label{thm:def_context_conv}
For any $\Gamma, \Gamma' \ctxd$ and $a, A \termd$ such that $\Gamma \vdashd a : A$ and $\Gamma \equiv \Gamma'$ and $\Gamma' \vdashd \ok$, we have $\Gamma' \vdashd a : A$.
\end{proposition}

\begin{proof}
Apply similar reasoning as in \cref{thm:context_conv}.
\end{proof}

\begin{proposition}[Classification]
\label{thm:def_classification}
For any $\Gamma \ctxd$ and $a, A \termd$ such that $\Gamma \vdashd a : A$, we have $\Gamma \vdashd A : \mathcal U_i$ for some $i \in \mathbb N$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdashd)$ in $\Gamma \vdashd a : A$, show that (i) $\Gamma \vdashd A : \mathcal U_i$ for some $i$, and (ii) for all $(x : B) \in \Gamma$, we have $\Gamma \vdashd B : \mathcal U_i$ for some $i$. The second condition is added so that $(\mathsf{define})$ and $(\mathsf{var})$ go through. For the cases $(\mathsf{let})$ $(\Pi\mathsf{elim})$ $(\Sigma\mathsf{fst})$ $(\Sigma\mathsf{snd})$, use the induction hypothesis, inversion on $\Gamma \vdashd (\ppi x A B) : \mathcal U_i$ or $\Gamma \vdashd (\sig x A B) : \mathcal U_i$, the $(\mathsf{let})$ $(\mathsf{conv})$ rules and \cref{thm:def_weakening}.
\end{proof}

\begin{proposition}[Subject reduction]
\label{thm:def_subject_reduction}
For any $\Gamma \ctxd$ and $a, a', A \termd$ such that $\Gamma \vdashd a : A$ and $a \rightsquigarrow_\Gamma a'$, we have $\Gamma \vdashd a' : A$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdashd)$ in $\Gamma \vdashd a : A$, show that (i) for all $a' \termd$ such that $a \rightsquigarrow_\Gamma a'$, we have $\Gamma \vdashd a' : A$, and (ii) for all $(x \defeq b : B) \in \Gamma$, we have $\Gamma \vdashd b : B$. Then apply similar reasoning as in \cref{thm:subject_reduction} with some variations:
\begin{itemize}[noitemsep]
    \item For $(\mathsf{var})$-$(\delta)$, use the side condition (ii).
    \item For $(\mathsf{let})$-$(\zeta)$, use the $(\mathsf{let})$ $(\mathsf{conv})$ rules and \cref{thm:def_strengthening}.
    \item For $(\Pi\mathsf{elim})$-$(\mathsf{\beta})$, apply inversions on $(\vdashd)$ and use the $(\mathsf{let})$ $(\mathsf{conv})$ rules, \cref{thm:def_injectivity_of_type_formers,thm:def_weakening}.
    \item For $(\Sigma\mathsf{fst})$-$(\mathsf{\pi{fst}})$ and $(\Sigma\mathsf{snd})$-$(\mathsf{\pi{snd}})$, apply inversions on $(\vdashd)$ and use the $(\mathsf{conv})$ rule, \cref{thm:def_injectivity_of_type_formers,thm:def_weakening}.
\end{itemize}
Again, as a corollary, $\Gamma \vdashd a : A$ is preserved by reduction on either of $\Gamma, a, A$.
\end{proof}

\begin{proposition}[Completeness]
\label{thm:def_completeness}
For any $\Gamma \ctxo$ and $a, A \termo$ such that $\Gamma \vdasho a : A$, we have $\Gamma \vdashd a : A$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdasho)$ in $\Gamma \vdasho a : A$. Only the modified rules, $(\Pi\mathsf{elim})$ $(\Sigma\mathsf{intro})$ $(\Sigma\mathsf{snd})$, require checking. This can be done via the $(\mathsf{conv})$ rule, \cref{thm:def_classification,thm:def_weakening} and inversion on $\Gamma \vdashd (\sig x A B) : \mathcal U_i$.
\end{proof}

The next lemma improves upon the one presented in \cite{severi1994pure}. The idea is natural: $(\delta)$ and $(\zeta)$ reductions merely break down term substitutions into smaller steps (\cref{thm:def_defeq_equivalence}), so as long as those smaller steps are terminating on their own, we can turn a reduction sequence back into the original system by `running all unfinished term substitutions to completion' in each step.

However, one must take care of reductions on `useless' let-bindings, which translate to identities instead of reductions in the base system. In \cite{severi1994pure}, this was remedied by including a useless $(\beta)$-redex in the translation of every let-expression. This worked since they assumed the rule $\app {(\lam x b)} a \rightsquigarrow_\Delta \subst x a b$ instead of the $\app {(\lam x b)} a \rightsquigarrow_\Delta \llet x a b$ presented here, the latter would no longer translate to one or more reductions, due to the creation of a new $(\beta)$-redex. The latter rule was adopted in \cite{mcbride2000dependently}, which however did not include a proof for the preservation of strong normalisation.

Here, we monkey patch this small issue by explicitly taking all let-bindings into the translation, including the `useless' ones:

\begin{proposition}[Strong normalisation]
\label{thm:def_red_termination}
For any $\Gamma \ctxd$ and $a,A \termd$ such that $\Gamma \vdashd a : A$, there exists no infinite reduction sequence of $(\rightsquigarrow_\Gamma)$ which starts at $a$.
\end{proposition}

\begin{proof}
For any $\Delta \envd$ and $a \termd$, define the multiset of all its let-bindings $\psi(\Delta, a)$ by
$$
\begin{aligned}
\psi(\Delta, (\llet x a b)) &\defeq \{\inter{a}_\Delta\} \sqcup \psi(\Delta, a) \sqcup \psi((\Delta, x \defeq a), b) \\
\psi(\Delta, x) &\defeq \psi(\Delta, a) & (x \defeq a \in \Delta) \\
\psi(\Delta, x) &\defeq \emptyset & (x\notin\dom\Delta) \\
&\ldots
\end{aligned}
$$
and all other cases by taking the disjoint union of $\psi$ over subterms, e.g. $\psi(\Delta, (\ppi x A B)) \defeq \psi(\Delta, A) \sqcup \psi(\Delta, B)$. The recursive definition itself is terminating, using the same size function as in \cref{thm:def_delta_zeta_termination}. We then define the translation $\trans{a}_\Delta$ as the finite multiset $\{\inter{a}_\Delta\} \sqcup \psi(\Delta, a)$.

Applying similar reasoning as in \cref{thm:def_defeq_equivalence}, we have:

\begin{itemize}[noitemsep]
    \item If $a \rightsquigarrow_\Delta a'$ via $(\delta)$ $(\zeta)$ and $(\mathsf{cong})$ rules, then $\trans{a}_\Delta \supseteq \trans{a'}_\Delta$.
    \item If $a \rightsquigarrow_\Delta a'$ via $(\tau)$ $(\beta)$ $(\pi\mathsf{fst})$ $(\pi\mathsf{snd})$ and $(\mathsf{cong})$ rules, then either $\inter{a}_\Delta \rightsquigarrow^\ast \inter{a'}_\Delta$ with at least one step of $(\rightsquigarrow)$, or that reduction happens inside some $c_i$ in a subterm $\llet x {c_i} b$ of $a$. Either case, $\trans{a'}_\Delta$ is obtained from $\trans{a}_\Delta$ by reducing at least one element $\inter{c_i}_\Delta \in \trans{a}_\Delta$ for at least one step, and then in the case of $(\beta)$, adding a strictly smaller subterm (the argument) $\inter{d}_\Delta < \inter{c_i}_\Delta$.
\end{itemize}

Let $a \rightsquigarrow_\Gamma a_1 \rightsquigarrow_\Gamma a_2 \rightsquigarrow_\Gamma \ldots$ be an infinite reduction sequence starting at $a$. Since there are no infinite consecutive $(\delta)$ and $(\zeta)$ reductions (\cref{thm:def_delta_zeta_termination}), the other reductions $(\tau)$ $(\beta)$ $(\pi\mathsf{fst})$ $(\pi\mathsf{snd})$ appear infinitely often. By the discussion above, the elements of $\trans{a}_\Gamma \cup \trans{a_1}_\Gamma \cup \trans{a_2}_\Gamma \cup \ldots$ as vertices, together with the reduction relation $(\rightsquigarrow)$ and the subterm relation $(>)$ as edges, form an infinite binary tree (\cref{fig:def_reduction_tree}).

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \node[minimum width = 4em] (a) {$\inter{a}_\Gamma$};
    \node[minimum width = 4em, below = 0.2 of a] (c0) {$\inter{c_0}_\Gamma$};
    \node[minimum width = 4em, below = 0.2 of c0] (ci) {$\vdots$};
    \node[minimum width = 4em, below = 0.2 of ci] (cn) {$\inter{c_{n-1}}_\Gamma$};
    \node[right = 0.5 of a] (aa) {$\bullet$};
    \node[right = 0.5 of aa] (aaa) {$\bullet$};
    \node[right = 0.5 of aaa] (aaaa) {$\ldots$};
    \node[right = 0.5 of c0] (cc0) {$\bullet$};
    \node[right = 0.5 of cc0] (ccc0) {$\bullet$};
    \node[right = 0.5 of ccc0] (cccc0) {$\ldots$};
    \node[right = 0.5 of cn] (ccn) {$\bullet$};
    \node[right = 0.5 of ccn] (cccn) {$\bullet$};
    \node[right = 0.5 of cccn] (ccccn) {$\ldots$};
    \node[below = 0.5 of ccn] (ccnn) {$\bullet$};
    \node[right = 0.5 of ccnn] (cccnn) {$\bullet$};
    \node[right = 0.5 of cccnn] (ccccnn) {$\ldots$};
    \node[below = 0.5 of cccnn] (cccnnn) {$\bullet$};
    \node[right = 0.5 of cccnnn] (ccccnnn) {$\bullet$};
    \node[right = 0.5 of ccccnnn] (cccccnnn) {$\bullet$};
    \node[right = 0.5 of cccccnnn] (ccccccnnn) {$\bullet$};
    \node[right = 0.5 of ccccccnnn] (cccccccnnn) {$\ldots$};
    \node[below = 0.5 of cccccnnn] (cccccnnnn) {$\bullet$};
    \node[right = 0.5 of cccccnnnn] (ccccccnnnn) {$\bullet$};
    \node[right = 0.5 of ccccccnnnn] (cccccccnnnn) {$\ldots$};
    \foreach \i/\j in {a/aa, aa/aaa, aaa/aaaa, c0/cc0, cc0/ccc0, ccc0/cccc0, cn/ccn, ccn/cccn, cccn/ccccn, ccnn/cccnn, cccnn/ccccnn, cccnnn/ccccnnn, ccccnnn/cccccnnn, cccccnnn/ccccccnnn, ccccccnnn/cccccccnnn, cccccnnnn/ccccccnnnn, ccccccnnnn/cccccccnnnn} {
        \path (\i) -- node[sloped, fill = white]{$\rightsquigarrow$} (\j);
    }
    \foreach \i/\j in {cn/ccnn, ccnn/cccnnn, ccccnnn/cccccnnnn} {
        \draw (\i) -- node[sloped, fill = white]{$>$} (\j);
    }
    \draw[thick, dotted] (a.north west) rectangle (cn.south east);
    \node[below left = 0 of c0] {$\{a\}_\Gamma = {}$};
    \end{tikzpicture}
\caption{Example reduction tree, where $(\bullet)$ denote terms.}
\label{fig:def_reduction_tree}
\end{figure}

As a corollary to KÅnig's infinity lemma \cite{konig1927schlussweise}, every finitely-branching infinite tree contains an infinite path starting at the root. This means we can choose an infinite sequence $\inter{c_i}_\Gamma \to \inter{d}_\Gamma \to \ldots$ where $\inter{c_i}_\Gamma \in \trans{a}_\Gamma$ and each $(\to)$ can be either $(\rightsquigarrow)$ or $(>)$. Because the sizes of terms strictly decrease along $(>)$, there can be no infinite consecutive chains of $(>)$, so $(\rightsquigarrow)$ appear infinitely often. Since a reduction on a subterm translates to a reduction on the original term, every $(\rightsquigarrow)$ in this path can be pulled back along $(>)$, forming an infinite reduction sequence starting at $\inter{c_i}_\Gamma$.

By soundness (\cref{thm:def_soundness}), $\inter{\Gamma} \vdasho \inter{a}_\Gamma : \inter{A}_\Gamma$ so $\inter{a}_\Gamma$ is well-typed under $(\vdasho)$. Furthermore, the definition of $(\vdashd)$ ensures all subterms of a well-typed term are well-typed. By soundness and definition of $\trans{a}_\Gamma$, every $c_i\in\trans{a}_\Gamma$ is well-typed under $(\vdasho)$. Then, strong normalisation of the original system (\cref{thm:red_termination}) implies that there can be no infinite reduction sequence of $(\rightsquigarrow)$ starting at every $\inter{c_i}_\Gamma \in \trans{a}_\Gamma$. This contradicts with the previous paragraph. Therefore, an infinite reduction sequence of $(\rightsquigarrow_\Gamma)$ cannot exist.
\end{proof}

\section{Bidirectional type checking}
\label{sec:bidirectional_type_checking}

Due to the lack of type annotations on arguments of function and pair constructors, as well as universe cumulativity, the typing judgments (\cref{fig:def_typing}) do \emph{not} uniquely determine the type of every well-typed term (e.g. $(\lam A {\lam a a})$ and $\mathcal U_i$). As a result, direct type checking in such cases requires type inference, which may be undecidable. However, if there are enough type annotations, a simple and deterministic \emph{bidirectional type-checking} algorithm can be employed.

Specifically, terms can be tagged as \emph{inferable} or \emph{checkable}, such that every inferable term is also checkable:

\begin{definition}[Bidirectional terms]
\label{def:bidirectional_terms}
The bidirectionally-inferable terms $a_+$ and the bidirectionally-checkable terms $a_-$ are defined by: $$
\begin{aligned}
A_+,B_+,a_+,b_+ ::=
    & \phantom{{}\mid{}} \mathcal U_{i \in \mathbb N} \mid x \mid {a_-} : {A_+} \mid \llet x {a_+} {b_+} \\
    & \mid \ppi x {A_+} {B_+} \mid \app {a_+} {b_-} \\
    & \mid \sig x {A_+} {B_+} \mid \fst {a_+} \mid \snd {a_+} \\
    & \mid \unit \\
a_-,b_- ::=
    & \phantom{{}\mid{}} a_+ \mid \llet x {a_+} {b_-} \\
    & \mid \lam x {a_-} \\
    & \mid \pair {a_-} {b_-} \\
    & \mid \sstar \\
\end{aligned}
$$
\end{definition}

\emph{Not all terms are covered by this definition:} in order to put a checkable term in a place where an inferable term is expected, a type annotation must be inserted around it. Since the only checkable terms are constructors, this rule may be summarised as `always put type annotations around constructor forms'.

For each typing rule in \cref{fig:def_typing}, if the subject in the conclusion is an inferable term, its \emph{most specific type} (which usually means its \emph{unique type}, unless the type is some universe, when we give $\mathcal U_i$ with the smallest possible $i\in\mathbb N$) can be computed and subsequently \emph{output} by the algorithm; or an expected type may still be supplied as an \emph{input} to the algorithm, in which case the term's most specific type is still calculated, but an error is raised unless the calculated type is a subtype of the provided type (implicitly applying the conversion rule). Otherwise, the subject is only a checkable term, and its expected type must be given as an \emph{input} to the algorithm.

This process is typically implemented as two mutually recursive functions, \texttt{infer} and \texttt{check}, where the former takes in an inferable term and returns its type, and the latter takes in an checkable term together with its expected type and returns if type checking succeeds.

The expected types supplied along with function and pair constructors provide necessary type information for the arguments, allowing one to choose at most one possible rule in \cref{fig:def_typing} to type them. At such, we say that the bidirectional type checking algorithm is \emph{syntax-directed}.

To simplify the presentation, we did not annotate \cref{fig:def_typing} with $+$'s and $-$'s, and will not do so explicitly except inside this section; the idea nevertheless generalises to all following discussions.

\section{From pairs to tuples}
\label{sec:from_pairs_to_tuples}

The $\Pi$ and $\Sigma$ types are very expressive abstractions, to a point that entire mathematical theories containing multiple assumptions, types, definitions and proofs can be packaged into a single term. Zenith assumes that each file contains a single \emph{module}, which is defined as a single term. The \emph{public interface} of the module is defined as its type.

For such a simplification to actually work, further optimisation is required. Instead of dependent pairs, Zenith implements native support for \emph{dependent tuples} \cite{pollack2000dependently}:

\begin{definition}[Dependent tuple type]
\label{def:dependent_tuple_types}
The notation $\Sigma(x_0. B_0, \ldots, x_{n-1}. B_{n-1})$ is defined as: $$\sig {x_{n-1}} {(\ldots (\sig {x_1} {(\sig {x_0} {\unit} {B_0})} {B_1}) \ldots)} {B_{n-1}}$$
\end{definition}

\begin{definition}[Dependent tuple constructor]
\label{def:dependent_tuple_constructors}
The notation $\langle x_0. b_0, \ldots, x_{n-1}. b_{n-1}\rangle$ is defined as: $$\llet {x_{n-1}} {\ldots \llet {x_1} {\llet {x_0} {\sstar} {\pair {x_0} {b_0}}} {\pair {x_1} {b_1}} \ldots} {\pair {x_{n-1}} {b_{n-1}}}$$
\end{definition}

In other words, tuples are \emph{left-associative} lists made from pairs and the unit type. Moreover, a tuple constructor allows referring to the \emph{initial segment} of length $i$ while defining the $i$-th entry. As a result, this allows referring to all previous entries in the definition of the $i$-th entry. The reduction and typing rules derived from this definition is given in \cref{fig:tuple_rules}.

\begin{figure}[ht]
    $\boxed{a \rightsquigarrow_\Delta b}$

    \begin{prooftree}
    \AxiomC{$i\in\{0, \ldots, n-1\}$}
    \AxiomC{$b_i \rightsquigarrow_{\Delta, (x_i \defeq \langle \ldots, x_{i-1}. b_{i-1} \rangle)} b_i'$}
    \RightLabel{$(\mathsf{cong})$}\BinaryInfC{$\langle \ldots, x_i. b_i, \ldots, x_{n-1}. b_{n-1}\rangle \rightsquigarrow_\Delta \langle \ldots, x_i. b_i', \ldots, x_{n-1}. b_{n-1}\rangle$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$i\in\{0, \ldots, n\}$}
    \RightLabel{$(\pi\mathsf{init})$}\UnaryInfC{$\fst^i \langle \ldots, x_{n-1}. b_{n-1}\rangle \rightsquigarrow_\Delta \langle \ldots, x_{n-i-1}. b_{n-i-1}\rangle$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$i\in\{0, \ldots, n-1\}$}
    \RightLabel{$(\pi\mathsf{proj})$}\UnaryInfC{$\snd \fst^i \langle \ldots, x_{n-1}. b_{n-1}\rangle \rightsquigarrow_\Delta \llet {x_{n-1}} {\langle \ldots, x_{n-i-2}. b_{n-i-2}\rangle} {b_{n-i-1}}$}
    \end{prooftree}

    \vspace{1em}
    $\boxed{\Gamma \vdashd a : A}$

    \begin{prooftree}
    \AxiomC{$
        \begin{array}{c}
            \Gamma \vdashd \Sigma (\ldots, x_{n-1}. B_{n-1}) : \mathcal U_i \\
            \Gamma, (x : \Sigma (\ldots, x_{n-1}. B_{n-1})) \vdashd B_n : \mathcal U_i \\
        \end{array}
    $}
    \RightLabel{$(\Sigma\mathsf{form})$}\UnaryInfC{$\Gamma \vdashd \Sigma (\ldots, x_n. B_n) : \mathcal U_i$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$
        \begin{array}{c}
            \Gamma \vdashd \langle \ldots, x_{n-1}. b_{n-1}\rangle : \Sigma (\ldots, x_{n-1}. B_{n-1}) \\
            \Gamma, (x_n \defeq \langle \ldots, x_{n-1}. b_{n-1}\rangle : \Sigma (\ldots, x_{n-1}. B_{n-1})) \vdashd b_n : B_n \\
            \Gamma \vdashd \Sigma (\ldots, x_n. B_n) : \mathcal U_i \\
        \end{array}
    $}
    \RightLabel{$(\Sigma\mathsf{intro})$}\UnaryInfC{$\Gamma \vdashd \langle \ldots, x_n. b_n\rangle : \Sigma (\ldots, x_n. B_n)$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd a : \Sigma (\ldots, x_{n-1}. B_{n-1})$}
    \AxiomC{$i \in \{0, \ldots, n\}$}
    \RightLabel{$(\mathsf{\Sigma init})$}\BinaryInfC{$\Gamma \vdashd \fst^i a : \Sigma (\ldots, x_{n-i-1}. B_{n-i-1})$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \vdashd a : \Sigma (\ldots, x_{n-1}. B_{n-1})$}
    \AxiomC{$i \in \{0, \ldots, n-1\}$}
    \RightLabel{$(\mathsf{\Sigma proj})$}\BinaryInfC{$\Gamma \vdashd \snd \fst^i a : \llet {x_{n-i-1}} {\fst^{i+1} a} B_{n-i-1}$}
    \end{prooftree}

\caption{Derivable rules for tuples}
\label{fig:tuple_rules}
\end{figure}

\begin{proposition}
\label{thm:tuple_rules}
The rules for dependent tuples listed in \cref{fig:tuple_rules} are admissible under \cref{fig:def_syntax,fig:def_typing}.
\end{proposition}

\begin{proof}
Expand \cref{def:dependent_tuple_types,def:dependent_tuple_constructors}, the rest is easy to check using the corresponding rules for dependent pairs.
\end{proof}

These rules permit the implementation of tuples as contiguous arrays of terms, and moreover constant-time projection (relative to the size of the tuple) in both values and types. In contrast, if tuples are defined as \emph{right-associative} lists, typing a projection on a variable of tuple type requires linear time, as we will have to substitute in preceding elements one-by-one.

Note that $(\Sigma\mathsf{intro})$ basically says `to type check a tuple, type check elements one-by-one, where the definition for each element can refer to previous elements', as one would expect in any ITP supporting multiple definitions in a single module.

With all the tuple machinery, this is how a Zenith module would look like: $$M_i\ \defeq\ (\lambda \underbrace{\ldots}_{\substack{\text{postulate names,}\\\text{parameter names}}}.\ \langle \underbrace{\ldots}_{\substack{\text{definitions,}\\\text{proofs}}} \rangle) : (\Pi \underbrace{\ldots}_{\substack{\text{postulates,}\\\text{parameters}}}.\ \Sigma(\underbrace{\ldots}_{\substack{\text{types,}\\\text{theorems}}}))$$

Modules can import each other, in which case the imported modules are represented as free variables $x_j$ in $M_i$. Then, type checking a module amounts to type checking the following closed term: $$
\begin{aligned}
& \llet {m_0} {M_0} {} \\
& \llet {m_1} {\llet {x_0} {m_0} {M_1}} {} \\
& \llet {m_2} {\llet {x_0} {m_0} {\llet {x_1} {m_1} {M_2}}} {} \\
& \ldots \\
& m_n
\end{aligned}
$$

The ordering of all referenced modules, as well as the assignments of $m_i$'s to $x_j$'s, are resolved before the closed term is produced and sent to the kernel for checking. This arrangement avoids duplicate work in the presence of diamond-shaped import graphs.

Although programming languages typically have module systems separate from their type systems, including in Agda \cite{norell2007towards} which cited this as an intentional design choice, we argue that a dedicated theorem prover would potentially benefit from the simplicity like presented here. Not only it simplifies the meta-theory and kernel implementation, but also tooling and interaction, by making no distinction between `global' and `local' contexts and allowing the same code and optimisations to work on both.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Some ingredients}
\label{sec:ingredients}

Having the stage set by the previous chapters, we will now borrow ideas from the optimised proof systems of ATPs to improve proof search efficiency in the type theory. This chapter will introduce some of the ideas and prepare for their integration.

\section{An example proof system}
\label{sec:example_proof_system}

To help getting a better understanding, we will first present the ideas in a fragment of the intuitionistic first-order logic \cite{gentzen1964investigations} containing only the logical connectives $(\to)$ $(\forall)$ $(\land)$ and $(\exists)$.

\subsection*{Natural deduction}

We take the following natural deduction proof system:
\begin{prooftree}
\AxiomC{}
\RightLabel{$(\mathsf{init})$}\UnaryInfC{$(\Gamma \ni A) \vdash A$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma, A \vdash B$}
\RightLabel{$({\to}\mathsf{intro})$}\UnaryInfC{$\Gamma \vdash A \to B$}
\DisplayProof
\qquad
\AxiomC{$\Gamma \vdash A \to B$}
\AxiomC{$\Gamma \vdash A$}
\RightLabel{$({\to}\mathsf{elim})$}\BinaryInfC{$\Gamma \vdash B$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma, x \vdash A$}
\RightLabel{$({\forall}\mathsf{intro})$}\UnaryInfC{$\Gamma \vdash \forall x. A$}
\DisplayProof
\qquad
\AxiomC{$\Gamma \vdash \forall x. A$}
\AxiomC{$\Gamma \vdash t$}
\RightLabel{$({\forall}\mathsf{elim})$}\BinaryInfC{$\Gamma \vdash \subst x t A$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma \vdash A$}
\AxiomC{$\Gamma \vdash B$}
\RightLabel{$({\land}\mathsf{intro})$}\BinaryInfC{$\Gamma \vdash A \land B$}
\DisplayProof
\qquad
\AxiomC{$\Gamma \vdash A \land B$}
\RightLabel{$({\land}\mathsf{fst})$}\UnaryInfC{$\Gamma \vdash A$}
\DisplayProof
\qquad
\AxiomC{$\Gamma \vdash A \land B$}
\RightLabel{$({\land}\mathsf{snd})$}\UnaryInfC{$\Gamma \vdash B$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma \vdash t$}
\AxiomC{$\Gamma \vdash \subst x t A$}
\RightLabel{$({\exists}\mathsf{intro})$}\BinaryInfC{$\Gamma \vdash \exists x. A$}
\DisplayProof
\qquad
\AxiomC{$\Gamma \vdash \exists x. A$}
\AxiomC{$\Gamma, x, A \vdash C$}
\RightLabel{$({\exists}\mathsf{elim})$}\BinaryInfC{$\Gamma \vdash C$}
\DisplayProof
\AxiomC{}
\end{prooftree}
where $(\Gamma \ni A)$ denotes some context $\Gamma$ containing $A$\footnote{For clearer formatting, especially when the sequents become long and we still want to preserve the entry $A$ in the premises.}, the notation $(\Gamma, x \vdash A)$ signifies that the variable name $x$ is not already present in $\Gamma$, and $(\Gamma \vdash t)$ states that $t$ is well-formed term with free variables in $\Gamma$.

\subsection*{Sequent calculus}

The fundamental idea common to every efficient proof system is \emph{intercalation} \cite{pfenning2004automated}: to apply constructors \emph{backwards}, guided by the expected types, and to apply eliminators \emph{forwards}, guided by the variable types. This is reminiscent of bidirectionality in type checking (\cref{sec:bidirectional_type_checking}), which avoids `unmotivated' choices of inference rules, reducing the number of possible choices at every step to a tractable level.

This idea results in a \emph{sequent calculus} \cite{gentzen1964investigations} proof system. Intercalated or bidirectional proof search in a natural deduction system becomes unidirectional in a sequent calculus system, which can be a more convenient presentation. % \footnote{An original version of the report attempted to specify the tableau system using the natural-deduction-like $(\vdashd)$, which took twice as long as the current iteration.}

Its main difference from natural deduction is the elimination rules: the $({\to}\mathsf{elim})$ $({\forall}\mathsf{elim})$ $({\land}\mathsf{fst})$ $({\land}\mathsf{snd})$ $({\exists}\mathsf{elim})$ rules are reformulated as
\begin{prooftree}
\AxiomC{$\Gamma \vdash A$}
\AxiomC{$\Gamma, B \vdash C$}
\RightLabel{$({\to}\mathsf{elim})$}\BinaryInfC{$(\Gamma \ni (A \to B)) \vdash C$}
\DisplayProof
\qquad
\AxiomC{$\Gamma \vdash t$}
\AxiomC{$\Gamma, \subst x t A \vdash C$}
\RightLabel{$({\forall}\mathsf{elim})$}\BinaryInfC{$(\Gamma \ni (\forall x. A)) \vdash C$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma, A, B \vdash C$}
\RightLabel{$({\land}\mathsf{elim})$}\UnaryInfC{$(\Gamma \ni (A \land B)) \vdash C$}
\DisplayProof
\qquad
\AxiomC{$\Gamma, x, A \vdash C$}
\RightLabel{$({\exists}\mathsf{elim})$}\UnaryInfC{$(\Gamma \ni (\exists x. A)) \vdash C$}
\DisplayProof
\AxiomC{}
\end{prooftree}
Instead of getting a component directly from $(A \to B)$ or $(A \land B)$, for example, the new rules explicitly allows us to prove whatever $C$ from the components we may get.

This idea can be generalised to dependent type theory: we simply need to combine $(\to)$ and $(\forall)$ into $(\Pi)$, combine $(\land)$ and $(\exists)$ into $(\Sigma)$, and annotate them with appropriate proof terms. Then we obtain the new formulations of $(\Pi\mathsf{elim})$ $(\Sigma\mathsf{fst})$ $(\Sigma\mathsf{snd})$ rules from \cref{fig:def_typing}:
\begin{prooftree}
    \AxiomC{$\Gamma \vdashd a : A$}
    \AxiomC{$\Gamma, (y \defeq \app b a : \llet x a B) \vdashd c : C$}
    \RightLabel{$(\Pi\mathsf{elim})$}\BinaryInfC{$\Gamma \ni (b : (\ppi x A B)) \vdashd \llet y {\app b a} c : C$}
\end{prooftree}
\begin{prooftree}
    \AxiomC{$\Gamma, (y \defeq \fst a : A), (z \defeq \snd a : \llet x {\fst a} B) \vdashd c : C$}
    \RightLabel{$(\Sigma\mathsf{elim})$}\UnaryInfC{$\Gamma \ni (a : (\sig x A B)) \vdashd \llet y {\fst a} {\llet z {\snd a} c} : C$}
\end{prooftree}
where $y, z \not\in (\free C)$ are fresh variable names. They are easily shown to be derivable under \cref{fig:def_typing}, by the application of $(\mathsf{define})$ $(\mathsf{let})$ $(\mathsf{conv})$ rules.

However, these rules in the example are still far from efficient. One problem is due to $({\forall}\mathsf{elim})$ and $({\exists}\mathsf{intro})$, where we need to come up with an arbitrary term $t$ which needs to fit exactly into the part of the proof we have not yet constructed. Directly enumerating well-formed terms is obviously infeasible, since it introduces huge search space at a single step. The solution is to use \emph{holes} (\cref{sec:holes_and_signatures}) as placeholders for $t$ whose content can be determined later, when we actually need them to be in particular shapes. Finding hole solutions with expected shapes is the process of \emph{unification} (\cref{sec:higher_order_unification}).

Another problem is the need for \emph{deep backtracking}. This is caused by the $({\to}\mathsf{elim})$ rule: the left branch $(\Gamma \vdash A)$ might be unprovable even if the goal $(\Gamma, A\to B \vdash C)$ is provable, so we might \emph{regret} applying this rule some time later. If that happens, we are tempted to discard all our work after the point we applied $({\to}\mathsf{elim})$, and restart proof search from there. Intuitively however, not all the work must be discarded if we had better `dependency analysis': for example, derivations in the left branch which do not ever touch the new goal $A$ can be saved.

% This is obvious if we perform bidirectional proof search in a natural deduction system, where a sequence of eliminations is viewed as a `floating' derivation tree, which can be discarded at will if not needed, but can be plugged into multiple places in the main derivation tree otherwise. Such flexibility becomes less clear in a sequent calculus formulation. Nevertheless, even in natural deduction, not all floating derivation trees can be plugged in every place in the main derivation tree -- the context in the conclusion of the floating tree must be \emph{subsumed} in the context of the hole in the main tree. We need some way to keep track of this subsumption relationship.

\subsection*{Labelled sequent calculus}

The problem of regret was solved for classical logic using \emph{multiple succedents}, when the sequent calculus was first invented \cite{gentzen1964investigations}. The idea was generalised to non-classical logics in \cite{wallen1987automated}. As suggested in \cite{waaler2001connections}, the first step is to formulate a \emph{labelled sequent calculus} which enjoys better permutation properties, eliminating regret.

For intuitionistic systems, the \emph{labels} $u, v$ refer to lists of variable names, for which we write $(uv)$ for list concatenation. Labelled contexts $\Gamma, \Delta$ have every entry annotated with a label superscript. Adopting Barendregt's convention that two variables of different identities have different names, the labelled sequent calculus rules for the example logic can be written down:
\begin{prooftree}
\AxiomC{}
\RightLabel{$(\mathsf{init})$}\UnaryInfC{$(\Gamma \ni A^u) \vdashs (\Delta \ni A^{uv})$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma, A^{ux} \vdashs \Delta, B^{ux}$}
\RightLabel{$({\to}\mathsf{intro})$}\UnaryInfC{$\Gamma \vdashs (\Delta \ni (A \to B)^u)$}
\DisplayProof
\qquad
\AxiomC{$\Gamma \vdashs \Delta, A^{uv}$}
\AxiomC{$\Gamma, B^{uv} \vdashs \Delta$}
\RightLabel{$({\to}\mathsf{elim})$}\BinaryInfC{$(\Gamma \ni (A \to B)^u) \vdashs \Delta$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma, x^{ux} \vdashs \Delta, A^{ux}$}
\RightLabel{$({\forall}\mathsf{intro})$}\UnaryInfC{$\Gamma \vdashs (\Delta \ni (\forall x. A)^u)$}
\DisplayProof
\qquad
\AxiomC{$uv \vdash t$}
\AxiomC{$\Gamma, \subst x t A^{uv} \vdashs \Delta$}
\RightLabel{$({\forall}\mathsf{elim})$}\BinaryInfC{$(\Gamma \ni (\forall x. A)^u) \vdashs \Delta$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma \vdashs \Delta, A^u$}
\AxiomC{$\Gamma \vdashs \Delta, B^u$}
\RightLabel{$({\land}\mathsf{intro})$}\BinaryInfC{$\Gamma \vdashs (\Delta \ni (A \land B)^u)$}
\DisplayProof
\qquad
\AxiomC{$\Gamma, A^u, B^u \vdashs \Delta$}
\RightLabel{$({\land}\mathsf{elim})$}\UnaryInfC{$(\Gamma \ni (A \land B)^u) \vdashs \Delta$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$u \vdash t$}
\AxiomC{$\Gamma \vdashs \Delta, {\subst x t A}^u$}
\RightLabel{$({\exists}\mathsf{intro})$}\BinaryInfC{$\Gamma \vdashs (\Delta \ni (\exists x. A)^u)$}
\DisplayProof
\qquad
\AxiomC{$\Gamma, x^u, A^u \vdashs \Delta$}
\RightLabel{$({\exists}\mathsf{elim})$}\UnaryInfC{$(\Gamma \ni (\exists x. A)^u) \vdashs \Delta$}
\DisplayProof
\AxiomC{}
\end{prooftree}

Here the intended meaning of a sequent $(\Gamma \vdashs \Delta)$ is that `$\Gamma$ proves one of $\Delta$'. The labels themselves can be understood as explicit annotations of \emph{scopes}:\footnote{The scopes are elements of the Kripke frame of intuitionistic logic. Another name for them is \emph{possible worlds}.} every assumption generated by $({\to}\mathsf{intro})$ and $({\forall}\mathsf{intro})$ opens a new scope, every theorem obtained by an elimination rule stays in the same scope as the premise \emph{(or optionally, weakened to a narrower scope)}, and an assumption from a narrower scope cannot be used to prove a goal in a wider scope.

If we remove the scope restriction in $(\mathsf{init})$, it will become a proof system for classical logic. Notice that the Peirce's law in classical logic, $((A \to \bot) \to A) \to A$, gives us the assumption $(A \to \bot)^u$ for free for the construction of $A^u$. Since $(A \to \bot)^u$ is an assumption, it can be used in a narrower scope, giving us $\bot^{uv}$ to close the branch whenever we have $A^{uv}$.

Effectively, Peirce's law permits arbitrary `long jumps' out of layers of scopes, or under the Curry-Howard analogy, `calling a function to return to an outer scope'. In type-theoretic settings, this has given rise to reduction rules which interpret it as the \emph{call-with-current-continuation} operator \cite{griffin1989formulae}. The assumption of type $(A \to \bot)^u$ can then be understood as a token for the \emph{continuation} at the place where Peirce's law is invoked. The extension of this operator with reduction rules and dependent types was investigated in David Davies' final year project \cite{david2021theorem}.

\subsection*{Matrix representation}

\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}

Observe that in branching rules like $(\land\mathsf{intro})$, both premise sequents include the common parts $\Gamma$ and $\Delta$. As a result, a simple representation of derivations may contain significant redundancy. A \emph{matrix} representation \cite{andrews1981theorem,bibel1981matrices,waaler2001connections} aims to enable sharing of such common parts.

Matrices $M, N$ are structures generated by labelled signed formulas, rows and columns:
$$
M, N ::= A^u_- \mid A^u_+ \mid \mat{M & N} \mid \mat{M \\ N}
$$
where negative and positive formulas represent antecedents and succedents, respectively. A \emph{path} going horizontally through a matrix corresponds one-to-one to a leaf sequent. The matrices $\mat{M_0 & \mat{M_1 & M_2}}$ and $\mat{\mat{M_0 & M_1} & M_2}$ have the same set of paths through them, so they are considered equivalent and written simply as $\mat{M_0 & M_1 & M_2}$. Same goes for column matrices:
$$
\mat{M_0 \\ \mat{M_1 \\ M_2}} = \mat{\mat{M_0 \\ M_1} \\ M_2} = \mat{M_0 \\ M_1 \\ M_2}
$$
With the matrix representation, applications of the rule $(\mathsf{init})$ become horizontal \emph{connections} of two formulas $A^u_-$ and $A^{uv}_+$. A finished proof should have every path containing at least one connection; in this case, we say that connections \emph{cover} or \emph{span} all paths. The other inference rules become matrix expansion rules:
$$
\begin{aligned}
& (A \to B)^u_+ && \rightsquigarrow && \mat{\shade{(A \to B)^u_+} & A^{ux}_- & B^{ux}_+} && ({\to}\mathsf{intro})\\
& (A \to B)^u_- && \rightsquigarrow && \mat{\shade{(A \to B)^u_-} & \mat{A^{uv}_+ \\ B^{uv}_-}} && ({\to}\mathsf{elim})\\
& (\forall x. A)^u_+ && \rightsquigarrow && \mat{\shade{(\forall x. A)^u_+} & A^{ux}_+} && ({\forall}\mathsf{intro})\\
& (\forall x. A)^u_- && \rightsquigarrow && \mat{\shade{(\forall x. A)^u_-} & \subst x t {A^{uv}_-}} && ({\forall}\mathsf{elim})\\
& (A \land B)^u_+ && \rightsquigarrow && \mat{\shade{(A \land B)^u_+} & \mat{A^u_+ \\ B^u_+}} && ({\land}\mathsf{intro})\\
& (A \land B)^u_- && \rightsquigarrow && \mat{\shade{(A \land B)^u_-} & A^u_- & B^u_-} && ({\land}\mathsf{elim})\\
& (\exists x. A)^u_+ && \rightsquigarrow && \mat{\shade{(\exists x. A)^u_+} & \subst x t {A^u_+}} && ({\exists}\mathsf{intro})\\
& (\exists x. A)^u_- && \rightsquigarrow && \mat{\shade{(\exists x. A)^u_-} & A^u_-} && ({\exists}\mathsf{elim})\\
\end{aligned}
$$
The gray parts are optional; they are only strictly necessary for the `generative' rules $(\forall\mathsf{elim})$ and $(\exists\mathsf{intro})$. A matrix proof is considered valid iff it is obtained via expansions and every horizontal path through it contains some connection. This definition is sound and complete, since we can always extract a \emph{symmetric} sequent calculus derivation for every valid matrix proof, and every sequent calculus derivation can be turned into some matrix proof.

As a concrete example, here are labelled sequent calculus and matrix proofs for the theorem $(A \to (A \to B) \to (A \land B))$. Copied but unused entries are omitted as `\ldots' for easier formatting. Note how the four branches in the symmetric sequent calculus derivation are compressed into four overlapping horizontal paths through the matrix.

\begin{figure}[ht]
    \centering
    \resizebox{\textwidth}{!}{
        \AxiomC{}
        \RightLabel{$(\mathsf{init})$}\UnaryInfC{$\textcolor{blue}{A^x}, (A \to B)^{xy} \vdashs \ldots \textcolor{blue}{A^{xy}}$}
        \AxiomC{}
        \RightLabel{$(\mathsf{init})$}\UnaryInfC{$\textcolor{green}{A^x} \ldots \vdashs \ldots B^{xy}, \textcolor{green}{A^{xy}}$}
        \AxiomC{}
        \RightLabel{$(\mathsf{init})$}\UnaryInfC{$A^x \ldots \textcolor{red}{B^{xy}} \vdashs \ldots \textcolor{red}{B^{xy}}$}
        \RightLabel{$({\to}\mathsf{elim})$}\BinaryInfC{$A^x, (A \to B)^{xy} \vdashs \ldots B^{xy}$}
        \RightLabel{$({\land}\mathsf{intro})$}\BinaryInfC{$A^x, (A \to B)^{xy} \vdashs \ldots (A \land B)^{xy}$}
        \RightLabel{$({\to}\mathsf{intro})$}\UnaryInfC{$A^x \vdashs \ldots ((A \to B) \to (A \land B))^x$}
        \RightLabel{$({\to}\mathsf{intro})$}\UnaryInfC{$\epsilon \vdashs (A \to (A \to B) \to (A \land B))$}
        \DisplayProof
    }
\caption{Example sequent calculus derivation}
\label{fig:example_sequent_calculus}
\end{figure}

\begin{figure}[ht]
    \centering
    \resizebox{\textwidth}{!}{
        \AxiomC{}
        \RightLabel{$(\mathsf{init})$}\UnaryInfC{$\textcolor{blue}{A^x}\ldots \vdashs \ldots \textcolor{blue}{A^{xy}}, A^{xy}$}
        \AxiomC{}
        \RightLabel{$(\mathsf{init})$}\UnaryInfC{$\textcolor{blue}{A^x} \ldots B^{xy} \vdashs \ldots \textcolor{blue}{A^{xy}}$}
        \RightLabel{$({\to}\mathsf{elim})$}\BinaryInfC{$A^x, (A \to B)^{xy} \vdashs \ldots A^{xy}$}
        \AxiomC{}
        \RightLabel{$(\mathsf{init})$}\UnaryInfC{$\textcolor{green}{A^x} \ldots \vdashs \ldots B^{xy}, \textcolor{green}{A^{xy}}$}
        \AxiomC{}
        \RightLabel{$(\mathsf{init})$}\UnaryInfC{$A^x \ldots \textcolor{red}{B^{xy}} \vdashs \ldots \textcolor{red}{B^{xy}}$}
        \RightLabel{$({\to}\mathsf{elim})$}\BinaryInfC{$A^x, (A \to B)^{xy} \vdashs \ldots B^{xy}$}
        \RightLabel{$({\land}\mathsf{intro})$}\BinaryInfC{$A^x, (A \to B)^{xy} \vdashs \ldots (A \land B)^{xy}$}
        \RightLabel{$({\to}\mathsf{intro})$}\UnaryInfC{$A^x \vdashs \ldots ((A \to B) \to (A \land B))^x$}
        \RightLabel{$({\to}\mathsf{intro})$}\UnaryInfC{$\epsilon \vdashs (A \to (A \to B) \to (A \land B))$}
        \DisplayProof
    }
\caption{Example symmetric sequent calculus derivation}
\label{fig:example_symmetric_sequent_calculus}
\end{figure}

\begin{figure}[ht]
    \centering
    \vspace{1em}
    \resizebox{\textwidth}{!}{
        $\mat{
            (A \to (A \to B) \to (A \land B))_+ &
            \tikzmarknode{a}{A^x_-} &
            ((A \to B) \to (A \land B))^x_+ &
            (A \to B)^{xy}_- &
            \mat{\tikzmarknode{b}{A^{xy}_+} \\ \tikzmarknode{c}{B^{xy}_-}} &
            (A \land B)^{xy}_+ &
            \mat{\tikzmarknode{d}{A^{xy}_+} \\ \tikzmarknode{e}{B^{xy}_+}}
        }$
        \begin{tikzpicture}[remember picture,overlay]
            \draw[blue](a.north) to[bend left=10] (d.north);
            \draw[green](a.north) to[bend left=10] (b.north);
            \draw[red](c.south) to[bend right=10] (e.south);
        \end{tikzpicture}
    }
    \vspace{1em}
    \begin{framed}
    $$
    \begin{aligned}
        & (A \to (A \to B) \to (A \land B))_+,
        \textcolor{blue}{A^x_-},
        ((A \to B) \to (A \land B))^x_+,
        (A \to B)^{xy}_-,
        A^{xy}_+,
        (A \land B)^{xy}_+,
        \textcolor{blue}{A^{xy}_+}
        \\
        & (A \to (A \to B) \to (A \land B))_+,
        \textcolor{blue}{A^x_-},
        ((A \to B) \to (A \land B))^x_+,
        (A \to B)^{xy}_-,
        B^{xy}_-,
        (A \land B)^{xy}_+,
        \textcolor{blue}{A^{xy}_+}
        \\
        & (A \to (A \to B) \to (A \land B))_+,
        \textcolor{green}{A^x_-},
        ((A \to B) \to (A \land B))^x_+,
        (A \to B)^{xy}_-,
        \textcolor{green}{A^{xy}_+},
        (A \land B)^{xy}_+,
        B^{xy}_+
        \\
        & (A \to (A \to B) \to (A \land B))_+,
        A^x_-,
        ((A \to B) \to (A \land B))^x_+,
        (A \to B)^{xy}_-,
        \textcolor{red}{B^{xy}_-},
        (A \land B)^{xy}_+,
        \textcolor{red}{B^{xy}_+}
        \\
    \end{aligned}
    $$
    \end{framed}
\caption{Example matrix proof and its paths}
\label{fig:example_matrix}
\end{figure}

Finally, in a first-order logic, the arbitrary term $t$ in $({\forall}\mathsf{elim})$ and $({\exists}\mathsf{intro})$ rules can be determined via unification during connection search. However, in a dependent type theory, well-formed terms and their unification are a more complex issue -- they may require proof search themselves. Therefore, in \cref{sec:the_connection_system}, we will formulate the $\Pi$ and $\Sigma$ rules more like $(\to)$ and $(\land)$, but with a link between the two branches.

\section{Holes and signatures}
\label{sec:holes_and_signatures}

To remove one source of combinatorial explosion in proof search, we need to have holes in terms that can be filled in later. Both proof search and unification require them to be typed.

\begin{definition}[Syntax]
\label{def:hole_syntax}
Extending on the rules listed in \cref{fig:def_syntax}:
\begin{itemize}[noitemsep]
    \item The set of $a \termh$ is defined by the same rules as $a \termd$ plus
    $$
    a ::= {} \ldots \mid {?x}
    $$
    \item The set of $\Gamma \ctxh$ is defined the same as $\Gamma \ctxd$ but with elements $a, A \termh$.
    \item The set of $\Delta \envh$ is defined the same as $\Delta \envd$ but with elements $a, A \termh$.
\end{itemize}
\end{definition}

Each hole ${?x}$ is then externally associated with its context $\Gamma$ and an expected type $A$. Both are allowed to contain earlier holes, as specified in hole signatures defined below:

\begin{definition}[Signatures]
\label{def:hole_signatures}
A signature $\Sigma$ is a list of tuples $({?x_i}, \Gamma_i, A_i)$ where every hole in $\Gamma_i$ or $A_i$ is some $?x_j$ with $j < i$.
\end{definition}

To be able to reduce and type-check terms containing holes, we can either extend the type system defined in \cref{sec:type_theory_def} to support holes \cite{mcbride2000dependently}, or simply interpret holes using existing language constructs \cite{norell2007towards}. Here the former approach is used. The difference is that we do not introduce hole binders, or block all computation on any term containing holes like in \cite{mcbride2000dependently}. % \footnote{A previous iteration of this section adopted the latter for the conceived convenience of reusing results from \cref{sec:type_theory_def_properties}, but it required the conversion and typing relations to be defined in terms of interpretations, which did not have clear inductive structures for proving additional properties.}

\begin{definition}[Conversion rules]
\label{def:hole_conversion}
The relation $(\rightsquigarrow_\Delta)$ for terms with holes is inductively defined by all rules listed in \cref{fig:def_syntax}, where the free variable map $(\free)$ is is extended by
$$
\free {?x} \defeq \mathcal V
$$
where $\mathcal V$ denotes the set of all legal variable names. This blocks most $(\zeta)$ reductions around a hole.

Then, define $(\rightsquigarrow_\Delta^\ast)$ as its reflexive transitive closure, $(\equiv_\Delta)$ as its equivalence closure, and $(\leq_\Delta)$ as $(\equiv_\Delta)$ extended with universe cumulativity, all the same as in \cref{fig:def_syntax}. The relation $(\equiv)$ between contexts is defined the same as in \cref{def:def_context_conv}.
\end{definition}

\begin{definition}[Typing rules]
\label{def:hole_typing}
The relation $(\vdashd)$ for terms with holes is inductively defined by all rules listed in \cref{fig:def_typing}, with $(\mathsf{empty})$ modified as
\begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\mathsf{empty})$}\UnaryInfC{$\epsilon; \epsilon \vdashd \ok$}
\end{prooftree}
plus the additional rules for signatures and holes
\begin{prooftree}
    \AxiomC{$\Sigma; \Gamma \vdashd A : \mathcal U_i$}
    \RightLabel{$(\mathsf{claim})$}\UnaryInfC{$(\Sigma, ({?x}, \Gamma, A)); \epsilon \vdashd \ok$}
\end{prooftree}
\begin{prooftree}
    \AxiomC{$({?x}, \Gamma, A) \in \Sigma$}
    \AxiomC{$\Gamma \equiv \Gamma'$}
    \AxiomC{$\Gamma' \subseteq \Gamma''$}
    \AxiomC{$\Sigma; \Gamma'' \vdashd \ok$}
    \RightLabel{$(\mathsf{hole})$}\QuaternaryInfC{$\Sigma; \Gamma'' \vdashd {?x} : A$}
\end{prooftree}
where we allow typing a hole in any \emph{converted and extended} context $\Gamma''$ to preserve the subject reduction property. Informally, this is because reduction rules can reduce contexts, or copy holes to places under extended contexts.
\end{definition}

It is easy to see that these relations are conservative extensions of their counterparts defined in \cref{sec:type_theory_def}. Therefore, we can use the same notations without ambiguity.

% Like in \cref{sec:type_theory_def_properties}, we still need to establish basic properties for this new system. However, we will simply gloss over most of the details in this section.

\begin{proposition}[Confluence]
\label{thm:hole_red_confluence}
For any $\Delta \envh$ and $a,b,c \termh$ such that $a\rightsquigarrow_\Delta^\ast b$ and $a\rightsquigarrow_\Delta^\ast c$, exists $d \termh$ such that $b\rightsquigarrow_\Delta^\ast d$ and $c\rightsquigarrow_\Delta^\ast d$.
\end{proposition}

\begin{proof}
Apply similar technique as in the proofs of \cref{thm:red_confluence,thm:def_red_confluence}. Since we did not introduce reduction rules on holes, there are no new cases to consider.
\end{proof}

As a corollary, the properties \cref{thm:def_characterisation_of_defeq,thm:def_injectivity_of_type_formers} still hold for the extended $(\vdashd)$.

In showing subject reduction, it is easy to check \cref{thm:def_context} still holds. The new case of holes occurs non-trivially in the next lemmas:

\begin{proposition}[Weakening]
\label{thm:hole_weakening}
For any signature $\Sigma$ and $\Gamma, \Gamma' \ctxh$ and $a, b, A, B \termh$:
\begin{itemize}[noitemsep]
    \item If $\Sigma; \Gamma \vdashd A : \mathcal U_i$ and $\Sigma; (\Gamma, \Gamma') \vdashd b : B$, we have $\Sigma; (\Gamma, x : A, \Gamma') \vdashd b : B$.
    \item If $\Sigma; \Gamma \vdashd a : A$ and $\Sigma; (\Gamma, x : A, \Gamma') \vdashd b : B$, we have $\Sigma; (\Gamma, x \defeq a : A, \Gamma') \vdashd b : B$.
\end{itemize}
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdashd)$. The only new case is $b = {?x}$, where from the $(\mathsf{hole})$ rule, we can infer that $\Sigma; \Gamma \vdashd {?x} : A$ implies $\Sigma; \Gamma' \vdashd {?x} : A$ for any $\Gamma' \supseteq \Gamma$ such that $\Sigma; \Gamma' \vdashd \ok$.
\end{proof}

\begin{proposition}[Strengthening]
\label{thm:hole_strengthening}
For any signature $\Sigma$ and $\Gamma, \Gamma' \ctxh$ and $a, b, A, B \termh$:
\begin{itemize}[noitemsep]
    \item If $x \notin \free b \cup \free B \cup \free{\Gamma'}$ and $\Sigma; (\Gamma, x : A, \Gamma') \vdashd b : B$, we have $\Sigma; (\Gamma, \Gamma') \vdashd b : B$.
    \item If $x \notin \free b \cup \free B \cup \free{\Gamma'}$ and $\Sigma; (\Gamma, x \defeq a : A, \Gamma') \vdashd b : B$, we have $\Sigma; (\Gamma, \Gamma') \vdashd b : B$.
\end{itemize}
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdashd)$. The only new case is $b = {?x}$, where $(\free b) = (\free {?x}) = \mathcal V$ means that the case is vacuous.
\end{proof}

\begin{proposition}[Context conversion]
\label{thm:hole_context_conv}
For any signature $\Sigma$ and $\Gamma, \Gamma' \ctxh$ and $a, A \termh$ such that $\Sigma; \Gamma \vdashd a : A$ and $\Gamma \equiv \Gamma'$ and $\Sigma; \Gamma' \vdashd \ok$, we have $\Sigma; \Gamma' \vdashd a : A$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdashd)$. The only new case is $a = {?x}$, where from the $(\mathsf{hole})$ rule, we can infer that $\Sigma; \Gamma \vdashd {?x} : A$ implies $\Sigma; \Gamma' \vdashd {?x} : A$ for any $\Gamma' \equiv \Gamma$ such that $\Sigma; \Gamma' \vdashd \ok$.
\end{proof}

\begin{proposition}[Classification]
\label{thm:hole_classification}
For any signature $\Sigma$ and $\Gamma \ctxh$ and $a, A \termh$ such that $\Sigma; \Gamma \vdashd a : A$, we have $\Sigma; \Gamma \vdashd A : \mathcal U_i$ for some $i \in \mathbb N$.
\end{proposition}

\begin{proof}
By induction on the structure of $(\vdashd)$ in $\Sigma; \Gamma \vdashd a : A$, show that (i) $\Sigma; \Gamma \vdashd A : \mathcal U_i$ for some $i$, and (ii) for all $(x : B) \in \Gamma$, we have $\Sigma; \Gamma \vdashd B : \mathcal U_i$ for some $i$, and (iii) for any $({?x}, \Gamma, B) \in \Sigma$, we have $\Sigma; \Gamma \vdash B : \mathcal U_i$ for some $i$. The extension of the proofs of \cref{thm:classification,thm:def_classification} is otherwise straightforward.
\end{proof}

\begin{proposition}[Subject reduction]
\label{thm:hole_subject_reduction}
For any signature $\Sigma$, $\Gamma \ctxh$ and $a, a', A \termh$ such that $\Sigma; \Gamma \vdashd a : A$ and $a \rightsquigarrow_\Gamma a'$, we have $\Sigma; \Gamma \vdashd a' : A$.
\end{proposition}

\begin{proof}
Apply similar technique as in the proofs of \cref{thm:subject_reduction,thm:def_subject_reduction}. Since we did not introduce reduction rules on holes, there are no new cases to consider.
\end{proof}

Like how we have used the notation $\subst x a b$ to denote term substitutions for variables, we will use $\subst {?x} a b$ to denote term substitutions for holes. If the substitution $\subst {?x} a \cdot$ is applied to a signature $\Sigma$, the tuple for ${?x}$ in $\Sigma$ will be removed.

\begin{proposition}[Hole filling]
\label{thm:hole_filling}
For any signature $\Sigma$, $\Gamma \ctxh$, $\Delta \envh$ and $t, a, b, A, B \termh$:
\begin{itemize}[noitemsep]
    \item $a \rightsquigarrow_\Delta b$ implies $\subst {?x} t a \rightsquigarrow_{\subst {?x} t \Delta} \subst {?x} t b$.
    \item $a \rightsquigarrow_\Delta^\ast b$ implies $\subst {?x} t a \rightsquigarrow_{\subst {?x} t \Delta}^\ast \subst {?x} t b$.
    \item $a \equiv_\Delta b$ implies $\subst {?x} t a \equiv_{\subst {?x} t \Delta} \subst {?x} t b$.
    \item $a \leq_\Delta b$ implies $\subst {?x} t a \leq_{\subst {?x} t \Delta} \subst {?x} t b$.
\end{itemize}
If $\Sigma = (\Sigma_0, ({?x}, \Gamma', A), \Sigma_1)$ and $\Sigma_0; \Gamma' \vdashd a : A$, we say the substitution $\subst {?x} a {\cdot}$ is \emph{well-typed} and:
\begin{itemize}[noitemsep]
    \item $\Sigma; \Gamma \vdashd \ok$ implies $\subst {?x} a \Sigma; \subst {?x} a \Gamma \vdashd \ok$.
    \item $\Sigma; \Gamma \vdashd b : B$ implies $\subst {?x} a \Sigma; \subst {?x} a \Gamma \vdashd \subst {?x} a b : \subst {?x} a B$.
\end{itemize}
\end{proposition}

\begin{proof}
The first four can be shown by simple induction. In the last two, the only interesting case is $b = {?x}$, where we use the side condition $\Sigma_0; \Gamma' \vdashd a : A$, context conversion and weakening.
\end{proof}

Strong normalisation will be given a more careful proof. Each hole $?x_i$ can be interpreted as a \emph{top-level function} $x_i$ applied to every assumed variable (but not \emph{defined} variable) in $\Gamma_i$.

\newcommand{\abstr}{\operatorname{abstr}}
\newcommand{\apply}{\operatorname{apply}}

\begin{definition}[Lambda lifting]
\label{def:context_environment_abstr_apply}
For any $\Gamma \ctxh$ and $b \termh$, the operations $\abstr_{\Pi,\lambda}(\Gamma, b)$ and $\apply(\Gamma, b)$ are defined by recursion:
$$
\begin{aligned}
    \abstr_\Pi(\epsilon, b) &\defeq b \\
    \abstr_\Pi((\Gamma, x : A), b) &\defeq \abstr_\Pi(\Gamma, (\ppi x A b)) \\
    \abstr_\Pi((\Gamma, x \defeq a : A), b) &\defeq \abstr_\Pi(\Gamma, \llet x a b) \\[0.3em]
    \abstr_\lambda(\epsilon, b) &\defeq b \\
    \abstr_\lambda((\Gamma, x : A), b) &\defeq \abstr_\lambda(\Gamma, (\lam x b)) \\
    \abstr_\lambda((\Gamma, x \defeq a : A), b) &\defeq \abstr_\lambda(\Gamma, \llet x a b) \\[0.3em]
    \apply(\epsilon, b) &\defeq b \\
    \apply((\Gamma, x : A), b) &\defeq (\app {\apply(\Gamma, b)} x) \\
    \apply((\Gamma, x \defeq a : A), b) &\defeq \apply(\Gamma, b) \\
\end{aligned}
$$
\end{definition}

% Note that $\abstr_{\Pi,\lambda}(\Gamma, b)$ duplicate all definitions in $(\defn \Gamma)$ as let-bindings. This is to ensure that $b$ always has access to them.

\begin{proposition}
\label{thm:context_environment_abstr_apply}
For any $\Gamma_0, \Gamma \ctxd$ and $b, B \termd$:
\begin{itemize}[noitemsep]
    % \item $\apply(\Gamma, \abstr_\lambda(\Gamma, b)) \equiv_{(\Gamma_0, \Gamma)} b$.
    \item $\Gamma_0 \vdashd \abstr_\Pi(\Gamma, B) : \mathcal U_i$ for some $i \in \mathbb N$ iff $(\Gamma_0, \Gamma) \vdashd B : \mathcal U_j$ for some $j \in \mathbb N$.
    \item $\Gamma_0 \vdashd \abstr_\lambda(\Gamma, b) : \abstr_\Pi(\Gamma, B)$ iff $(\Gamma_0, \Gamma) \vdashd b : B$.
    \item $\Gamma_0 \vdashd b : \abstr_\Pi(\Gamma, B)$ iff $(\Gamma_0, \Gamma) \vdashd \apply(\Gamma, b) : B$.
\end{itemize}
\end{proposition}

\begin{proof}
All can be shown by induction on $\Gamma$. The inductive hypotheses are denoted by IH.
\begin{enumerate}
    % \item For $\Gamma = (\Gamma', x : A)$ we need to show $\app {(\lam x b)} x \equiv_{(\Gamma_0, \Gamma)} b$, where both sides reduce to $b$. For $\Gamma = (\Gamma', x \defeq a : A)$ we need $\llet x a b \equiv_{(\Gamma_0, \Gamma)} b$, where both sides reduce to $\subst x a b$.

    \item For $\Gamma = (\Gamma', x : A)$, the left side is equivalent to $(\Gamma_0, \Gamma') \vdashd (\ppi x A B) : \mathcal U_k$ by IH, which is furthermore equivalent to the right side by $(\Pi\mathsf{form})$ $(\mathsf{conv})$ and \cref{thm:def_classification}. For $\Gamma = (\Gamma', x \defeq a : A)$, the left side is equivalent to $(\Gamma_0, \Gamma') \vdashd \llet x a B : \mathcal U_k$ by IH, which is furthermore equivalent to the right side by $(\mathsf{let})$ and $(\mathsf{conv})$ rules.

    \item For $\Gamma = (\Gamma', x : A)$, the left side is equivalent to $(\Gamma_0, \Gamma') \vdashd (\lam x b) : (\ppi x A B)$ by IH, which is furthermore equivalent to the right side by $(\Pi\mathsf{intro})$ $(\mathsf{conv})$ and \cref{thm:def_injectivity_of_type_formers}. For $\Gamma = (\Gamma', x \defeq a : A)$, the left side is equivalent to $(\Gamma_0, \Gamma') \vdashd \llet x a b : \llet x a B$ by IH, which is furthermore equivalent to the right side by $(\mathsf{let})$ and $(\mathsf{conv})$ rules.

    \item For $\Gamma = (\Gamma', x : A)$, the left side is equivalent to $(\Gamma_0, \Gamma') \vdashd \apply(\Gamma', b) : (\ppi x A B)$ by IH, which is furthermore equivalent to the right side by $(\Pi\mathsf{elim})$ and $(\mathsf{var})$ rules. For $\Gamma = (\Gamma', x \defeq a : A)$, the left side is equivalent to $(\Gamma_0, \Gamma') \vdashd \apply(\Gamma', b) : \llet x a B$ by IH, which is furthermore equivalent to the right side by $(\mathsf{conv})$ rule. \qedhere
\end{enumerate}
\end{proof}

\begin{definition}[Signature interpretation]
\label{def:hole_signature_interpretations}
For any signature $\Sigma = (T_0, \ldots, T_{n-1})$, write each $T_i = ({?x_i}, \Gamma_i, A_i)$, and let $x_0, \ldots, x_{n-1}$ be fresh variable names. Then:
\begin{itemize}[noitemsep]
    \item The context $\Sigma^\ast$ is defined as $(x_0 : \abstr_\Pi(\Gamma_0, A_0)), \ldots, (x_{n-1} : \abstr_\Pi(\Gamma_{n-1}, A_{n-1}))$.
    \item The operation $\inter\cdot$ on terms, contexts or environments is defined as the recursive substitution $\subst {?x_i} {\inter{\apply(\Gamma_i, x_i)}} {(\cdot)}$. This is terminating since $\Gamma_i$ only contains holes $?x_j$ with $j < i$.
\end{itemize}
\end{definition}

\begin{proposition}
\label{thm:hole_interpretation_across_abstr_apply}
For any signature $\Sigma$ with interpretation $(\Sigma^\ast, \inter\cdot)$, for any $\Gamma \ctxh$ and $b \termh$:
\begin{itemize}[noitemsep]
    \item $\inter{\abstr_\Pi(\Gamma, b)} = \abstr_\Pi(\inter \Gamma, \inter b)$.
    \item $\inter{\abstr_\lambda(\Gamma, b)} = \abstr_\lambda(\inter \Gamma, \inter b)$.
    \item $\inter{\apply(\Gamma, b)} = \apply(\inter \Gamma, \inter b)$.
    \item $\inter{\inter b} = \inter b$.
\end{itemize}
\end{proposition}

\begin{proof}
The first three are by simple induction on $\Gamma$. The last one is by definition of $\inter\cdot$.
\end{proof}

\begin{proposition}[Soundness]
\label{thm:hole_interpretation_soundness}
For any signature $\Sigma$ with interpretation $(\Sigma^\ast, \inter\cdot)$, for any $\Delta \envh$ and $\Gamma \ctxh$ and $a, b, A \termh$:
\begin{itemize}[noitemsep]
    \item $a \rightsquigarrow_\Delta b$ implies $\inter a \rightsquigarrow_{\inter{\Delta}} \inter b$.
    \item $a \rightsquigarrow_\Delta^\ast b$ implies $\inter a \rightsquigarrow_{\inter{\Delta}}^\ast \inter b$.
    \item $a \equiv_\Delta b$ implies $\inter a \equiv_{\inter{\Delta}} \inter b$.
    \item $a \leq_\Delta b$ implies $\inter a \leq_{\inter{\Delta}} \inter b$.
    \item $\Sigma; \Gamma \vdashd \ok$ implies $\inter{\Sigma^\ast, \Gamma} \vdashd \ok$.
    \item $\Sigma; \Gamma \vdashd a : A$ implies $\inter{\Sigma^\ast, \Gamma} \vdashd \inter a : \inter A$.
\end{itemize}
\end{proposition}

\begin{proof}
The first four properties are clear by simple induction. The last two are simultaneously by induction on $(\vdashd)$ for terms with holes. Only new rules are non-trivial:
\begin{itemize}[noitemsep]
    \item The $(\mathsf{claim})$ rule directly follows from \cref{thm:context_environment_abstr_apply,thm:hole_interpretation_across_abstr_apply} and $(\mathsf{assume})$.
    \item For $(\mathsf{hole})$ we need to show $\inter{\Sigma^\ast, \Gamma} \vdashd \inter{\apply(\Gamma_i, x_i)} : \inter{A_i}$, where $x_i$ is declared in $\inter{\Sigma^\ast}$ with type $\inter{\abstr_\Pi(\Gamma_i, A_i)}$, such that $\Gamma_i \equiv \Gamma'$ for some $\Gamma' \subseteq \Gamma$. But again this follows from \cref{thm:context_environment_abstr_apply,thm:hole_interpretation_across_abstr_apply,thm:def_weakening,thm:def_context_conv}. \qedhere
\end{itemize}
\end{proof}

It is interesting to note \cite{nanevski2008contextual} internalises $(\abstr_\Pi)$ as contextual modal types representing \emph{code}, $(\abstr_\lambda)$ as \emph{quasi-quotations} and $(\apply)$ as \emph{executions}, resulting in a type system capable to express primitives of \emph{metaprogramming} while generalising the modal logic $\mathsf{S4}$. This was discussed in conjunction with the \emph{provability} modal logic $\mathsf{GL}$ in my supervisor Alyssa Renata's final year project \cite{alyssa2019provability}. For our present purpose, it is only important that strong normalisation is preserved:

\begin{proposition}[Strong normalisation]
\label{thm:hole_red_termination}
For any signature $\Sigma$, $\Gamma \ctxh$ and $a,A \termh$ such that $\Sigma; \Gamma \vdashd a : A$, there exists no infinite reduction sequence of $(\rightsquigarrow_\Gamma)$ which starts at $a$.
\end{proposition}

\begin{proof}
Let $(\Sigma^\ast, \inter\cdot)$ be an interpretation of $\Sigma$. The conditions implies $\inter{\Sigma^\ast, \Gamma} \vdashd \inter a : \inter A$ by \cref{thm:hole_interpretation_soundness}, so $\inter a$ is strongly normalising under $(\rightsquigarrow_{\inter{\Sigma^\ast, \Gamma}}) = (\rightsquigarrow_{\inter{\Gamma}})$ by \cref{thm:def_red_termination}, which implies $a$ is strongly normalising under $(\rightsquigarrow_\Gamma)$.
\end{proof}

% Finally, it worth emphasis that both writing definitions and proving theorems can be reduced to the problem of finding solutions for holes in a term. This process is usually incremental, and new holes may be generated as a part of a solution. But such operations will never accidentally destroy already completed parts of a proof:

% \begin{definition}[Signature extension]
% \label{def:hole_signature_extension}
% For well-typed signatures $\Sigma, \Sigma'$, we say that $\Sigma'$ \emph{extends} $\Sigma$ if $(\equiv_\Sigma) \subseteq (\equiv_{\Sigma'})$ and $(\vdash_\Sigma) \subseteq (\vdash_{\Sigma'})$.
% \end{definition}

% \begin{proposition}
% \label{thm:hole_signature_extension}
% For well-typed signatures $\Sigma, \Sigma'$, $\Sigma'$ extends $\Sigma$ if one of the following holds:
% \begin{itemize}[noitemsep]    
%     \item $\Sigma'$ can be obtained by adding a new hole with its type to $\Sigma$;
%     \item $\Sigma'$ can be obtained by adding a new solution to an unsolved hole in $\Sigma$;
%     \item $\Sigma'$ can be obtained by swapping two adjacent holes in $\Sigma$ which do not refer each other in their contexts, environments, types or solutions.
% \end{itemize}
% \end{proposition}

% \begin{proof}
% By unfolding the definition of $(\vdash_\Sigma)$ and then applying induction on $(\vdash)$ and $(\equiv_\Delta)$. The `no reference' condition in the last case is required for $\Sigma'$ to remain a well-typed signature, but is nevertheless written here for clarity.
% \end{proof}

\section{Higher-order unification}
\label{sec:higher_order_unification}

During proof construction by hole filling, we sometimes want to satisfy certain equations between types containing holes. For $\Delta \envh$ and $a, b \termh$, the form $(a \cong_\Delta b)$ or $(a \preceq_\Delta b)$ is also called a \emph{unification constraint}. Finding term substitutions for holes $\sigma = \subst {?x_{n-1}} {t_{n-1}} {\ldots \subst {?x_0} {t_0} \cdot}$ so that $(\sigma a \equiv_{\sigma \Delta} \sigma b)$ or $(\sigma a \leq_{\sigma \Delta} \sigma b)$ holds is the problem of \emph{higher-order unification}.

Despite the problem being only semi-decidable \cite{huet1973undecidability,goldfarb1981undecidability}, Huet's algorithm \cite{huet1975unification,elliott1990extensions} provides a general framework for efficiently dealing with unification constraints. It builds a `matching tree', which is basically a search tree whose nodes contain sets of unification constraints. Every search step can be one of the two actions: $\mathsf{simpl}$ and $\mathsf{match}$.

\subsection*{The $\mathsf{simpl}$ step}

A $\mathsf{simpl}$ step simplifies the set of constraints in some node, by repeatedly splitting a constraint $U$ into a set of \emph{equivalent} constraints $U_0, \ldots, U_{n-1}$ whenever this is possible.

\begin{example}
The unification constraint $(\ppi x A B) \cong_\Delta (\ppi x {A'} {B'})$ can be split into the equivalent set of constraints
$$
\begin{aligned}
A &\cong_\Delta A' \\
B &\cong_\Delta B' \\
\end{aligned}
$$
\end{example}

\begin{example}
The unification constraint $(\app {\app {\app f {a_0}} {\ldots}} {a_{n-1}}) \cong_\Delta (\app {\app {\app g {a_0'}} {\ldots}} {a_{n-1}'})$ where $f, g$ are variables can be split into the equivalent set of constraints
$$
\begin{aligned}
a_0 &\cong_\Delta a_0' \\
&\ldots \\
a_{n-1} &\cong_\Delta a_{n-1}' \\
\end{aligned}
$$
iff $f = g$. Otherwise, the original constraint is unsatisfiable.
\end{example}

Such splits can only happen when both sides of a constraint have `rigid heads', or `heads that never change under substitutions'. The exact definition will be given later. Otherwise, the resulting set of constraints may not be equivalent to the original one. Therefore, $\mathsf{simpl}$ is also known as \emph{rigid-rigid matching}.

\subsection*{The $\mathsf{match}$ step}

A $\mathsf{match}$ step deals with situations where one side of the constraint has `flexible head', while the other side has `rigid head'. At such, $\mathsf{match}$ is also known as \emph{flexible-rigid matching}. In our situation, having a `flexible head' means having a hole applied to zero or more arguments. We have to fill in the hole to match the other side of the constraint. There are two choices:

\begin{itemize}[noitemsep]
    \item The $\mathsf{imitation}$ rule fills the hole with the same rigid head as the other side.
    \item The $\mathsf{projection}$ rule fills the hole with a projection function, which returns one of its arguments. The argument may have the same rigid head as the other side.
\end{itemize}

\begin{example}
Consider the unification constraint $\app {\app {\app {?x} {a_0}} \ldots} {a_{p-1}} \cong_\Delta \app {\app {\app f {b_0}} \ldots} {b_{q-1}}$, where $f$ is some variable.

\begin{itemize}
    \item Using $\mathsf{imitation}$, it can be handled by first filling
    $$
    {?x} \defeq
        \lam {x_0} {\ldots \lam {x_{p-k-1}} {
            (\app {\app f {?y_0}} {\ldots {?y_{q-k-1}}})
        }}
    $$
    for some $k \leq p,q$,\footnote{If $(\eta)$-conversion is admitted, we only need to consider $k = 0$.} and then splitting into
    $$
    \begin{aligned}
    \llet {x_0} {a_0} {\ldots \llet {x_{p-k-1}} {a_{p-k-1}} {?y_0}} &\cong_\Delta b_0 \\
      & \ldots \\
    \llet {x_0} {a_0} {\ldots \llet {x_{p-k-1}} {a_{p-k-1}} {?y_{q-k-1}}} &\cong_\Delta b_{q-k-1} \\
    a_{p-k} &\cong_\Delta b_{q-k} \\
      & \ldots \\
    a_{p-1} &\cong_\Delta b_{q-1} \\
    \end{aligned}
    $$

    \item Using $\mathsf{projection}$, it can be handled by first filling
    $$
    {?x} \defeq
        \lam {x_0} {\ldots \lam {x_{p-k-1}} {
            (\app {\app {x_i} {?y_0}} {\ldots {?y_{m-1}}})
        }}
    $$
    for some $k \leq p$ and $i < p - k$ and $m \in \mathbb N$, and then splitting the updated constraint whenever possible. The choice of $m$ may be restricted if type information is available.
\end{itemize}
\end{example}

\begin{example}
Consider the unification constraint $\app {\app {\app {?x} {a_0}} \ldots} {a_{p-1}} \cong_\Delta \lam {z_0} {\ldots \lam {z_{m-1}} {(\app {\app {\app f {b_0}} \ldots} {b_{q-1}})}}$, where $f$ is either some free variable or one of $z_0, \ldots, z_{m-1}$.

\begin{itemize}
    \item Using $\mathsf{imitation}$, it can be handled by first filling
    $$
    {?x} \defeq
        \lam {x_0} {\ldots \lam {x_{p-1}} {\lam {z_0} {\ldots \lam {z_{m-1}} {
            (\app {\app f {?y_0}} {\ldots {?y_{q-1}}})
        }}}}
    $$
    and then splitting into
    $$
    \begin{aligned}
    \llet {x_0} {a_0} {\ldots \llet {x_{p-1}} {a_{p-1}} {?y_0}} &\cong_\Delta b_0 \\
      & \ldots \\
    \llet {x_0} {a_0} {\ldots \llet {x_{p-1}} {a_{p-1}} {?y_{q-1}}} &\cong_\Delta b_{q-1} \\
    \end{aligned}
    $$

    \item The case of $\mathsf{projection}$ is similar as in the previous example.
\end{itemize}
\end{example}

\subsection*{The other case}

Finally, when both sides of a constraint have `flexible heads', its solution is delayed. We repeat $\mathsf{simpl}$ and $\mathsf{match}$ until arriving at a node where all constraints are flexible-flexible pairs. Huet's algorithm was initially proposed for simple type theories where every type is inhabited, in which case a unifier can be found by setting every hole to some canonical instance of its type.

However, under our settings, not every type is inhabited. Furthermore, we need to be able to cover \emph{all} valid constructions to ensure completeness, instead of only a particular unifier.

\begin{example}
If not all types are inhabited, solving the trivial constraint
$${?x} \cong_\Delta {?x}$$
where $\Sigma; \Gamma \vdashd {?x} : A$ becomes a proof search problem to construct a term ${?x}$ of type $A$.

A slightly non-trivial case is
$$\app {?x} a \cong_\Delta \app {?x} a'$$
where $\Sigma; \Gamma \vdashd {?x} : (\ppi x A B)$. If $a$ is not unifiable with $a'$, we need to fill ${?x} \defeq (\lam {x} {?y})$ and construct a term ${?y}$ of type $B$ that does not use the argument $x$ of type $A$.
\end{example}

\subsection*{The generalisation}

Previous works for generalising Huet's algorithm to dependent type theories \cite{pym1990proofs,elliott1990extensions} appear much more involved. This is partly due to ill-typed terms that may occur during the process: the need for type conversion checking means that ill-typed terms can get us into infinite loops. Also, flexible-flexible pairs are hard, and some functions can have a variable number of arguments depending on the values of its initial arguments, complicating the description of $\mathsf{match}$.

\begin{example}
The function $\bot : (\ppi A {\mathcal U_0} A)$ has a variable number of arguments: consider
$$\app \bot B : B \qquad \text{and} \qquad \app {\app \bot (\ppi x B B)} b : B$$
\end{example}

Here we take another point of view: in dependent type theory, unification search and proof search should be considered as two sides of a single semi-decidable problem. Therefore, we aim to handle them together in our proof system. In the same spirit as in \cite{norell2007towards}, we block hole-filling until type compatibility is fully established, so that every term involved in unification is well-typed. The flexible-flexible pairs simply require interleaving some usual proof search during unification.

It turns out that with such an arrangement, we only need to establish the correctness of $\mathsf{simpl}$, while the behaviour of $\mathsf{match}$ is simulated accurately by $\mathsf{simpl}$ and any hole-filling actions applicable. An example will be given in \cref{sec:tableau_unification_search}. The remaining part of this section will introduce basic concepts, define $\mathsf{simpl}$ and prove that it indeed gives an equivalent set of unification constraints.

Recall that in order to test if two well-typed terms are definitionally equal, it suffices to reduce them to their normal forms and compare if they are syntactically equal. However, as comparisons are recursive, we can reduce terms `by need' during the process. This can be achieved by \emph{weak head reduction}, which does the bare minimum to uncover the `actual' top-level structure of a term:

\newcommand{\whr}{\rightsquigarrow^{\mathsf{wh}}}

\begin{definition}[Weak head reduction]
\label{def:weak_head_reduction}
The relation $(\whr_\Delta)$ between terms with holes is inductively defined by the same rules as $(\rightsquigarrow_\Delta)$ in \cref{def:hole_conversion}, except that the set of $(\mathsf{cong})$ rules is restricted to:
\begin{center}
    \AxiomC{$b \rightsquigarrow^\delta_{(x \defeq a)} b'$}
    \UnaryInfC{$\llet x a b \whr_\Delta \llet x a {b'}$}
    \DisplayProof
\end{center}
\begin{center}
    \AxiomC{$a \whr_\Delta a'$}
    \UnaryInfC{$\app a b \whr_\Delta \app {a'} b$}
    \DisplayProof
\end{center}
\begin{center}
    \AxiomC{$a \whr_\Delta a'$}
    \UnaryInfC{$\fst a \whr_\Delta \fst {a'}$}
    \DisplayProof
    \qquad
    \AxiomC{$a \whr_\Delta a'$}
    \UnaryInfC{$\snd a \whr_\Delta \snd {a'}$}
    \DisplayProof
\end{center}
where $(\rightsquigarrow^\delta_\Delta)$ is $(\rightsquigarrow_\Delta)$ with only $(\delta)$ and $(\mathsf{cong})$ rules, and is used in the first rule only to ensure that all references to the variable $x$ inside the body $b$ can be removed.\footnote{However, if $b$ contains holes, we actually need $(\mathsf{attach})$ as a reduction rule in $(\whr_\Delta)$ as well (see the example in \cref{sec:evaluation_type_theory}). This indicates a problem with our theory.}
\end{definition}

In other words, $(\whr_\Delta)$ only reduces the subterm being `used' (i.e. the function being applied to an argument, or the tuple being projected from), removing let-bindings in its path. We say that a term is in \emph{weak head normal form} (WHNF) under $\Delta$ if no $(\whr_\Delta)$ rule is applicable to it.

There can be many possible alternative forms of \cref{def:weak_head_reduction}. Regardless of the details in the definition, the importance is that it yields WHNFs in well-defined shapes:

\begin{proposition}[Weak head normal form]
\label{thm:weak_head_normal_form}
For any $a \termh$ well-typed under signature $\Sigma$ and $\Gamma \ctxh$, $a$ is in weak head normal form under $\Gamma$ iff it has one of the following forms:
$$
\begin{aligned}
a_\mathsf{ne} ::=
    & \phantom{{}\mid{}} \textcolor{blue}{\mathcal U_{i \in \mathbb N}} \mid \textcolor{yellow}{x \notin (\defn \Gamma)} \mid \textcolor{gray}{?x} \\
    & \mid \textcolor{blue}{\ppi x A B} \mid \textcolor{red}{\app {a_\mathsf{ne}} b} \\
    & \mid \textcolor{blue}{\sig x A B} \mid \textcolor{red}{\fst {a_\mathsf{ne}}} \mid \textcolor{red}{\snd {a_\mathsf{ne}}} \\
    & \mid \textcolor{blue}{\unit} \\[0.3em]
a_\mathsf{whnf} ::=
    & \phantom{{}\mid{}} a_\mathsf{ne} \mid \textcolor{green}{\lam x b} \mid \textcolor{green}{\pair a b} \mid \textcolor{green}{\sstar} \\
\end{aligned}
$$
and we introduce the following standard terminology:
\begin{itemize}[noitemsep]
    \item Assumed variables $x \notin (\defn \Gamma)$ are called \emph{rigid heads};
    \item Holes $?x$ are called \emph{flexible heads};
    \item The top-level forms $\mathcal U_i$, $(\ppi \_ \_ \_)$, $(\sig \_ \_ \_)$, $\unit$ are called \emph{type formers};
    \item The top-level forms $(\app \_ \_)$, $(\fst \_)$, $(\snd \_)$ are called \emph{eliminators};
    \item The top-level forms $(\lam \_ \_)$, $\pair \_ \_$, $\sstar$ are called \emph{constructors};
    \item Terms $a_\mathsf{ne}$ are called \emph{neutral terms}.
    \item Terms $a_\mathsf{whnf}$ begin with either a type former, a constructor, or a head optionally followed by a sequence of eliminators. Such sequences of eliminators are called \emph{spines}. The head and the spine are together called a \emph{body}.
\end{itemize}
\end{proposition}

\begin{proof}
By \cref{def:weak_head_reduction}, a well-typed term is \emph{not} in WHNF iff it either has a reducible top-level form (e.g. a defined variable), or is an eliminator on a constructor (in this case, well-typedness ensures that they correspond, so there is an immediate redex), or is an eliminator on a subterm which is still not in WHNF. The complement of such set of terms is described exactly by $a_\mathsf{whnf}$.\footnote{Here we make the \emph{wrong} assumption that all let-binders are reducible top-level forms. Although this is true in our implementation, it is not true for our on-paper formulation of $(\whr_\Delta)$ unless we include $(\mathsf{attach})$ as a reduction rule (see the example in \cref{sec:evaluation_type_theory}), which would break strong normalisation. Due to the late discovery of this problem before the project deadline and the complexity of a potential fix, we leave this as future work.}
\end{proof}

As a corollary, this leads to similar a classification of normal forms, which are simply terms whose subterms are all WHNFs.

The top-level structure of a WHNF can be thought as the `actual' top-level structure of a term, since it persists under arbitrary further reductions:

\begin{proposition}[Persistence of type formers]
\label{thm:persistence_of_type_formers}
For any $\Delta \envh$ and $A, B, t \termh$:
\begin{itemize}[noitemsep]
    \item If $\mathcal U_i \rightsquigarrow_\Delta^\ast t$, then $t = \mathcal U_i$;
    \item If $(\ppi x A B) \rightsquigarrow_\Delta^\ast t$, then $t = (\ppi x {A'} {B'})$ where $A \rightsquigarrow_\Delta^\ast {A'}$ and $B \rightsquigarrow_\Delta^\ast {B'}$;
    \item If $(\sig x A B) \rightsquigarrow_\Delta^\ast t$, then $t = (\sig x {A'} {B'})$ where $A \rightsquigarrow_\Delta^\ast {A'}$ and $B \rightsquigarrow_\Delta^\ast {B'}$;
    \item If $\unit \rightsquigarrow_\Delta^\ast t$, then $t = \unit$.
\end{itemize}
\end{proposition}

\begin{proof}
By induction on the structure of $(\rightsquigarrow_\Delta^\ast)$ as the transitive closure of $(\rightsquigarrow_\Delta)$, then unfolding the definition and case analysis on $(\rightsquigarrow_\Delta)$.
\end{proof}

\begin{proposition}[Persistence of constructors]
\label{thm:persistence_of_constructors}
For any $\Delta \envh$ and $a, b, t \termh$:
\begin{itemize}[noitemsep]
    \item If $(\lam x b) \rightsquigarrow_\Delta^\ast t$, then $t = (\lam x {b'})$ where $b \rightsquigarrow_\Delta^\ast {b'}$;
    \item If $\pair a b \rightsquigarrow_\Delta^\ast t$, then $t = \pair {a'} {b'}$ where $a \rightsquigarrow_\Delta^\ast {a'}$ and $b \rightsquigarrow_\Delta^\ast {b'}$;
    \item If $\sstar \rightsquigarrow_\Delta^\ast t$, then $t = \sstar$.
\end{itemize}
\end{proposition}

\begin{proof}
Similar as above. This lemma depends on the fact that the system does not include $(\eta)$-like reductions, e.g. $(\lam x {\app f x}) \rightsquigarrow f$.
\end{proof}

\newcommand{\form}[2]{{#1}\,@\,({#2})}
\newcommand{\sol}{\operatorname{sol}}

In the next lemma, we will use the uniform notation $\form h {e_0, \ldots, e_{n-1}}$ to denote a body, where $h$ is the head, which may be either a variable or a hole; $(e_0, \ldots, e_{n-1})$ is the spine, where each eliminator $e_i$ may be either an argument (term) or a projection ($\fst$ or $\snd$). We will also write e.g. $e_i \rightsquigarrow_\Delta^\ast e_i'$, which has the usual meaning if $e_i$ is a term, or $e_i = e_i'$ if $e_i$ is a projection.

\begin{proposition}[Persistence of heads]
\label{thm:persistence_of_bodies}
For any $\Delta \envh$ and $a$ such that $a = \form h {e_0, \ldots, e_{n-1}}$ is in WHNF under $\Delta$:
\begin{itemize}[noitemsep]
    \item If $h$ is a variable, then for any $t \termh$ such that $a \rightsquigarrow_\Delta^\ast t$, we have $t = \form h {e_0', \ldots, e_{n-1}'}$ where each $e_i \rightsquigarrow_\Delta^\ast e_i'$;
    \item If $h$ is a hole, then for any $t \termh$ such that $a \rightsquigarrow_\Delta^\ast t$, we have $t = \form h {e_0', \ldots, e_{n-1}'}$ where each $e_i \rightsquigarrow_\Delta^\ast e_i'$.
\end{itemize}
\end{proposition}

\begin{proof}
By induction on the length of the spine, then similar as above.
\end{proof}

During unification, only holes may be filled: we never give new definitions to variables, only to holes. The first part says that bodies with variable heads still persist in this case, which is why we call variable heads as \emph{rigid}. The word \emph{variable} should really be understood as \emph{constant} in the process.

Based on the behaviours of different top-level forms, let us extend the uniform notation:

\begin{definition}[Uniform notation]
\label{def:uniform_notation} Terms in WHNF can be written as $\form h {e_0, \ldots, e_{n-1}}$ where:
$$
\begin{aligned}
&\form {\textcolor{blue}{[\mathcal U_i]}} {} &&\defeq \mathcal U_i \\
&\form {\textcolor{blue}{[\Pi_x]}} {A, B} &&\defeq (\ppi x A B) \\
&\form {\textcolor{blue}{[\Sigma_x]}} {A, B} &&\defeq (\sig x A B) \\
&\form {\textcolor{blue}{[\unit]}} {} &&\defeq \unit \\
&\form {\textcolor{green}{[\lambda_x]}} {b} &&\defeq (\lam x b) \\
&\form {\textcolor{green}{[\sstar]}} {a, b} &&\defeq \pair a b \\
&\form {\textcolor{green}{[\sstar]}} {} &&\defeq \sstar \\
&\form {\textcolor{yellow}{[x]}} {\textcolor{red}{e_0, \ldots, e_{n-1}}} &&\text{is variable $x$ applied to eliminators $e_0, \ldots, e_{n-1}$ one-by-one} \\
&\form {\textcolor{gray}{[?x]}} {\textcolor{red}{e_0, \ldots, e_{n-1}}} &&\text{is hole ${?x}$ applied to eliminators $e_0, \ldots, e_{n-1}$ one-by-one} \\
\end{aligned}
$$
and we introduce the following non-standard terminology:
\begin{itemize}[noitemsep]
    \item Holes ${?x}$ are called \emph{flexible generalised heads};
    \item All other forms of $h$ are called \emph{rigid generalised heads};
    \item The lists $(e_0, \ldots, e_{n-1})$ are called \emph{generalised spines}, whose length $n$ is the \emph{arity}.
\end{itemize}
\end{definition}

This allows us to state the main theorem concisely:

\begin{proposition}[Rigid-rigid matching]
\label{thm:rigid_rigid_matching}
For any $\Delta \envh$ and $a, b \termh$ where $a = \form h {e_0, \ldots, e_{n-1}}$ and $b = \form {h'} {e'_0, \ldots, e'_{m-1}}$, if both $h$ and $h'$ are rigid, then for any term substitution for holes $\sigma$, we have $\sigma a \equiv_{\sigma \Delta} \sigma b$ iff $h = h'$ and $n = m$ and each $\sigma {e_i} \equiv_{\sigma \Delta} \sigma {e'_i}$.
\end{proposition}

\begin{proof}
To show both directions:
\begin{itemize}[noitemsep]
    \item $(\leftarrow)$: by congruence of $(\equiv_{\sigma \Delta})$ and \cref{thm:hole_filling}.
    \item $(\rightarrow)$: by confluence and $\form h {\sigma {e_0}, \ldots, \sigma {e_{n-1}}} \equiv_{\sigma \Delta} \form {h'} {\sigma {e'_0}, \ldots, \sigma {e'_{m-1}}}$ we have $c \termh$ such that both sides $\rightsquigarrow_{\sigma \Delta}^\ast c$. By \cref{thm:persistence_of_type_formers,thm:persistence_of_constructors,thm:persistence_of_bodies}, in every case we have $h = h'$ and $n = m$, and for any $i < n$ the existence of $c_i \termh$ such that $\sigma {e_i}, \sigma {e'_i} \rightsquigarrow_{\sigma \Delta} c_i$. The result follows by transitivity of $(\equiv_{\sigma \Delta})$. \qedhere
\end{itemize}
\end{proof}

In other words, when both sides of the constraint $(a \cong_\Delta b)$ have WHNFs with rigid generalised heads $h, h'$ and generalised spines $(e_0, \ldots, e_{n-1}), (e'_0, \ldots, e'_{m-1})$ respectively, the constraint is unsatisfiable if $h \neq h'$ or $n \neq m$, or can be equivalently split into the smaller constraints $(e_i \cong_\Delta e'_i)$ otherwise. As a corollary, we also have the following:

\begin{proposition}[Rigid-rigid subtype matching]
\label{thm:rigid_rigid_subtype_matching}
For any $\Delta \envh$ and $a, b \termh$ where $a = \form h {e_0, \ldots, e_{n-1}}$ and $b = \form {h'} {e'_0, \ldots, e'_{m-1}}$, if both $h$ and $h'$ are rigid, then for any term substitutions for holes $\sigma$, we have $\sigma a \leq_{\sigma \Delta} \sigma b$ iff either:
\begin{itemize}[noitemsep]
    \item $h = \mathcal U_i$ and $h' = \mathcal U_j$ where $i \leq j$;
    \item $h = h'$ and $n = m$ and each $\sigma {e_i} \equiv_{\sigma \Delta} \sigma {e'_i}$.
\end{itemize}
\end{proposition}

\begin{proof}
To show both directions:
\begin{itemize}[noitemsep]
    \item $(\leftarrow)$: By definition of $(\leq_{\sigma \Delta})$ and using \cref{thm:rigid_rigid_matching}.
    \item $(\rightarrow)$: By induction on $(\leq_{\sigma \Delta})$ and using \cref{thm:rigid_rigid_matching}. \qedhere
\end{itemize}
\end{proof}

In other words, the constraint $(a \preceq_\Delta b)$ can be splitted in the same way as $(a \cong_\Delta b)$, except that we allow the additional case where $h = \mathcal U_i$ and $h' = \mathcal U_j$ with $i \leq j$.

With these at hand, matching between flexible-rigid and flexible-flexible forms are handled by first filling the holes until both sides become rigid, then performing rigid-rigid matching. During filling of the hole, flexible-rigid unification constraints may act as its \emph{expected values}, and be used to filter out options leading to the failure of the subsequent rigid-rigid matching. The $\mathsf{match}$ steps of Huet's algorithm now becomes type- and value-directed hole-filling.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The sequent calculi}
\label{sec:sequent_calculi}

Interleaving the ideas from the previous chapter, we can now formulate sequent calculi to serve as underlying principles to the tableau and connection proof systems for the type theory.

\section{The sequent calculi}
\label{sec:sequent_calculi_spec}

This section will specify the single-succedent sequent calculus $(\vdashf)$ and the multi-succedent $(\vdashs)$. Both will include \emph{labels} and \emph{proof terms} in cedents, which are their main difference from previous formulations \cite{pym1990proofs}. The multi-succedent $(\vdashs)$ is novel for dependent type theories.

\begin{definition}[Labels]
\label{def:labels}
The \emph{labels} are contexts $u, v \ctxh$ for which we define:
\begin{itemize}[noitemsep]
    \item For any labels $u, v$ with $(\dom u) \cap (\dom v) = \emptyset$, write $(uv)$ for context concatenation;
    \item For any labels $u, v$, write $u \leq v$ iff there exists some context $w$ such that $uw = v$.
\end{itemize}
\end{definition}

Instead of letting labels be simple lists of variable names as in the example system of \cref{sec:example_proof_system}, we add in their type information as well. This can be convenient, since every entry in a cedent can make sense on its own, independent of each other.\footnote{Also, contexts are the `possible worlds' in intuitionistic type theories, which labels intend to denote.}

Starting from this section, we use $u, v$ to denote contexts, so that $\Gamma, \Delta$ are left for cedents.

\begin{definition}[Cedents]
\label{def:cedents}
The \emph{antecedents} $\Gamma$ and \emph{succedents} $\Delta$ are unordered lists:
$$
\begin{aligned}
    & \Gamma ::=
        \epsilon \mid \Gamma, (a : A)^u \\
    & \Delta ::=
        \epsilon \mid \Delta, (a : A)^u \\
\end{aligned}
$$
where $a \termh$ are the \emph{proof terms}, $A \termh$ are their \emph{types} and $u \ctxh$ are their \emph{labels}.
\end{definition}

\begin{definition}[Sequents]
\label{def:sequents}
The \emph{focused sequents} $(\Sigma; \Gamma \vdashf \Delta)$ are tuples consisting of signature $\Sigma$, antecedent $\Gamma$, and a singleton succedent $\Delta = (c : C)^w$. The \emph{defocused sequents} $(\Sigma; \Gamma \vdashs \Delta)$ are tuples consisting of signature $\Sigma$, antecedent $\Gamma$ and succedent $\Delta$.
\end{definition}

\begin{definition}[Well-formed sequents]
\label{def:well_formed_sequents}
The sequent $(\Sigma; \Gamma \vdashf \Delta)$ or $(\Sigma; \Gamma \vdashs \Delta)$ is \emph{well-formed} iff every term in $\Gamma$ and every type in $\Delta$ is well-typed:
\begin{itemize}[noitemsep]
    \item For any $(a : A)^u \in \Gamma$ we have $\Sigma; u \vdashd a : A$.
    \item For any $(a : A)^u \in \Delta$ we have $\Sigma; u \vdashd A : \mathcal U_i$ for some $i\in\mathbb N$.
\end{itemize}
\end{definition}

Notably, in order for the sequents to be well-formed, there is no constraint on the proof terms $a$ in succedents. We consider $a$ as an `answer' from a derivation, but well-formedness only concerns the `question'. The correctness of the `answer' will be the subject of the soundness theorems (\cref{thm:focused_calculus_soundness,thm:defocused_calculus_soundness}).

\begin{definition}[Full sequents]
\label{def:full_sequents}
The sequent $(\Sigma; \Gamma \vdashf \Delta)$ or $(\Sigma; \Gamma \vdashs \Delta)$ is \emph{full} iff every label in $\Delta$ has all its entries present in $\Gamma$:
\begin{itemize}[noitemsep]
    \item For any $u \in \Delta$ and $v \leq u$, write $v = (w, (x : A))$ or $(w, (x \defeq a : A))$, then there is some $(x : A')^v \in \Gamma$ where $A \rightsquigarrow^\ast_v A'$.
\end{itemize}
\end{definition}

This is a rather technical way to say `every accessible variable is ready to be used', which is necessary for the completeness theorems (\cref{thm:focused_calculus_completeness,thm:defocused_calculus_completeness}). As another option, this condition may be replaced by a new sequent calculus rule allowing the `extraction' of every variable in the label of a succedent, but let us just stick to this formulation for now.

\begin{definition}
\label{def:sequents_and_contexts}
For any context $u \ctxh$ the antecedent $u^\ast$ is obtained by replacing in $u$ every entry $(x : A)$ or $(x \defeq a : A)$ by $(x : A)^v$, where $v \leq u$ is the sub-context of $u$ containing entries up to the $(x : A)$ or $(x \defeq a : A)$.
\end{definition}

\begin{example}
\label{thm:well_formed_sequents_from_contexts}
For any signature $\Sigma$ and $u \ctxh$ and $A \termh$ such that $\Sigma; u \vdashd A : \mathcal U_i$, for any $a \termh$, the sequents $(\Sigma; u^\ast \vdashf (a : A)^u)$ and $(\Sigma; u^\ast \vdashs (a : A)^u)$ are both well-formed and full.
\end{example}

This turns a context into an antecedent containing all its entries, which can be used to set up a starting point of proof search.

\begin{definition}[Unification rules]
\label{def:conversion_calculus}
Derivations for $(a \cong_u b)$ and $(a \preceq_u b)$ are generated by the rules listed in \cref{fig:sequent_calculi}:
\begin{itemize}
    \item In the $(\mathsf{split})$ rules, $\form {h} {e_0, \ldots, e_{n-1}}$ denotes some term in WHNF (\cref{def:uniform_notation}).
    
    If some $e_i$ is a projection, the premise $(e_i \cong_u e'_i)$ should be understood as $e_i = e'_i$, as made explicit by the $(\mathsf{proj})$ rules.
\end{itemize}
\end{definition}

\begin{definition}[Inference rules]
\label{def:typing_calculus}
Derivations for $(\Sigma; \Gamma \vdashf \Delta)$ and $(\Sigma; \Gamma \vdashs \Delta)$ are generated by the rules listed in \cref{fig:sequent_calculi}:
\begin{itemize}
    \item In the $(\mathsf{hole})$ rules, the greyed premise is optional.

    \item In the $(\mathsf{reduce})$ rules, reduction is only performed on types: $\Gamma \rightsquigarrow^\ast \Gamma'$ means any $(a : A)^u \in \Gamma$ can be reduced to $(a : A')^u \in \Gamma'$ iff $A \rightsquigarrow^\ast_u A'$. The same goes for $\Delta \rightsquigarrow^\ast \Delta'$.
\end{itemize}
\end{definition}

\begin{figure}
    $\boxed{a \cong_\delta b,\ a \preceq_\delta b}$

    \begin{prooftree}
    \AxiomC{$a \rightsquigarrow_\delta^\ast a'$}
    \AxiomC{$b \rightsquigarrow_\delta^\ast b'$}
    \AxiomC{$a' \cong_\delta b'$}
    \RightLabel{$(\mathsf{reduce})$}\TrinaryInfC{$a \cong_\delta b$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$a \rightsquigarrow_\delta^\ast a'$}
    \AxiomC{$b \rightsquigarrow_\delta^\ast b'$}
    \AxiomC{$a' \preceq_\delta b'$}
    \RightLabel{$(\mathsf{reduce})$}\TrinaryInfC{$a \preceq_\delta b$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$h = h' \text{ rigid}$}
    \AxiomC{$n = n'$}
    \AxiomC{$e_0 \cong_\delta e'_0$}
    \AxiomC{$\ldots$}
    \AxiomC{$e_{n-1} \cong_\delta e'_{n-1}$}
    \RightLabel{$(\mathsf{split})$}\QuinaryInfC{$\form {h} {e_0, \ldots, e_{n-1}} \cong_\delta \form {h'} {e'_0, \ldots, e'_{n'-1}}$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$h = h' \text{ rigid} \mid (h, h') = (\mathcal U_k, \mathcal U_l), k \leq l$}
    \AxiomC{$n = n'$}
    \AxiomC{$e_0 \cong_\delta e'_0$}
    \AxiomC{$\ldots$}
    \AxiomC{$e_{n-1} \cong_\delta e'_{n-1}$}
    \RightLabel{$(\mathsf{split})$}\QuinaryInfC{$\form {h} {e_0, \ldots, e_{n-1}} \preceq_\delta \form {h'} {e'_0, \ldots, e'_{n'-1}}$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\vphantom{\leq}$}
    \RightLabel{$(\mathsf{proj})$}\UnaryInfC{$\fst \cong_\delta \fst$}
    \DisplayProof
    \qquad
    \AxiomC{$\vphantom{\leq}$}
    \RightLabel{$(\mathsf{proj})$}\UnaryInfC{$\snd \cong_\delta \snd$}
    \DisplayProof
    \AxiomC{}
    \end{prooftree}

    \vspace{1em}
    $\boxed{\Sigma; \Gamma \vdashf \Delta}$

    \begin{prooftree}
    \AxiomC{$(\textcolor{gray}{?x}, u, A) \in \Sigma$}
    \AxiomC{$\shade{\Sigma; \Gamma \vdashf (a : A)^u}$}
    \RightLabel{$(\mathsf{hole})$}\BinaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{gray}{?x} : A)^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$A \preceq_{uv} B$}
    \RightLabel{$(\mathsf{init})$}\UnaryInfC{$\Sigma; \Gamma \ni (\textcolor{yellow}{a} : A)^u \vdashf (\textcolor{yellow}{a} : B)^{uv}$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$k < l$}
    \RightLabel{$(\mathsf{univ})$}\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{blue}{\mathcal U_k} : \mathcal U_l)^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Sigma; \Gamma \vdashf (\textcolor{blue}{A} : \mathcal U_l)^u$}
    \AxiomC{$\Sigma; \Gamma, (\textcolor{blue}{x} : A)^{u, (x : A)} \vdashf (\textcolor{blue}{B} : \mathcal U_l)^{u, (x : A)}$}
    \RightLabel{$(\Pi\mathsf{form})$}\BinaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{blue}{\ppi x A B} : \mathcal U_l)^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Sigma; \Gamma \vdashf (\textcolor{blue}{A} : \mathcal U_l)^u$}
    \AxiomC{$\Sigma; \Gamma, (\textcolor{blue}{x} : A)^{u, (x : A)} \vdashf (\textcolor{blue}{B} : \mathcal U_l)^{u, (x : A)}$}
    \RightLabel{$(\Sigma\mathsf{form})$}\BinaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{blue}{\sig x A B} : \mathcal U_l)^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\unit\mathsf{form})$}\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{blue}{\unit} : \mathcal U_l)^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Sigma; \Gamma, (\textcolor{green}{x} : A)^{u, (x : A)} \vdashf (\textcolor{green}{b} : B)^{u, (x : A)}$}
    \RightLabel{$(\Pi\mathsf{intro})$}\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{green}{\lam x b} : (\ppi x A B))^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Sigma; \Gamma \vdashf (\textcolor{green}{a} : A)^u$}
    \AxiomC{$\Sigma; \Gamma \vdashf (\textcolor{green}{b} : \llet x a B)^u$}
    \RightLabel{$(\Sigma\mathsf{intro})$}\BinaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{green}{\pair a b} : (\sig x A B))^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\unit\mathsf{intro})$}\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{green}{\sstar} : \unit)^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Sigma; \Gamma \vdashf (\textcolor{red}{a} : A)^{uv}$}
    \AxiomC{$\Sigma; \Gamma, (\textcolor{red}{\app b a} : \llet x a B)^{uv} \vdashf (c : C)^w$}
    \RightLabel{$(\Pi\mathsf{elim})$}\BinaryInfC{$\Sigma; \Gamma \ni (\textcolor{red}{b} : (\ppi x A B))^u \vdashf (c : C)^w$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Sigma; \Gamma, (\textcolor{red}{\fst a} : A)^u, (\textcolor{red}{\snd a} : \llet x {\fst a} B)^u \vdashf (c : C)^w$}
    \RightLabel{$(\Sigma\mathsf{elim})$}\UnaryInfC{$\Sigma; \Gamma \ni (\textcolor{red}{a} : (\sig x A B))^u \vdashf (c : C)^w$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \rightsquigarrow^\ast \Gamma'$}
    \AxiomC{$C \rightsquigarrow^\ast_w C'$}
    \AxiomC{$\Sigma; \Gamma' \vdashf (c : C')^w$}
    \RightLabel{$(\mathsf{reduce})$}\TrinaryInfC{$\Sigma; \Gamma \vdashf (c : C)^w$}
    \end{prooftree}

\caption{The sequent calculi}
\label{fig:sequent_calculi}
\end{figure}

\begin{figure}
    \ContinuedFloat
    $\boxed{\Sigma; \Gamma \vdashs \Delta}$

    \begin{prooftree}
    \AxiomC{$(\textcolor{gray}{?x}, u, A) \in \Sigma$}
    \AxiomC{$\shade{\Sigma; \Gamma \vdashs \Delta, (a : A)^u}$}
    \RightLabel{$(\mathsf{hole})$}\BinaryInfC{$\Sigma; \Gamma \vdashs \Delta \ni (\textcolor{gray}{?x} : A)^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$A \preceq_{uv} B$}
    \RightLabel{$(\mathsf{init})$}\UnaryInfC{$\Sigma; \Gamma \ni (\textcolor{yellow}{a} : A)^u \vdashs \Delta \ni (\textcolor{yellow}{a} : B)^{uv}$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$k < l$}
    \RightLabel{$(\mathsf{univ})$}\UnaryInfC{$\Sigma; \Gamma \vdashs \Delta \ni (\textcolor{blue}{\mathcal U_k} : \mathcal U_l)^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Sigma; \Gamma \vdashs \Delta, (\textcolor{blue}{A} : \mathcal U_l)^u$}
    \AxiomC{$\Sigma; \Gamma, (\textcolor{blue}{x} : A)^{u, (x : A)} \vdashs \Delta, (\textcolor{blue}{B} : \mathcal U_l)^{u, (x : A)}$}
    \RightLabel{$(\Pi\mathsf{form})$}\BinaryInfC{$\Sigma; \Gamma \vdashs \Delta \ni (\textcolor{blue}{\ppi x A B} : \mathcal U_l)^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Sigma; \Gamma \vdashs \Delta, (\textcolor{blue}{A} : \mathcal U_l)^u$}
    \AxiomC{$\Sigma; \Gamma, (\textcolor{blue}{x} : A)^{u, (x : A)} \vdashs \Delta, (\textcolor{blue}{B} : \mathcal U_l)^{u, (x : A)}$}
    \RightLabel{$(\Sigma\mathsf{form})$}\BinaryInfC{$\Sigma; \Gamma \vdashs \Delta \ni (\textcolor{blue}{\sig x A B} : \mathcal U_l)^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\unit\mathsf{form})$}\UnaryInfC{$\Sigma; \Gamma \vdashs \Delta \ni (\textcolor{blue}{\unit} : \mathcal U_l)^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Sigma; \Gamma, (\textcolor{green}{x} : A)^{u, (x : A)} \vdashs \Delta, (\textcolor{green}{b} : B)^{u, (x : A)}$}
    \RightLabel{$(\Pi\mathsf{intro})$}\UnaryInfC{$\Sigma; \Gamma \vdashs \Delta \ni (\textcolor{green}{\lam x b} : (\ppi x A B))^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Sigma; \Gamma \vdashs \Delta, (\textcolor{green}{a} : A)^u$}
    \AxiomC{$\Sigma; \Gamma \vdashs \Delta, (\textcolor{green}{b} : \llet x a B)^u$}
    \RightLabel{$(\Sigma\mathsf{intro})$}\BinaryInfC{$\Sigma; \Gamma \vdashs \Delta \ni (\textcolor{green}{\pair a b} : (\sig x A B))^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\unit\mathsf{intro})$}\UnaryInfC{$\Sigma; \Gamma \vdashs \Delta \ni (\textcolor{green}{\sstar} : \unit)^u$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Sigma; \Gamma \vdashs \Delta, (\textcolor{red}{a} : A)^{uv}$}
    \AxiomC{$\Sigma; \Gamma, (\textcolor{red}{\app b a} : \llet x a B)^{uv} \vdashs \Delta$}
    \RightLabel{$(\Pi\mathsf{elim})$}\BinaryInfC{$\Sigma; \Gamma \ni (\textcolor{red}{b} : (\ppi x A B))^u \vdashs \Delta$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Sigma; \Gamma, (\textcolor{red}{\fst a} : A)^u, (\textcolor{red}{\snd a} : \llet x {\fst a} B)^u \vdashs \Delta$}
    \RightLabel{$(\Sigma\mathsf{elim})$}\UnaryInfC{$\Sigma; \Gamma \ni (\textcolor{red}{a} : (\sig x A B))^u \vdashs \Delta$}
    \end{prooftree}

    \begin{prooftree}
    \AxiomC{$\Gamma \rightsquigarrow^\ast \Gamma'$}
    \AxiomC{$\Delta \rightsquigarrow^\ast \Delta'$}
    \AxiomC{$\Sigma; \Gamma' \vdashs \Delta'$}
    \RightLabel{$(\mathsf{reduce})$}\TrinaryInfC{$\Sigma; \Gamma \vdashs \Delta$}
    \end{prooftree}

\caption{The sequent calculi continued}
\label{fig:sequent_calculi_continued}
\end{figure}

There is a mechanical correspondence between the rules for $(\vdashf)$ and $(\vdashs)$: every rule for $(\vdashf)$ can be turned into one for $(\vdashs)$ by allowing multiple succedents.

\section{Structural properties}
\label{sec:sequent_calculi_properties}

It helps to consider a derivation as a tree structure, where inferences are the vertices and sequents are the edges. In general, sequent well-formedness and fullness propagate up in the tree, except where well-formedness of the `question' in the right branch is contingent on the correctness of `answer' in the left branch:

\begin{proposition}
\label{thm:well_formed_sequents_upwards}
In any $(\vdashf)$- or $(\vdashs)$-inference, if the conclusion sequent is well-formed:
\begin{itemize}[noitemsep]
    \item For $(\Pi\mathsf{form})$ and $(\Sigma\mathsf{form})$, if $\Sigma; u \vdash A : \mathcal U_l$, both premise sequents are well-formed.
    \item For $(\Sigma\mathsf{intro})$, if $\Sigma; u \vdashd a : A$, both premise sequents are well-formed.
    \item For $(\Pi\mathsf{elim})$, if $\Sigma; uv \vdashd a : A$, both premise sequents are well-formed.
    \item In all other cases, any premise sequent is well-formed.
\end{itemize}
Moreover, if the conclusion sequent is full:
\begin{itemize}[noitemsep]
    \item For $(\Pi\mathsf{elim})$, if $(uv) \leq w$ for some $w \in \Delta$, both premise sequents are full.
    \item In all other cases, any premise sequent is full.
\end{itemize}
\end{proposition}

\begin{proof}
By simple checking of \cref{def:well_formed_sequents,def:full_sequents}.
\end{proof}

If a $(\vdashf)$-derivation $\mathcal D$ has been pruned of inaccessible formulas (\cref{thm:sequent_calculi_pruning}), the soundness theorem (\cref{thm:focused_calculus_soundness}) will simplify the conditions: every sequent in $\mathcal D$ is well-formed/full iff the root sequent of $\mathcal D$ is well-formed/full.

To search for a proof of $A$ under context $u$, one may start with the well-formed and full root sequent $(\Sigma; u^\ast \vdashf ({?x} : A)^u)$ or $(\Sigma; u^\ast \vdashs ({?x} : A)^u)$ and build a derivation upwards. To maintain well-formedness of the new `questions', we can `answer' the left branch first, but we can too use \emph{holes} to satisfy the requirement of \cref{thm:well_formed_sequents_upwards} and go to the right branch first. Well-formedness allows us to freely reduce any type in the sequent without worrying about non-termination.

The holes we introduce are temporary: they should not be in the final proof term after the proof is finished. For this, we can either fill holes \emph{during} proof search, or have their term assignment \emph{delayed}. Both will be discussed in this section.

\subsection*{Structures of derivations}

We introduce some common terminology to describe the structures of derivations:

\newcommand{\preserved}{\operatorname{pre}}
\newcommand{\principal}{\operatorname{pri}}
\newcommand{\constituent}{\operatorname{con}}

\begin{definition}[Base signatures]
\label{def:sequent_calculi_base_signatures}
For any $(\vdashf)$- or $(\vdashs)$-derivation $\mathcal D$, observe from the rules that the signature $\Sigma$ in every sequent in $\mathcal D$ remains the same. This $\Sigma$ is the \emph{base signature} of $\mathcal D$.
\end{definition}

\begin{definition}[Perfect derivations]
\label{def:sequent_calculi_classifications}
For any $(\vdashf)$- or $(\vdashs)$-derivation $\mathcal D$, we say that:
\begin{itemize}[noitemsep]
    \item $\mathcal D$ is \emph{perfect} if it does not contain $(\mathsf{hole})$ rules.
    \item $\mathcal D$ is \emph{semi-perfect} if every $(\mathsf{hole})$ rule in $\mathcal D$ contains the optional premise.
\end{itemize}
\end{definition}

\begin{definition}[Formulas]
\label{def:sequent_calculi_formulas}
For any $(\vdashf)$- or $(\vdashs)$-derivation $\mathcal D$, an \emph{occurrence} of an entry in a sequent in $\mathcal D$ is called a \emph{formula}. Between two formulas we distinguish between \emph{content equality} and \emph{same identity}.
\end{definition}

\begin{definition}[Preserved formulas]
\label{def:sequent_calculi_preserved_formulas}
For any $(\vdashs)$-derivation $\mathcal D$ and any inference $\mathcal I \in \mathcal D$, write $\mathcal I$ as
\begin{center}
    \AxiomC{$\mathcal C$}
    \AxiomC{$\mathcal S_0$}
    \AxiomC{$\ldots$}
    \AxiomC{$\mathcal S_{n-1}$}
    \RightLabel{$t$}\QuaternaryInfC{$\mathcal S$}
    \DisplayProof
\end{center}
where $\mathcal C$ are the non-sequent side conditions, while each $\mathcal S_i = (\Sigma; \Gamma_i \vdashs \Delta_i)$ and $\mathcal S = (\Sigma; \Gamma \vdashs \Delta)$ are sequents. Then for any $i < n$:
\begin{itemize}[noitemsep]
    \item For the $k$-th entry $F \in \Gamma$, its $i$-th \emph{preserved copy} $(\preserved_i F)$ refers to the $k$-th entry in $\Gamma_i$;
    \item For the $k$-th entry $F \in \Delta$, its $i$-th \emph{preserved copy} $(\preserved_i F)$ refers to the $k$-th entry in $\Delta_i$.
\end{itemize}
Checking every $(\vdashs)$-rule, we can see that $(\preserved_i F)$ is defined for any $F \in \mathcal D$, such that $F$ and $(\preserved_i F)$ are equal in content except when $t = (\mathsf{reduce})$, where a reduction on the type is possible.
\end{definition}

\begin{definition}[Principal and constituent formulas]
\label{def:sequent_calculi_principal_and_constituent_formulas}
For any $(\vdashf)$- or $(\vdashs)$-derivation $\mathcal D$ and any inference $\mathcal I \in \mathcal D$, write $\mathcal I$ as
\begin{center}
    \AxiomC{$\mathcal C$}
    \AxiomC{$\mathcal S_0$}
    \AxiomC{$\ldots$}
    \AxiomC{$\mathcal S_{n-1}$}
    \RightLabel{$t$}\QuaternaryInfC{$\mathcal S$}
    \DisplayProof
\end{center}
Then the sets of \emph{principal formulas} $(\principal \mathcal I) \subseteq \mathcal S$ and \emph{constituent formulas}\footnote{Also known as \emph{side formulas} in some literature, but this phrase can also refer to the non-principal formulas and cause confusion.} $(\constituent_i \mathcal I) \subseteq \mathcal S_i$ contain entry occurrences specified in \cref{tab:sequent_calculi_principal_and_constituent_formulas}.
\end{definition}

\begin{table}[ht]
    \centering
    \begin{tabular}{lll}\hline
    Tag $(t)$ & Principal formulas $(\principal \mathcal I)$ & Constituent formulas $({\constituent_0 \mathcal I}; {\constituent_1 \mathcal I})$ \\\hline
    $(\mathsf{hole})$ & $(\textcolor{gray}{?x} : A)^u$ & $(\textcolor{gray}a : A)^u$ \\
    $(\mathsf{init})$ & $(\textcolor{yellow}{a} : A)^u, (\textcolor{yellow}{a} : B)^{uv}$ & \\
    $(\mathsf{univ})$ & $(\textcolor{blue}{\mathcal U_k} : \mathcal U_l)^u$ & \\
    $(\Pi\mathsf{form})$ & $(\textcolor{blue}{\ppi x A B} : \mathcal U_l)^u$ & $(\textcolor{blue}{A} : \mathcal U_l)^u; (\textcolor{blue}{x} : A)^{ux}, (\textcolor{blue}{B} : \mathcal U_l)^{ux}$ \\
    $(\Sigma\mathsf{form})$ & $(\textcolor{blue}{\sig x A B} : \mathcal U_l)^u$ & $(\textcolor{blue}{A} : \mathcal U_l)^u; (\textcolor{blue}{x} : A)^{ux}, (\textcolor{blue}{B} : \mathcal U_l)^{ux}$ \\
    $(\unit\mathsf{form})$ & $(\textcolor{blue}{\unit} : \mathcal U_l)^u$ & \\
    $(\Pi\mathsf{intro})$ & $(\textcolor{green}{\lam x b} : (\ppi x A B))^u$ & $(\textcolor{green}{x} : A)^{ux}, (\textcolor{green}{b} : B)^{ux}$ \\
    $(\Sigma\mathsf{intro})$ & $(\textcolor{green}{\pair a b} : (\sig x A B))^u$ & $(\textcolor{green}{a} : A)^u; (\textcolor{green}{b} : \llet x a B)^u$ \\
    $(\unit\mathsf{intro})$ & $(\textcolor{green}{\sstar} : \unit)^u$ & \\
    $(\Pi\mathsf{elim})$ & $(\textcolor{red}{b} : (\ppi x A B))^u$ & $(\textcolor{red}{a} : A)^{uv}; (\textcolor{red}{\app b a} : \llet x a B)^{uv}$ \\
    $(\Sigma\mathsf{elim})$ & $(\textcolor{red}{a} : (\sig x A B))^u$ & $(\textcolor{red}{\fst a} : A)^u, (\textcolor{red}{\snd a} : \llet x {\fst a} B)^u$ \\
    $(\mathsf{reduce})$ & & \\\hline
    \end{tabular}
\caption{Principal and constituent formulas}
\label{tab:sequent_calculi_principal_and_constituent_formulas}
\end{table}

% \begin{definition}[Copy equivalence]
% \label{def:sequent_calculi_formulas_copy_equivalence}
% For any $(\vdashs)$-derivation $\mathcal D$ and any two formulas $F, G \in \mathcal D$, we say that $F, G$ are \emph{copy-equivalent} iff they are both preserved copies of some formula $H \in \mathcal D$.
% \end{definition}

% \begin{definition}[Ancestors]
% \label{def:sequent_calculi_formulas_ancestors}
% For any $(\vdashf)$- or $(\vdashs)$-derivation $\mathcal D$...
% \end{definition}

\begin{definition}
\label{def:sequent_calculi_ordering}
For any antecedents $\Gamma, \Gamma'$ and succedents $\Delta, \Delta'$, write $(\Gamma, \Delta) \leq (\Gamma', \Delta')$ iff:
\begin{itemize}[noitemsep]
    \item For any $(a : A)^u \in \Gamma$, exists $(a : B)^u \in \Gamma'$ such that $A \geq_u B$ \emph{(`contravariant' on types)}.
    \item For any $(a : A)^u \in \Delta$, exists $(a : B)^u \in \Delta'$ such that $A \leq_u B$ \emph{(`covariant' on types)}.
\end{itemize}
\end{definition}

The relation may be read as `$\Gamma'$ is more powerful than $\Gamma$, and $\Delta'$ is more inclusive than $\Delta$'. Under this definition, if $\Gamma \subseteq \Gamma'$ and $\Delta \subseteq \Delta'$ then $(\Gamma, \Delta) \leq (\Gamma', \Delta')$.

\begin{proposition}[Weakening]
\label{thm:sequent_calculi_weakening}
For any derivable sequent $(\Sigma; \Gamma \vdashf \Delta)$ or $(\Sigma; \Gamma \vdashs \Delta)$ and any $(\Gamma', \Delta') \geq (\Gamma, \Delta)$, we can derive $(\Sigma; \Gamma' \vdashf \Delta')$ or $(\Sigma; \Gamma' \vdashs \Delta')$.
\end{proposition}

\begin{proof}
By induction on some derivation $\mathcal D$, using confluence (\cref{thm:hole_red_confluence}) and persistence of type formers (\cref{thm:persistence_of_type_formers}) and the $(\mathsf{reduce})$ rule.
\end{proof}

\begin{proposition}[Monotonicity]
\label{thm:defocused_calculus_monotonicity}
For any derivation $\mathcal D$ of $(\Sigma; \Gamma \vdashs \Delta)$ and any sequent $(\Sigma; \Gamma' \vdashs \Delta') \in \mathcal D$, we have $(\Gamma', \Delta') \geq (\Gamma, \Delta)$.
\end{proposition}

\begin{proof}
By induction on $\mathcal D$. Note that every rule has a conclusion sequent of the form $(\Sigma; \Gamma \vdashs \Delta)$, and in all premise sequents we always include $\Gamma$ and $\Delta$ on two sides.
\end{proof}

The above two lemmas together imply that `the subgoals do not become any harder' as we go up in any $(\vdashs)$-derivation tree, so we never need to regret over choices that are already made.

\subsection*{Pruning inaccessible parts}

For the purpose of exact correspondence with tableaux and matrices, the sequent calculi have deliberately relaxed rules: in particular, the label $(uv)$ in $(\Pi\mathsf{elim})$ is allowed to contain completely irrelevant variables in $v$. However, this does not essentially strengthen the calculi, since irrelevant formulas can never be `used':

\begin{definition}[Inaccessible formulas]
\label{def:inaccessible_formulas}
For any sequent $(\Sigma; \Gamma \vdashf \Delta)$ or $(\Sigma; \Gamma \vdashs \Delta)$:
\begin{itemize}[noitemsep]
    \item A label $u \in \Gamma$ is called \emph{inaccessible} iff $u \not\leq v$ for any $v \in \Delta$.
    \item A formula $(a : A)^u \in \Gamma$ is called \emph{inaccessible} iff its label $u$ is \emph{inaccessible}.
\end{itemize}
\end{definition}

\begin{proposition}
\label{thm:sequent_calculi_pruning_step}
For any derivation $\mathcal D$ of $(\Sigma; \Gamma \vdashf \Delta)$ or $(\Sigma; \Gamma \vdashs \Delta)$, for any inaccessible label $i \in \Gamma$, removing the following from $\mathcal D$ gives a new derivation:
\begin{itemize}[noitemsep]
    \item Every antecedent formula with label $u \geq i$.
    \item Every $(\Pi\mathsf{elim})$ inference with label $(uv) \geq i$, along with its left sub-derivation.
    \item Every $(\Sigma\mathsf{elim})$ inference with label $u \geq i$.
\end{itemize}
\end{proposition}

\begin{proof}
Take any succedent formula in $\mathcal D$, its has some label $w' \geq w$ for some $(c : C)^w \in \Delta$, where every new variable name $x \in \dom (w' \setminus w)$ is fresh (in particular, $x \notin i$).
\begin{itemize}
    \item If $i$ is incomparable to $w$, then $i$ is incomparable to $w' \geq w$. Otherwise, there is an maximum element in $\{i, w, w'\}$, and $\leq$ being the prefix relation means they are linearly ordered, so $i$ is comparable to $w$, contradiction.
    \item Otherwise $i > w$, then either $i > w' = w$, or that $i$ is incomparable to $w' > w$ since every $x \in \dom (w' \setminus w)$ has $x \notin i$.
\end{itemize}
Either case, any label $u \geq i$ has $u \not\leq w'$. This means every formula we remove is inaccessible in its own sequent. The conclusion follows by induction on the structure of $\mathcal D$.
\end{proof}

In short, this lemma says that inaccessible formulas, as well as their descendants and inferences expanding them \emph{(which are all inaccessible anywhere)}, may be removed from a derivation.

\begin{definition}
\label{def:sequent_calculi_pruned_derivations}
For any $(\vdashf)$- or $(\vdashs)$-derivation $\mathcal D$, we say that $\mathcal D$ is \emph{pruned} iff every sequent in $\mathcal D$ does not contain inaccessible formulas.
\end{definition}

\begin{proposition}[Pruning derivations]
\label{thm:sequent_calculi_pruning}
For any derivation $\mathcal D$ of $(\Sigma; \Gamma \vdashf \Delta)$ or $(\Sigma; \Gamma \vdashs \Delta)$, there is a pruned derivation $\mathcal E$ of $(\Sigma; \Gamma' \vdashf \Delta)$ or $(\Sigma; \Gamma' \vdashs \Delta)$, where $\Gamma'$ is $\Gamma$ with inaccessible formulas removed.
\end{proposition}

\begin{proof}
By induction on the structure of $\mathcal D$, using \cref{thm:sequent_calculi_pruning_step}.
\end{proof}

Every $(\Pi\mathsf{elim})$ inference in a pruned $\mathcal E$ has label $(uv) \leq w$. This is a convenient fact, since $(uv) \leq w$ is a well-typed context whenever $w$ is well-typed. It also means that $\mathcal E$ contains only full sequents if its root sequent is full. Finally, being able to only search among pruned derivations means that labels are no longer a real constraint in proof search using $(\vdashf)$, where there is always a unique succedent formula.\footnote{Their main purpose here is to keep the similarity between $(\vdashf)$ and $(\vdashs)$ so that translations can be simpler.}

\subsection*{The hole rules}

\emph{(This section assumes results in \cref{sec:sequent_calculi_soundness_and_completeness}. Proofs in \cref{sec:sequent_calculi_soundness_and_completeness} do not depend on results in this section.)}

Both $(\vdashf)$ and $(\vdashs)$ include a $(\mathsf{hole})$ rule which allows some sub-derivation to be \emph{delayed}: anywhere across the derivation, we may use it to insert a temporary hole ${?x}$ as the proof term, and later come back to find some term $a$ that can be used to fill ${?x}$. Another use of this rule is to mark sub-derivations as \emph{proof-irrelevant}, which means the rest of the proof does not depend on any particular shape of the term $a$ -- any term that can fill ${?x}$ will suffice. The $(\mathsf{hole})$ rule is not necessary for completeness (\cref{thm:focused_calculus_completeness}), but may provide some flexibility in proof search.

In the absence of dependent types, every inference can be gated behind a $(\mathsf{hole})$. However, with rules like $(\Sigma\mathsf{intro})$ $(\Pi\mathsf{elim})$ where a type on the right branch can contain a term on the left branch, the structure of the right sub-derivation might be affected by the actual term assigned to the left branch. Such a \emph{proof-relevant} situation can require us to fix a particular choice of proof term on the left branch, which corresponds to \emph{not} using a $(\mathsf{hole})$.

It helps to consider the derivation tree to be \emph{partitioned} by $(\mathsf{hole})$ inferences. In fact, soundness (\cref{thm:focused_calculus_soundness,thm:defocused_calculus_soundness}) holds independently for every partition. We can piece two partitions together, given that the hole ${?x}$ is the subject of a \emph{unique} instance of $(\mathsf{hole})$:

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \pgfmathsetmacro{\dent}{0.3}
    \foreach \x/\y in {-3/0, -3/3, 3/0.5, 3/2.5} {
        \draw[thick] (\x, \y - \dent) -- (\x + \dent, \y) -- (\x + 2, \y) -- (\x + 2, \y + 2) -- (\x + \dent, \y + 2) -- (\x, \y + 2 - \dent) -- (\x - \dent, \y + 2) -- (\x - 2, \y + 2) -- (\x - 2, \y) -- (\x - \dent, \y) -- cycle;
    }
    \draw[thick, dotted, -latex] (-3, 3 - \dent) -- (-3, 2);
    \draw[thick, double, -latex] (-0.5, 2.5) -- (0.5, 2.5);
    \draw (-3 - 2, 3 + 1) node[left] {$\mathcal D_1 = {}$};
    \draw (-3 - 2, 0 + 1) node[left] {$\mathcal D_0 = {}$};
    \draw (3 + 2, 2.5 + 1) node[right] {${} = \subst {?x} a {\mathcal D_1}$};
    \draw (3 + 2, 0.5 + 1) node[right] {${} = \subst {?x} a {\mathcal D_0}$};
    \draw (-3, 0 + 2 - \dent) node[below] {$\Sigma; \Gamma \vdashf ({?x} : A)^u$};
    \draw (-3, 3 + 0.1) node[above] {$\Sigma; \Gamma \vdashf (a : A)^u$};
    \draw (3, 0.5 + 2 - \dent) node[below] {$\subst {?x} a \Sigma; \Gamma \vdashf (a : A)^u$};
    \end{tikzpicture}
\caption{Hole filling}
\label{fig:focused_calculus_hole_filling}
\end{figure}

\begin{proposition}[Hole filling]
\label{thm:focused_calculus_hole_filling}
For any $(\vdashf)$-derivation $\mathcal D$, for any $(\mathsf{hole})$ inference $\mathcal I \in \mathcal D$ with premise proof term $a$ and subject hole ${?x}$, if ${?x}$ is not the subject of any other $(\mathsf{hole})$ inference, removing $\mathcal I$ and replacing all ${?x}$ by $a$ in $\mathcal D$ gives a new derivation $\mathcal E$.
\end{proposition}

\begin{proof}
By simple induction on the structure of $\mathcal D$. For $(\mathsf{reduce})$ inferences, use \cref{thm:hole_filling}.
\end{proof}

In order for the derivations to be well-formed, we need one additional condition to rule out any possibility of `circular dependencies' among holes. % The proof of the following lemma assumes soundness (\cref{thm:focused_calculus_soundness}), which nevertheless does not introduce a circular dependency.

\begin{proposition}
\label{thm:focused_calculus_hole_filling_well_formed}
In the previous lemma, if $\mathcal D$ contains only well-formed sequents and every hole in $a$ precedes ${?x}$ in $\Sigma$, the new derivation $\mathcal E$ also contains only well-formed sequents.
\end{proposition}

\begin{proof}
It is implied by well-formedness that $\Sigma; u \vdashd A : \mathcal U_l$ so $\Sigma$ is well-typed. The $(\mathsf{hole})$ inference $\mathcal I \in \mathcal D$ requires that $({?x}, u, A) \in \Sigma$, which means we can write $\Sigma = (\Sigma_0, ({?x}, u, A), \Sigma_1)$ where $u, A$ may only contain holes in $\Sigma_0$.

Using \cref{thm:focused_calculus_soundness} on the sub-derivation of $\mathcal I$ we have $\Sigma; u \vdashd a : A$. Since $u, a, A$ only contain holes in $\Sigma_0$ we have $\Sigma_0; u \vdashd a : A$. Now we can show that every sequent remains well-formed after the substitution $\subst {?x} {a} \cdot$ using \cref{thm:hole_filling}. In particular, $\subst {?x} {a} \Sigma$ is well-typed signature.
\end{proof}

These lemmas provide a way to fill holes \emph{during} proof search, which can be used to construct a \emph{perfect} $(\vdashf)$-derivation. However, the condition that every hole is a subject of a unique $(\mathsf{hole})$ inference can be maintained in tableaux, but not straightforwardly in matrices where a hole may be shared among branches. For the latter, we have to \emph{delay} hole filling in some cases.

\subsection*{Semi-perfect term assignment}

\emph{(This section assumes results in \cref{sec:sequent_calculi_soundness_and_completeness}. Proofs in \cref{sec:sequent_calculi_soundness_and_completeness} do not depend on results in this section.)}

For \emph{delayed} hole filling, the proof search procedure only constructs a \emph{semi-perfect} $(\vdashf)$-derivation. Such a derivation can contain $(\mathsf{hole})$ inferences, but each one comes with a sub-derivation containing a plan to fill in the hole. The problem is that there can be \emph{multiple} plans for a same hole; in this case, we pick the `first' one of them and discard the rest. We also need to exclude the possibility of circular references among plans.

\begin{definition}
\label{def:focused_calculus_hole_sets}
For any derivation $\mathcal D$ of $(\Sigma; \Gamma \vdashf (a : A)^u)$:
\begin{itemize}[noitemsep]
    \item Its set of \emph{input} holes $H_{\mathsf{in}}(\mathcal D)$ contains exactly the holes in $\Gamma, u, A$.
    \item Its set of \emph{output} holes $H_{\mathsf{out}}(\mathcal D)$ contains exactly the holes in $\Gamma, u, a, A$.
    \item Its set of \emph{introduced} holes $H(\mathcal D)$ is defined by
    $$
    H(\mathcal D) \defeq \{{?x} \mid \text{exists $(\mathsf{hole})$ inference } \mathcal I \in \mathcal D \text{ with subject hole } {?x}\}
    $$
\end{itemize}
The definitions are also extended to inferences $\mathcal I \in \mathcal D$, for which we consider the sub-derivation rooted at $\mathcal I$.
\end{definition}

\begin{proposition}
\label{thm:focused_calculus_hole_sets}
For any pruned $(\vdashf)$-derivation $\mathcal D$, $H_{\mathsf{out}}(\mathcal D) \subseteq H_{\mathsf{in}}(\mathcal D) \cup H(\mathcal D)$.
\end{proposition}

\begin{proof}
By induction on the structure of $\mathcal D$. For every branching derivation, the $H_{\mathsf{in}}$ of the left branch is covered by $H_{\mathsf{in}}$ of the whole derivation, and the $H_{\mathsf{in}}$ of the right branch is covered by the $H_{\mathsf{in}}$ of the whole derivation merged with $H_{\mathsf{out}}$ of the left branch.
\end{proof}

\begin{definition}
\label{def:focused_calculus_traversal_ordering}
For any $(\vdashf)$-derivation $\mathcal D$ and inferences $\mathcal I, \mathcal J \in \mathcal D$, write $\mathcal I \prec \mathcal J$ iff either:
\begin{itemize}[noitemsep]
    \item $\mathcal I$ is to the left of $\mathcal J$, i.e. at the first difference in the paths from the root to $\mathcal I$ and $\mathcal J$, $\mathcal I$ is on the left branch and $\mathcal J$ is on the right branch;
    \item $\mathcal I$ is to the above of $\mathcal J$, i.e. $\mathcal I$ is in the sub-derivation of $\mathcal J$.
\end{itemize}
In other words, $(\prec)$ is generated by the post-order traversal on the derivation tree $\mathcal D$. This is a strict linear order.
\end{definition}

\begin{proposition}
\label{thm:focused_calculus_traversal_ordering}
For any pruned $(\vdashf)$-derivation $\mathcal D$ and inference $\mathcal I \in \mathcal D$:
$$
H_{\mathsf{out}}(\mathcal I) \subseteq H_{\mathsf{in}}(\mathcal D) \cup \{{?x} \mid \text{exists $(\mathsf{hole})$ inference $\mathcal J \preceq \mathcal I$ with subject hole } {?x}\}
$$
\end{proposition}

\begin{proof}
Let us abbreviate $\{{?x} \mid \text{exists $(\mathsf{hole})$ inference $\mathcal J \preceq \mathcal I$ with subject hole } {?x}\}$ as $\{\ldots\}$.

Consider going up the path from the root to $\mathcal I$. For every step going to the left branch, $H_{\mathsf{in}}$ does not change. For every step going to the right branch, $H_{\mathsf{in}}$ merges with $H_{\mathsf{out}}$ of the left branch, which amounts to add all holes that is the subject of some $(\mathsf{hole})$ inference in the left branch (by \cref{thm:focused_calculus_hole_sets}). This ultimately gives $H_{\mathsf{in}}(\mathcal I) \subseteq H_{\mathsf{in}}(\mathcal D) \cup \{\ldots\}$.

The conclusion then follows from the fact $H_{\mathsf{out}}(\mathcal I) \subseteq H_{\mathsf{in}}(\mathcal I) \cup H(\mathcal I)$ (by \cref{thm:focused_calculus_hole_sets}) where $H(\mathcal I) \subseteq \{\ldots\}$ (by \cref{def:focused_calculus_hole_sets,def:focused_calculus_traversal_ordering}).
\end{proof}

\begin{definition}[Hole ordering]
\label{def:focused_calculus_hole_ordering}
For any $(\vdashf)$-derivation $\mathcal D$ and ${?x} \in H(\mathcal D)$, the \emph{canonical inference} introducing ${?x}$ is the \emph{first} $(\mathsf{hole})$ inference $\mathcal I \in \mathcal D$ with subject ${?x}$, ordered by $(\prec)$. Then, for any holes ${?x}, {?y} \in H(\mathcal D)$, write ${?x} \prec {?y}$ iff the canonical inference of ${?x}$ precedes that of ${?y}$. This is a strict linear order.
\end{definition}

The holes will be filled in this order, using the proof terms given in their canonical inferences. Proof terms given by non-canonical $(\mathsf{hole})$ inferences, if any, are simply discarded.

\begin{proposition}[Semi-perfect term assignment]
\label{thm:focused_calculus_semi_perfect_term_assignment}
For any semi-perfect pruned derivation $\mathcal D$ of well-formed sequent $(\Sigma; \Gamma \vdashf (c : C)^w)$, we have $\sigma \Sigma; \sigma w \vdashd \sigma c : \sigma C$ where $\sigma$ replaces every ${?x} \in H(\mathcal D)$ by some term containing no holes other than $H_{\mathsf{in}}(\mathcal D)$.
\end{proposition}

\begin{proof}
Let ${?x_0}, \ldots {?x_{n-1}}$ be the elements of $H(\mathcal D)$ ordered by $(\prec)$.

For any ${?x_i}$, write the premise sequent of its canonical inference $\mathcal I \in \mathcal D$ as $(\Sigma; \Gamma_i \vdashf (a_i : A_i)^{u_i})$. The side condition of $\mathcal I$ means we have a record $({?x_i}, u_i, A_i) \in \Sigma$. Let $\mathcal E$ be the sub-derivation of the premise sequent, then all holes in $\Gamma_i, u_i, a_i, A_i$ are elements of $H_{\mathsf{out}}(\mathcal I)$. By \cref{thm:focused_calculus_traversal_ordering} we have $H_{\mathsf{out}}(\mathcal I) \subseteq H_{\mathsf{in}}(\mathcal D) \cup \{{?y} \mid {?y} \prec {?x}\} = H_{\mathsf{in}}(\mathcal D) \cup \{{?x_0}, \ldots, {?x_{i-1}}\}$.

In other words, every hole depends only on $H_{\mathsf{in}}(\mathcal D)$ and preceding holes. This means we can reorder the records for ${?x_i}$ in $\Sigma$ to match the ordering of $(\prec)$, and $\Sigma$ remains well-typed.

Let $\sigma \defeq \subst {?x_0} {a_0} {\ldots \subst {?x_{n-1}} {a_{n-1}} {\cdot}}$ be the desired substitution. The above result implies that $\sigma$ indeed replaces every ${?x} \in H(\mathcal D)$ by some term containing only holes in $H_{\mathsf{in}}(\mathcal D)$.

By \cref{thm:focused_calculus_soundness} we have $\Sigma; w \vdashd c : C$, as well as $\Sigma; u_i \vdashd a_i : A_i$ for every ${?x_i}$. By induction using \cref{thm:hole_filling} we have $\sigma \Sigma; \sigma w \vdashd \sigma c : \sigma C$ as desired.
\end{proof}

In particular, if $\Gamma, w, C$ do not contain holes, $H_{\mathsf{in}}(\mathcal D) = \emptyset$, and the theorem gives some $\sigma$ such that $(\sigma c)$ no longer contain holes. The description looks complicated, but it really boils down to simply filling holes introduced by the $(\mathsf{hole})$ rules, and the complexity stems from the need of an ordering. In an actual implementation, holes are references to a global table, where we do not need such an ordering other than the fact that it exists \emph{(no circular references)}.

\section{Soundness and completeness}
\label{sec:sequent_calculi_soundness_and_completeness}

This section will deal with $(\cong)$ and $(\preceq)$ first, on which both $(\vdashf)$ and $(\vdashs)$ depend.

\begin{proposition}[Soundness of $\cong$ and $\preceq$]
\label{thm:conversion_calculus_soundness}
For any $\delta \envh$ and $a, b \termh$, if $(a \cong_\delta b)$ or $(a \preceq_\delta b)$ is derivable, we have $a \equiv_\delta b$ or $a \leq_\delta b$ respectively.
\end{proposition}

\begin{proof}
By induction on the derivation, using transitivity of $(\equiv)$, $(\leq)$ and congruence rules.
\end{proof}

\begin{proposition}[Completeness of $\cong$ and $\preceq$]
\label{thm:conversion_calculus_completeness}
For any $\delta \envd$ and $a, b \termd$ strongly normalising under $\delta$, if $a \equiv_\delta b$ or $a \leq_\delta b$, then $(a \cong_\delta b)$ or $(a \preceq_\delta b)$ is derivable respectively.
\end{proposition}

\begin{proof}
Here we required that $\delta, a, b$ do not contain holes. Using the $(\mathsf{reduce})$ rules, it suffices to consider $a, b$ in normal forms. By induction using \cref{thm:weak_head_normal_form}, we have either $a = b$ or $a = \mathcal U_i$, $b = \mathcal U_j$ where $i \leq j$. The lemma then follows from induction on $a$, using the other rules.
\end{proof}

\begin{proposition}[Stability of $\cong$ and $\preceq$]
\label{thm:conversion_calculus_stability}
For any $(\cong)$- or $(\preceq)$-derivation $\mathcal D$, for any substitution $\sigma$ for holes, $\sigma \mathcal D$ is a $(\cong)$- or $(\preceq)$-derivation.
\end{proposition}

\begin{proof}
By induction on the structure of $\mathcal D$. For $(\mathsf{reduce})$, use \cref{thm:hole_filling}. For $(\mathsf{split})$, note that rigid heads do not change under $\sigma$.
\end{proof}

\begin{proposition}[Inversion of $\cong$ and $\preceq$]
\label{thm:conversion_calculus_inversion}
For any $(\cong)$- or $(\preceq)$-rule, the conclusion is derivable iff all premises are derivable.
\end{proposition}

\begin{proof}
The $(\leftarrow)$ direction is trivial, so we consider $(\rightarrow)$. For $(\mathsf{reduce})$, apply induction on derivations, using confluence and persistence of rigid heads (\cref{sec:higher_order_unification}). For $(\mathsf{split})$, note that any derivation of the conclusion necessarily applies a $(\mathsf{split})$ after zero or more $(\mathsf{reduce})$, but $(\mathsf{split})$ and $(\mathsf{reduce})$ permute with each other.
\end{proof}

Soundness and completeness of $(\vdashf)$ can be established now. In fact, $(\vdashf)$ is designed to construct all well-typed normal forms without holes.

\begin{proposition}[Soundness of $\vdashf$]
\label{thm:focused_calculus_soundness}
If the well-formed sequent $(\Sigma; \Gamma \vdashf (c : C)^w)$ is derivable, we have $\Sigma; w \vdashd c : C$.
\end{proposition}

\begin{proof}
Let $\mathcal D$ be a derivation of $(\Sigma; \Gamma \vdashf (c : C)^w)$. By \cref{thm:sequent_calculi_pruning}, $\mathcal D$ can be chosen such that every sequent in $\mathcal D$ does not contain inaccessible formulas.

By induction on the structure of $\mathcal D$. We show the most important cases:

\begin{itemize}
    \item For $(\mathsf{hole})$, the condition $({?x}, u, A) \in \Sigma$ already implies that $\Sigma; u \vdashd {?x} : A$.
    
    \item For $(\mathsf{init})$, well-formedness of the conclusion implies $\Sigma; u \vdashd a : A$, then from \cref{thm:conversion_calculus_soundness} we have $A \leq_{uv} B$, then by conversion and weakening we have $\Sigma; uv \vdashd a : B$.

    \item For $(\Pi\mathsf{form})$, well-formedness of the conclusion implies $(\Sigma; \Gamma \vdashf (A : \mathcal U_l)^u)$ is well-formed, so by induction hypothesis on the left branch we have $\Sigma; u \vdashd A : \mathcal U_l$. Now $(\Sigma; \Gamma, (x : A)^{u, (x : A)} \vdashf (B : \mathcal U_l)^{u, (x : A)})$ is well-formed, so by another induction hypothesis $\Sigma; u, (x : A) \vdashd B : \mathcal U_l$. Using rule for $(\vdashd)$ we have $\Sigma; u \vdashd (\ppi x A B) : \mathcal U_l$.

    \item For $(\Pi\mathsf{intro})$, well-formedness of the conclusion implies $\Sigma; u \vdashd A : \mathcal U_i$ and $\Sigma; u, (x : A) \vdashd B : \mathcal U_i$ and so $(\Sigma; \Gamma, (x : A)^{u, (x : A)} \vdashf (b : B)^{u, (x : A)})$ is well-formed. By induction hypothesis $\Sigma; u, (x : A) \vdashd b : B$. Using rule for $(\vdashd)$ we have $\Sigma; u \vdashd (\lam x b) : (\ppi x A B)$.

    \item For $(\Pi\mathsf{elim})$, well-formedness of the conclusion implies $\Sigma; u \vdashd A : \mathcal U_i$, then well-typedness of $(uv) \leq w$ gives $\Sigma; uv \vdashd A : \mathcal U_i$. By induction hypothesis we have $\Sigma; uv \vdashd a : A$. Using rule for $(\vdashd)$ and weakening we have $\Sigma; uv \vdashd \app b a : \llet x a B$, so $(\Sigma; \Gamma, (\app b a : \llet x a B)^{uv} \vdashf (c : C)^w)$ is well-formed. By another induction hypothesis $\Sigma; w \vdashd c : C$.

    \item For $(\mathsf{reduce})$, the well-formedness condition propagates through by subject reduction (\cref{thm:hole_subject_reduction}), and we get the conclusion back by conversion.
\end{itemize}

The remaining cases are similarly straightforward.
\end{proof}

\begin{proposition}[Completeness of $\vdashf$]
\label{thm:focused_calculus_completeness}
If $\epsilon; w \vdashd c : C$ where $c \termd$ is in normal form, any full sequent $(\epsilon; \Gamma \vdashf (c : C)^w)$ is perfectly derivable.
\end{proposition}

\begin{proof}
By induction on the structure of $c$, and in each case, inversion on $(\vdashd)$ in $\epsilon; w \vdashd c : C$. Since $c$ is in normal form, it is in WHNF where all subterms are also in normal forms. We show the most important cases:

\begin{itemize}
    \item If $c = \mathcal U_k$, it must be the case that $\mathcal U_{k + 1} \leq_w C$, which means $C \equiv_w \mathcal U_l$ for some $l > k$, so $C \rightsquigarrow_w^\ast \mathcal U_l$ by confluence (\cref{thm:hole_red_confluence}). Then we can derive $(\epsilon; \Gamma \vdashf (c : C)^w)$ from the rules $(\mathsf{reduce})$ $(\mathsf{univ})$.

    \item If $c = (\ppi x A B)$, it must be that $\mathcal U_k \leq_w C$ and $\epsilon; w \vdashd A : \mathcal U_k$ and $\epsilon; w, (x : A) \vdashd B : \mathcal U_k$ for some $k \in \mathbb N$, so similarly $C \rightsquigarrow^\ast_w \mathcal U_l$ for some $l \geq k$.

    By conversion, $\epsilon; w \vdashd A : \mathcal U_l$ and $\epsilon; w, (x : A) \vdashd B : \mathcal U_l$. Then we can derive $(\epsilon; \Gamma \vdashf (c : C)^w)$ from the two induction hypotheses and the rules $(\mathsf{reduce})$ $(\Pi\mathsf{form})$.

    \item If $c = (\lam x b)$, it must be that $(\ppi x A B) \leq_w C$ for some $A, B \termh$ and $\epsilon; w, (x : A) \vdashd b : B$, so $C \rightsquigarrow^\ast_w (\ppi x {A'} {B'})$ for some $A', B' \termh$ where $A \rightsquigarrow^\ast_w A'$ and $B \rightsquigarrow^\ast_w B'$.

    By conversion, $\epsilon; w, (x : A') \vdashd b : B'$. Then we can derive $(\epsilon; \Gamma \vdashf (c : C)^w)$ from the induction hypothesis and the rules $(\mathsf{reduce})$ $(\Pi\mathsf{form})$.

    \item If $c = \form {[x]} {e_0, \ldots, e_{m-1}}$, by repeated inversions on $(\vdashd)$ in $\epsilon; w \vdashd c : C$ we can obtain $C_0, \ldots, C_m \termh$ where $C_m = C$ and for any $j < m$:

    \begin{itemize}
        \item $\epsilon; w \vdashd \form{[x]}{e_0, \ldots, e_{j-1}} : C_j$;
        \item If $e_j \termo$, $C_j = (\ppi {x_j} {A_j} {B_j})$ where $\epsilon; w \vdashd e_j : A_j$ and $\llet {x_j} {e_j} {B_j} \leq_w C_{j+1}$;
        \item If $e_j = \fst$, $C_j = (\sig {x_j} {A_j} {B_j})$ where $A_j \leq_w C_{j+1}$;
        \item If $e_j = \snd$, $C_j = (\sig {x_j} {A_j} {B_j})$ where $\llet {x_j} {\fst \form{[x]}{e_0, \ldots, e_{j-1}}} B_j \leq_w C_{j+1}$.
    \end{itemize}

    Then we show for any $j \leq m$ and antecedent $\Gamma_j \supseteq \Gamma$ containing $(\form {[x]} {e_0, \ldots, e_{j-1}} : D)^u$ for some $D \leq_w C_j$ and $u \leq w$, such that $\epsilon; u \vdashd \form {[x]} {e_0, \ldots, e_{j-1}} : D$, the sequent $(\epsilon; \Gamma_j \vdashf (c : C)^w)$ is derivable.

    This can be done by induction on $(m - j)$:

    \begin{itemize}
        \item If $j = m$, then $(c : D)^u \in \Gamma_j$ for some $D \leq_w C$ and $u \leq w$. This means we can simply derive $(\epsilon; \Gamma_j \vdashf (c : C)^w)$ using the rule $(\mathsf{init})$ and \cref{thm:conversion_calculus_completeness}.

        \item If $j < m$ and $e_j \termo$, then $(\form {[x]} {e_0, \ldots, e_{j-1}} : D)^u \in \Gamma_j$ for some $D \leq_w (\ppi {x_j} {A_j} {B_j})$ and $u\leq w$. This means $D \rightsquigarrow^\ast_w (\ppi {x_j} {A_j'} {B_j'})$ where $A_j \rightsquigarrow^\ast_w A_j'$ and $B_j \rightsquigarrow^\ast_w B_j'$. We establish the two branches:
        $$
        \begin{aligned}
        & \epsilon; w \vdashd e_j : A_j & \text{(from above)} \\
        & \epsilon; w \vdashd e_j : A_j' & \text{(conversion)} \\
        & \epsilon; \Gamma_j \vdashf (e_j : A_j')^w & \text{(outer IH)} \\[0.3em]
        & \epsilon; w \vdashd \form {[x]} {e_0, \ldots, e_{j-1}} : (\ppi {x_j} {A_j} {B_j}) & \text{(from above)} \\
        & \epsilon; w \vdashd \form {[x]} {e_0, \ldots, e_j} : \llet {x_j} {e_j} B_j & \text{(using rule for $\vdashd$)} \\
        & \epsilon; w \vdashd \form {[x]} {e_0, \ldots, e_j} : \llet {x_j} {e_j} B_j' & \text{(conversion)} \\
        & \llet {x_j} {e_j} B_j' \equiv_w \llet {x_j} {e_j} B_j \leq_w C_{j+1} & \text{(from above)} \\
        & \epsilon; \Gamma_j, (\form {[x]} {e_0, \ldots, e_j} : \llet {x_j} {e_j} B_j')^w \vdashf (c : C)^w & \text{(inner IH)} \\
        \end{aligned}
        $$
        Here since $\Gamma$ is full for $w$, $\Gamma_j \supseteq \Gamma$ is too, so we can apply the outer IH.
        
        Finally, applying $(\mathsf{reduce})$ and $(\Pi\mathsf{elim})$ gives a derivation of $(\epsilon; \Gamma_j \vdashf (c : C)^w)$.

        \item If $j < m$ and $e_j = \fst$ or $e_j = \snd$, then $(\form {[x]} {e_0, \ldots, e_{j-1}} : D)^u \in \Gamma_j$ for some $D \leq_w (\sig {x_j} {A_j} {B_j})$ and $u \leq w$. This means $D \rightsquigarrow^\ast_w (\sig {x_j} {A_j'} {B_j'})$ where $A_j \rightsquigarrow^\ast_w A_j'$ and $B_j \rightsquigarrow^\ast_w B_j'$. We have:
        $$
        \begin{aligned}
        & \epsilon; w \vdashd \form {[x]} {e_0, \ldots, e_{j-1}} : (\sig {x_j} {A_j} {B_j}) & \text{(from above)} \\
        & \epsilon; w \vdashd \form {[x]} {e_0, \ldots, e_{j-1}, \fst} : A_j & \text{(using rule for $\vdashd$)} \\
        & \epsilon; u \vdashd \form {[x]} {e_0, \ldots, e_{j-1}, \fst} : A_j' & \text{(conversion, strengthening)} \\
        & \epsilon; w \vdashd \form {[x]} {e_0, \ldots, e_{j-1}, \snd} : \llet {x_j} {\fst \ldots} {B_j} & \text{(using rule for $\vdashd$)} \\
        & \epsilon; u \vdashd \form {[x]} {e_0, \ldots, e_{j-1}, \snd} : \llet {x_j} {\fst \ldots} {B_j'} & \text{(conversion, strengthening)} \\
        \end{aligned}
        $$
        Here strengthening is possible, as we assumed $\epsilon; u \vdashd \form {[x]} {e_0, \ldots, e_{j-1}} : D$ in the inner induction, and $(\sig {x_j} {A_j'} {B_j'})$ is a reduct of $D$, so all of these terms have free variables declared in $u$.

        Then for $e_j = \fst$:
        $$
        \begin{aligned}
        & A_j' \equiv_w A_j \leq_w C_{j+1} & \text{(from above)} \\
        &
            \begin{aligned}
                \epsilon; \Gamma_j,\, & (\form {[x]} {e_0, \ldots, e_{j-1}, \fst} : A_j')^u, \\
                & (\form {[x]} {e_0, \ldots, e_{j-1}, \snd} : \llet {x_j} {\fst \ldots} {B_j'})^u \vdashf (c : C)^w \\
            \end{aligned} & \text{(inner IH)} \\
        \end{aligned}
        $$
        Then for $e_j = \snd$:
        $$
        \begin{aligned}
        & \llet {x_j} {\fst \ldots} {B_j'} \equiv_w \llet {x_j} {\fst \ldots} {B_j} \leq_w C_{j+1} & \text{(from above)} \\
        &
            \begin{aligned}
                \epsilon; \Gamma_j,\, & (\form {[x]} {e_0, \ldots, e_{j-1}, \fst} : A_j')^u, \\
                & (\form {[x]} {e_0, \ldots, e_{j-1}, \snd} : \llet {x_j} {\fst \ldots} {B_j'})^u \vdashf (c : C)^w \\
            \end{aligned} & \text{(inner IH)} \\
        \end{aligned}
        $$
        Either case, applying $(\mathsf{reduce})$ and $(\Sigma\mathsf{elim})$ gives a derivation of $(\epsilon; \Gamma_j \vdashf (c : C)^w)$.
    \end{itemize}

    Since $\epsilon; w \vdashd x : C_0$, it must be the case that $(x : X) \in w$ or $(x \defeq a : X) \in w$ for some $X \leq_w C_0$. Let $u \leq w$ be the sub-context of entries up to $x$. Since $\Gamma$ is full, this implies $(x : D)^u \in \Gamma$ where $D \equiv_u X \leq_w C_0$ and $u \leq w$. We also have $\epsilon; u \vdashd x : X$ and $X \rightsquigarrow_u^\ast D$, which together imply $\epsilon; u \vdashd x : D$.

    Using the lemma at $j \defeq 0$ and $\Gamma_0 \defeq \Gamma$ then gives $(\epsilon; \Gamma \vdashf (c : C)^w)$.
\end{itemize}

The remaining cases are similar. Note that we assumed $w,c,C$ do not contain holes, so there is no need to consider the case $c = \form{[?x]}{e_0, \ldots, e_{m-1}}$.
\end{proof}

Since the proof does not involve $(\mathsf{hole})$ inferences, it constructs a \emph{perfect} derivation. In fact, the proof also constructs a \emph{pruned} derivation if $\Gamma$ does not contain inaccessible formulas.

The completeness theorem requires that the sequent in question does not contain holes. It is possible to extend completeness to sequents with holes, by adding to \cref{fig:sequent_calculi} \emph{non-invertible} versions of $(\mathsf{split})$ which accepts flexible heads, and allowing holes to be subject to elimination rules. However, for the purpose of this project, such an extension seems to be unnecessary: we expect the `questions' themselves do not contain holes, and our aim is to construct finished `answers' to them.

\newcommand{\focus}{\operatorname{focus}}
\newcommand{\defocus}{\operatorname{defocus}}
\def\proofSkipAmount{\vskip 0.1em}

Now, soundness and completeness of $(\vdashs)$ will be established by back-and-forth translations between $(\vdashf)$ and $(\vdashs)$. The rest of the section defines the following:

\begin{itemize}
    \item The map $(\defocus)$ which translates $(\vdashf)$ into $(\vdashs)$. For this, we only need to add the missing copies of succedent formulas.

    \item The map $(\focus)$ which translates $(\vdashs)$ into $(\vdashf)$. For this, we choose one of the succedents based on the availability of proof terms.
\end{itemize}

\begin{proposition}
\label{thm:defocusing_lemma}
For any derivation $\mathcal D$ of $(\Sigma; \Gamma \vdashs \Delta)$, adding a new entry $(a : A)^u$ to every succedent in $\mathcal D$ gives a derivation $\mathcal E$ of $(\Sigma; \Gamma \vdashs \Delta, (a : A)^u)$.
\end{proposition}

\begin{proof}
By induction on the structure of $\mathcal D$.
\end{proof}

\begin{proposition}[Defocusing]
\label{thm:defocusing}
If $(\Sigma; \Gamma \vdashf \Delta)$ is ((semi-)perfectly) derivable, then $(\Sigma; \Gamma \vdashs \Delta)$ is ((semi-)perfectly) derivable.
\end{proposition}

\begin{proof}
By induction on the structure of some derivation $\mathcal D$ of $(\Sigma; \Gamma \vdashf \Delta)$, we may construct a derivation $(\defocus \mathcal D)$ of $(\Sigma; \Gamma \vdashf \Delta)$:
\begin{itemize}
    \item If the root inference $\mathcal I \in \mathcal D$ is one of the $(\vdashf)$-axioms $(\mathsf{init})$ $(\mathsf{univ})$ $(\unit\mathsf{form})$ $(\unit\mathsf{intro})$, then $(\defocus \mathcal D)$ is the corresponding $(\vdashs)$-axiom.
    
    For example:
    \begin{prooftree}
    \AxiomC{$A \preceq_{uv} B$}
    \RightLabel{$(\mathsf{init})$}\UnaryInfC{$\Sigma; \Gamma \ni (\textcolor{yellow}{a} : A)^u \vdashf (\textcolor{yellow}{a} : B)^{uv}$}
    \end{prooftree}
    becomes
    \begin{prooftree}
    \AxiomC{$A \preceq_{uv} B$}
    \RightLabel{$(\mathsf{init})$}\UnaryInfC{$\Sigma; \Gamma \ni (\textcolor{yellow}{a} : A)^u \vdashs (\textcolor{yellow}{a} : B)^{uv}$}
    \end{prooftree}

    \item If the root inference $\mathcal I \in \mathcal D$ is $(\mathsf{hole})$ $(\Pi\mathsf{intro})$ with sub-derivation $\mathcal D_0$, then $(\defocus \mathcal D)$ is obtained by applying the corresponding $(\vdashs)$-inference to $\mathcal E_0$, where $\mathcal E_0$ is $(\defocus \mathcal D_0)$ with a new entry $(\principal \mathcal I)$ added to every succedent (as per \cref{thm:defocusing_lemma}).
    
    For example:
    \begin{prooftree}
    \AxiomC{$\mathcal D_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma, (\textcolor{green}{x} : A)^{u, (x : A)} \vdashf (\textcolor{green}{b} : B)^{u, (x : A)}$}
    \RightLabel{$(\Pi\mathsf{intro})$}\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{green}{\lam x b} : (\ppi x A B))^u$}
    \end{prooftree}
    becomes
    \begin{prooftree}
    \AxiomC{$\mathcal E_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma, (\textcolor{green}{x} : A)^{u, (x : A)} \vdashs (\textcolor{green}{\lam x b} : (\ppi x A B))^u, (\textcolor{green}{b} : B)^{u, (x : A)}$}
    \RightLabel{$(\Pi\mathsf{intro})$}\UnaryInfC{$\Sigma; \Gamma \vdashs (\textcolor{green}{\lam x b} : (\ppi x A B))^u$}
    \end{prooftree}

    \item If the root inference $\mathcal I \in \mathcal D$ is $(\Pi\mathsf{form})$ $(\Sigma\mathsf{form})$ $(\Sigma\mathsf{intro})$ with left and right sub-derivations $\mathcal D_0$ and $\mathcal D_1$, then $(\defocus \mathcal D)$ is obtained by applying the corresponding $(\vdashs)$-inference to $\mathcal E_0$ and $\mathcal E_1$, where each $\mathcal E_i$ is $(\defocus \mathcal D_i)$ with a new entry $(\principal \mathcal I)$ added to every succedent (as per \cref{thm:defocusing_lemma}).
    
    For example:
    \begin{prooftree}
    \AxiomC{$\mathcal D_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{green}{a} : A)^u$}
    \AxiomC{$\mathcal D_1$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{green}{b} : \llet x a B)^u$}
    \RightLabel{$(\Sigma\mathsf{intro})$}\BinaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{green}{\pair a b} : (\sig x A B))^u$}
    \end{prooftree}
    becomes
    \begin{prooftree}
    \resizebox{0.9\textwidth}{!}{
        \AxiomC{$\mathcal E_0$}
        \noLine\UnaryInfC{$\Sigma; \Gamma \vdashs (\textcolor{green}{\pair a b} : (\sig x A B))^u, (\textcolor{green}{a} : A)^u$}
        \AxiomC{$\mathcal E_1$}
        \noLine\UnaryInfC{$\Sigma; \Gamma \vdashs (\textcolor{green}{\pair a b} : (\sig x A B))^u, (\textcolor{green}{b} : \llet x a B)^u$}
        \RightLabel{$(\Sigma\mathsf{intro})$}\BinaryInfC{$\Sigma; \Gamma \vdashs (\textcolor{green}{\pair a b} : (\sig x A B))^u$}
        \DisplayProof
    }
    \AxiomC{}
    \end{prooftree}

    \item If the root inference $\mathcal I \in \mathcal D$ is $(\Pi\mathsf{elim})$ with left and right sub-derivations $\mathcal D_0$ and $\mathcal D_1$, then $(\defocus \mathcal D)$ is obtained by applying the $(\Pi\mathsf{elim})$ inference to $\mathcal E_0$ and $\mathcal E_1$, where $\mathcal E_0$ is $(\defocus \mathcal D_0)$ with a new entry $(\principal \mathcal I)$ added to every succedent (as per \cref{thm:defocusing_lemma}), and $\mathcal E_1 \defeq (\defocus \mathcal D_1)$.
    
    For example:
    \begin{prooftree}
    \AxiomC{$\mathcal D_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{red}{a} : A)^{uv}$}
    \AxiomC{$\mathcal D_1$}
    \noLine\UnaryInfC{$\Sigma; \Gamma, (\textcolor{red}{\app b a} : \llet x a B)^{uv} \vdashf (c : C)^w$}
    \RightLabel{$(\Pi\mathsf{elim})$}\BinaryInfC{$\Sigma; \Gamma \ni (\textcolor{red}{b} : (\ppi x A B))^u \vdashf (c : C)^w$}
    \end{prooftree}
    becomes
    \begin{prooftree}
    \AxiomC{$\mathcal E_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashs (c : C)^w, (\textcolor{red}{a} : A)^{uv}$}
    \AxiomC{$\mathcal E_1$}
    \noLine\UnaryInfC{$\Sigma; \Gamma, (\textcolor{red}{\app b a} : \llet x a B)^{uv} \vdashs (c : C)^w$}
    \RightLabel{$(\Pi\mathsf{elim})$}\BinaryInfC{$\Sigma; \Gamma \ni (\textcolor{red}{b} : (\ppi x A B))^u \vdashs (c : C)^w$}
    \end{prooftree}

    \item If the root inference $\mathcal I \in \mathcal D$ is $(\Sigma\mathsf{elim})$ $(\mathsf{reduce})$ with sub-derivation $\mathcal D_0$, then $(\defocus \mathcal D)$ is obtained by applying the corresponding $(\vdashs)$-inference to $\mathcal E_0 \defeq (\defocus \mathcal D_0)$.

    For example:
    \begin{prooftree}
    \AxiomC{$\mathcal D_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma, (\textcolor{red}{\fst a} : A)^u, (\textcolor{red}{\snd a} : \llet x {\fst a} B)^u \vdashf (c : C)^w$}
    \RightLabel{$(\Sigma\mathsf{elim})$}\UnaryInfC{$\Sigma; \Gamma \ni (\textcolor{red}{a} : (\sig x A B))^u \vdashf (c : C)^w$}
    \end{prooftree}
    becomes
    \begin{prooftree}
    \AxiomC{$\mathcal E_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma, (\textcolor{red}{\fst a} : A)^u, (\textcolor{red}{\snd a} : \llet x {\fst a} B)^u \vdashs (c : C)^w$}
    \RightLabel{$(\Sigma\mathsf{elim})$}\UnaryInfC{$\Sigma; \Gamma \ni (\textcolor{red}{a} : (\sig x A B))^u \vdashs (c : C)^w$}
    \end{prooftree}
\end{itemize}
Since the translation $(\defocus)$ does not add new $(\mathsf{hole})$ inferences, (semi-)perfect derivations translate to (semi-)perfect derivations.
\end{proof}

\begin{proposition}[Focusing]
\label{thm:focusing}
If $(\Sigma; \Gamma \vdashs \Delta)$ is ((semi-)perfectly) derivable, then there exists $(c : C)^w \in \Delta$ such that $(\Sigma; \Gamma \vdashf (c : C)^w)$ is ((semi-)perfectly) derivable.
\end{proposition}

\begin{proof}
By induction on the structure of some derivation $\mathcal D$ of $(\Sigma; \Gamma \vdashs \Delta)$, we may construct a derivation $(\focus \mathcal D)$ of $(\Sigma; \Gamma \vdashf (c : C)^w)$:
\begin{itemize}
    \item If the root inference $\mathcal I \in \mathcal D$ is one of the $(\vdashs)$-axioms $(\mathsf{init})$ $(\mathsf{univ})$ $(\unit\mathsf{form})$ $(\unit\mathsf{intro})$, then $(\focus \mathcal D)$ is the corresponding $(\vdashf)$-axiom. In this case, the succedent of $(\focus \mathcal D)$ is the principal formula of $\mathcal I$.

    For example:
    \begin{prooftree}
    \AxiomC{$A \preceq_{uv} B$}
    \RightLabel{$(\mathsf{init})$}\UnaryInfC{$\Sigma; \Gamma \ni (\textcolor{yellow}{a} : A)^u \vdashs \Delta \ni (\textcolor{yellow}{a} : B)^{uv}$}
    \end{prooftree}
    becomes
    \begin{prooftree}
    \AxiomC{$A \preceq_{uv} B$}
    \RightLabel{$(\mathsf{init})$}\UnaryInfC{$\Sigma; \Gamma \ni (\textcolor{yellow}{a} : A)^u \vdashf (\textcolor{yellow}{a} : B)^{uv}$}
    \end{prooftree}

    \item If the root inference $\mathcal I \in \mathcal D$ is $(\mathsf{hole})$ $(\Pi\mathsf{intro})$ with sub-derivation $\mathcal D_0$, let $\mathcal E_0 \defeq (\focus \mathcal D_0)$:
    \begin{itemize}
        \item If the constituent succedent formula occurs as the succedent of $\mathcal E_0$, then $(\focus \mathcal D)$ is obtained by applying the corresponding $(\vdashf)$-inference to $\mathcal E_0$. In this case, the succedent of $(\focus \mathcal D)$ is the principal formula of $\mathcal I$.

        \item Otherwise, then $(\focus \mathcal D)$ is $\mathcal E_0$ with entries containing the label $(u, (x : A))$, if any, removed (as per \cref{thm:sequent_calculi_pruning_step}). In this case, the succedent of $(\focus \mathcal D)$ is the same as the that of $\mathcal E_0$.
    \end{itemize}

    For example:
    \begin{prooftree}
    \AxiomC{$\mathcal D_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma, (\textcolor{green}{x} : A)^{u, (x : A)} \vdashs \Delta, (\textcolor{green}{b} : B)^{u, (x : A)}$}
    \RightLabel{$(\Pi\mathsf{intro})$}\UnaryInfC{$\Sigma; \Gamma \vdashs \Delta \ni (\textcolor{green}{\lam x b} : (\ppi x A B))^u$}
    \end{prooftree}
    becomes either:
    \begin{prooftree}
    \AxiomC{$\mathcal E_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma, (\textcolor{green}{x} : A)^{u, (x : A)} \vdashf (\textcolor{green}{b} : B)^{u, (x : A)}$}
    \RightLabel{$(\Pi\mathsf{intro})$}\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{green}{\lam x b} : (\ppi x A B))^u$}
    \end{prooftree}
    or, obtained from $\mathcal E_0$ by removing inaccessible entries:
    \begin{prooftree}
    \AxiomC{$\mathcal E_0'$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashf (c : C)^w$}
    \end{prooftree}
    depending on which formula occurs as the succedent of $\mathcal E_0$.

    \item If the root inference $\mathcal I \in \mathcal D$ is $(\Pi\mathsf{form})$ $(\Sigma\mathsf{form})$ $(\Sigma\mathsf{intro})$ with left and right sub-derivations $\mathcal D_0$ and $\mathcal D_1$, let each $\mathcal E_i \defeq (\focus \mathcal D_i)$.

    \begin{itemize}
        \item If both constituent succedent formulas occur as the succedents of $\mathcal E_0$ and $\mathcal E_1$, then $(\focus \mathcal D)$ is obtained by applying the corresponding $(\vdashf)$-inference to $\mathcal E_0$ and $\mathcal E_1$. In this case, the succedent of $(\focus \mathcal D)$ is the principal formula of $\mathcal I$.

        \item If only the first constituent succedent formula occurs as the succedent of $\mathcal E_0$, then $(\focus \mathcal D)$ is $\mathcal E_1$ with entries containing the label $(u, (x : A))$, if any, removed (as per \cref{thm:sequent_calculi_pruning_step}). In this case, the succedent of $(\focus \mathcal D)$ is the same as that of $\mathcal E_1$.

        \item Otherwise, $(\focus \mathcal D) \defeq \mathcal E_0$. In this case, the succedent of $(\focus \mathcal D)$ is the same as that of $\mathcal E_0$.
    \end{itemize}

    For example:
    \begin{prooftree}
    \AxiomC{$\mathcal D_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashs \Delta, (\textcolor{green}{a} : A)^u$}
    \AxiomC{$\mathcal D_1$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashs \Delta, (\textcolor{green}{b} : \llet x a B)^u$}
    \RightLabel{$(\Sigma\mathsf{intro})$}\BinaryInfC{$\Sigma; \Gamma \vdashs \Delta \ni (\textcolor{green}{\pair a b} : (\sig x A B))^u$}
    \end{prooftree}
    becomes either:
    \begin{prooftree}
    \AxiomC{$\mathcal E_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{green}{a} : A)^u$}
    \AxiomC{$\mathcal E_1$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{green}{b} : \llet x a B)^u$}
    \RightLabel{$(\Sigma\mathsf{intro})$}\BinaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{green}{\pair a b} : (\sig x A B))^u$}
    \end{prooftree}
    or, obtained from $\mathcal E_1$:
    \begin{prooftree}
    \AxiomC{$\mathcal E_1$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashf (c : C)^w$}
    \end{prooftree}
    or, obtained from $\mathcal E_0$:
    \begin{prooftree}
    \AxiomC{$\mathcal E_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashf (c : C)^w$}
    \end{prooftree}
    depending on which formulas occur as the succedents of $\mathcal E_0$ and $\mathcal E_1$.
    
    \item If the root inference $\mathcal I \in \mathcal D$ is $(\Pi\mathsf{elim})$ with left and right sub-derivations $\mathcal D_0$ and $\mathcal D_1$, let each $\mathcal E_i \defeq (\focus \mathcal D_i)$.
    
    \begin{itemize}
        \item If the constituent succedent formula $(a : A)^{uv}$ occurs as the succedent of $\mathcal E_0$, then $(\focus \mathcal D)$ is obtained by applying the $(\Pi\mathsf{elim})$ inference to $\mathcal E_0$ and $\mathcal E_1$. In this case, the succedent of $(\focus \mathcal D)$ is the same as that of $\mathcal E_1$.

        \item Otherwise $(a : A)^{uv}$ does not occur, then $(\focus \mathcal D) \defeq \mathcal E_0$. In this case, the succedent of $(\focus \mathcal D)$ is the same as that of $\mathcal E_0$.
    \end{itemize}

    For example:
    \begin{prooftree}
    \AxiomC{$\mathcal D_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashs \Delta, (\textcolor{red}{a} : A)^{uv}$}
    \AxiomC{$\mathcal D_1$}
    \noLine\UnaryInfC{$\Sigma; \Gamma, (\textcolor{red}{\app b a} : \llet x a B)^{uv} \vdashs \Delta$}
    \RightLabel{$(\Pi\mathsf{elim})$}\BinaryInfC{$\Sigma; \Gamma \ni (\textcolor{red}{b} : (\ppi x A B))^u \vdashs \Delta$}
    \end{prooftree}
    becomes either:
    \begin{prooftree}
    \AxiomC{$\mathcal E_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{red}{a} : A)^{uv}$}
    \AxiomC{$\mathcal E_1$}
    \noLine\UnaryInfC{$\Sigma; \Gamma, (\textcolor{red}{\app b a} : \llet x a B)^{uv} \vdashf (c : C)^w$}
    \RightLabel{$(\Pi\mathsf{elim})$}\BinaryInfC{$\Sigma; \Gamma \ni (\textcolor{red}{b} : (\ppi x A B))^u \vdashf (c : C)^w$}
    \end{prooftree}
    or, obtained from $\mathcal E_0$:
    \begin{prooftree}
    \AxiomC{$\mathcal E_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma \vdashf (c : C)^w$}
    \end{prooftree}
    depending on which formula occurs as the succedent of $\mathcal E_0$.
    
    \item If the root inference $\mathcal I \in \mathcal D$ is $(\Sigma\mathsf{elim})$ $(\mathsf{reduce})$ with sub-derivation $\mathcal D_0$, let $\mathcal E_0 \defeq (\focus \mathcal D_0)$, then $(\focus \mathcal D)$ is obtained by applying the corresponding $(\vdashf)$-inference to $\mathcal E_0$. In this case, the succedent of $\mathcal E$ is the same as that of $\mathcal E_0$.

    For example:
    \begin{prooftree}
    \AxiomC{$\mathcal D_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma, (\textcolor{red}{\fst a} : A)^u, (\textcolor{red}{\snd a} : \llet x {\fst a} B)^u \vdashs \Delta$}
    \RightLabel{$(\Sigma\mathsf{elim})$}\UnaryInfC{$\Sigma; \Gamma \ni (\textcolor{red}{a} : (\sig x A B))^u \vdashs \Delta$}
    \end{prooftree}
    becomes:
    \begin{prooftree}
    \AxiomC{$\mathcal E_0$}
    \noLine\UnaryInfC{$\Sigma; \Gamma, (\textcolor{red}{\fst a} : A)^u, (\textcolor{red}{\snd a} : \llet x {\fst a} B)^u \vdashf (c : C)^w$}
    \RightLabel{$(\Sigma\mathsf{elim})$}\UnaryInfC{$\Sigma; \Gamma \ni (\textcolor{red}{a} : (\sig x A B))^u \vdashf (c : C)^w$}
    \end{prooftree}
\end{itemize}
Since the translation $(\focus)$ does not add new $(\mathsf{hole})$ inferences, (semi-)perfect derivations translate to (semi-)perfect derivations.
\end{proof}

% If $(\focus D_0)$ gives a derivation for $(\Sigma; \Gamma \vdashf (a' : A)^u)$, and $(\focus D_1)$ gives a derivation for $(\Sigma; \Gamma \vdashf (b : \llet x a B)^u)$, then $(\focus D)$ is defined only when the additional condition $a' = a$ holds. In this case, it will be a derivation for $(\Sigma; \Gamma \vdashf (\pair a b : (\sig x A B))^u)$. We may imagine the proof term $a$ as a sort of `link' between the branches.

% \begin{proposition}[Refocusing]
% \label{thm:refocusing}
% For any $(\vdashf)$-derivation $D$, we have $(\focus \defocus D) = D$.
% \end{proposition}

% \begin{proof}
% It can be shown by induction, that for any $D'$ obtained by adding a new entry $A^u$ to every succedent in $(\defocus D)$, we still have $(\focus D') = D$. The original proposition then follows by simple induction on $D$.
% \end{proof}

% This means $(\focus)$ is a left inverse of $(\defocus)$, which is why we defined them like this in the first place. In this way, soundness and completeness for $(\vdashs)$ are consequences of those for $(\vdashf)$, which are relatively less clumsy to show.

\def\proofSkipAmount{\defaultProofSkipAmount}

\begin{proposition}[Soundness of $\vdashs$]
\label{thm:defocused_calculus_soundness}
If the well-formed sequent $(\Sigma; \Gamma \vdashs \Delta)$ is derivable, we have $\Sigma; w \vdashd c : C$ for some $(c : C)^w \in \Delta$.
\end{proposition}

\begin{proof}
Let $\mathcal D$ be a derivation. By \cref{thm:focusing}, $\mathcal E \defeq (\focus \mathcal D)$ is a derivation of $(\Sigma; \Gamma \vdashf (c : C)^w)$ for some $(c : C)^w \in \Delta$. By \cref{thm:focused_calculus_soundness}, we have $\Sigma; w \vdashd c : C$.
\end{proof}

\begin{proposition}[Completeness of $\vdashs$]
\label{thm:defocused_calculus_completeness}
If $\epsilon; w \vdashd c : C$ where $c \termd$ is in normal form, any full sequent $(\epsilon; \Gamma \vdashs (c : C)^w)$ is perfectly derivable.
\end{proposition}

\begin{proof}
By \cref{thm:focused_calculus_completeness}, $(\epsilon; \Gamma \vdashf (c : C)^w)$ is perfectly derivable. Let $\mathcal D$ be a perfect derivation. By \cref{thm:defocusing}, $\mathcal E \defeq (\defocus \mathcal D)$ is a perfect derivation of $(\epsilon; \Gamma \vdashs (c : C)^w)$.
\end{proof}

\section{The cut rule}
\label{sec:sequent_calculi_cut_rule}

Despite being complete, the sequent calculi still have one major problem to be used for anything non-trivial: the ability to introduce definitions and store lemmas to reduce repetition in proof terms. For this, we can introduce one more rule to both $(\vdashf)$ and $(\vdashs)$, traditionally called the \emph{cut rule} in literature:

\begin{prooftree}
\AxiomC{$\Sigma; \Gamma \vdashf (A : \mathcal U_l)^u$}
\AxiomC{$\Sigma; \Gamma \vdashf (a : A)^u$}
\AxiomC{$\Sigma; \Gamma, (a : A)^u \vdashf (c : C)^w$}
\RightLabel{$(\mathsf{cut})$}\TrinaryInfC{$\Sigma; \Gamma \vdashf (c : C)^w$}
\end{prooftree}

However, with the existing setup, this formulation still leads to $a$ being copied in $c$ every time it is used. Does it make more sense if we actually introduce a let-expression, so that we can use the variable name $x$ to reference $a$ in $c$? This would require opening up a new scope $(u, (x \defeq a : A))$ for the new name $x$:

\begin{prooftree}
\AxiomC{$\Sigma; \Gamma \vdashf (A : \mathcal U_l)^u$}
\AxiomC{$\Sigma; \Gamma \vdashf (a : A)^u$}
\AxiomC{$\Sigma; \Gamma, (x : A)^{u, (x \defeq a : A)} \vdashf (c : C)^{\textcolor{red}{?}}$}
\RightLabel{$(\mathsf{cut}?)$}\TrinaryInfC{$\Sigma; \Gamma \vdashf (\llet x {a : A} c : C)^w$}
\end{prooftree}

If the succedents in the last premise still use the same labels as in the conclusion, the new lemma $A$ will be inaccessible in its proof since $(u, (x \defeq a : A)) \not\leq w$, rendering the cut rule useless. So we have to modify the labels in the succedent accordingly, for example replacing all or some prefix $u$ by $(u, (x \defeq a : A))$. This introduces additional complexity, but is also necessary: otherwise, free occurrences of the name $x$ would not make sense.

Alternatively, we may require each application of the cut rule to \emph{target} exactly one of the succedents, but at the cost of reduced proof-sharing opportunities when generalised to $(\vdashs)$:

\begin{prooftree}
\AxiomC{$\Sigma; \Gamma \vdashf (A : \mathcal U_l)^w$}
\AxiomC{$\Sigma; \Gamma \vdashf (a : A)^w$}
\AxiomC{$\Sigma; \Gamma, (x : A)^{w, (x \defeq a : A)} \vdashf (c : C)^{w, (x \defeq a : A)}$}
\RightLabel{$(\mathsf{cut}')$}\TrinaryInfC{$\Sigma; \Gamma \vdashf (\llet x {a : A} c : C)^w$}
\end{prooftree}

We believe that a better implementation strategy is to use $(\mathsf{cut})$ in proof search, so that labelling is simpler. Duplication may be avoided via structural sharing. After the search finishes, it can be automatically determined whether each cut term should be converted into a definition, depending on whether it is used more than once in the final proof.

We may expect that all theorems in \cref{sec:sequent_calculi_properties,sec:sequent_calculi_soundness_and_completeness} still hold for $(\vdashf)$ and $(\vdashs)$ extended with the $(\mathsf{cut})$ rules: definitions for $(\defocus)$ and $(\focus)$ are easily generalised following the pattern, soundness can be shown, completeness is inherited.

\section{Limitations}
\label{sec:sequent_calculi_polarities}

The previous section nevertheless reveals a limitation of our generalisation. It did not occur to the bare-bones type theory defined in \cref{sec:logical_framework,sec:kernel}, since the elimination rules for $\Pi$ and $\Sigma$ are all presented \emph{negatively}. For \emph{positively} presented types, like the sum type $\oplus$:
\begin{prooftree}
\AxiomC{$\Sigma; \Gamma \vdashf (a : A)^u$}
\RightLabel{$(\oplus\mathsf{inl})$}\UnaryInfC{$\Sigma; \Gamma \vdashf (\app {\mathsf{inl}} a : A \oplus B)^u$}
\DisplayProof
\qquad
\AxiomC{$\Sigma; \Gamma \vdashf (b : B)^u$}
\RightLabel{$(\oplus\mathsf{inr})$}\UnaryInfC{$\Sigma; \Gamma \vdashf (\app {\mathsf{inr}} b : A \oplus B)^u$}
\DisplayProof
\AxiomC{}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Sigma; \Gamma, (x : A)^{u, (x : A)} \vdashf (c : C)^{\textcolor{red}{?}}$}
\AxiomC{$\Sigma; \Gamma, (y : B)^{u, (y : B)} \vdashf (d : C)^{\textcolor{red}{?}}$}
\RightLabel{$(\oplus\mathsf{elim}?)$}\BinaryInfC{$\Sigma; \Gamma \ni (a : A \oplus B) \vdashf (\mathsf{match}\ a\ \mathsf{with}\ x. c \mid y.d : C)^w$}
\end{prooftree}
the elimination rule always opens up new scopes.

This does not align well with the labelled sequent calculus of intuitionistic logic\footnote{The Kripke semantics also says $u \Vdash A \lor B$ iff $u \Vdash A$ and $u \Vdash B$.} where we expect the following \emph{negatively} presented rules for the disjunction $(\lor)$:
\begin{prooftree}
\AxiomC{$\Gamma \vdash \Delta, A^u, B^u$}
\RightLabel{$(\lor\mathsf{intro})$}\UnaryInfC{$\Gamma \vdash \Delta \ni (A \lor B)^u$}
\DisplayProof
\qquad
\AxiomC{$\Gamma, A^u \vdash \Delta$}
\AxiomC{$\Gamma, B^u \vdash \Delta$}
\RightLabel{$(\lor\mathsf{elim})$}\BinaryInfC{$\Gamma \ni (A \lor B)^u \vdash \Delta$}
\DisplayProof
\AxiomC{}
\end{prooftree}
note that the elimination rule has the same label $u$ in both premises and the conclusion.

Therefore, a straightforward extension to our generalisation loses some proof-sharing opportunities anyway, compared to the original connection method.

So far, it seems that a proof term assignment scheme that works like $(\lor\mathsf{elim})$ remains elusive. A possible future direction is to introduce holes in antecedents and fill them during the term assignment process, which likely involves essential changes to this chapter. Another possibility is to use the $(\oplus\mathsf{elim}?)$ variant, but this requires figuring out a better interaction with techniques like label unification.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The tableau system}
\label{sec:the_tableau_system}

The focused calculus $(\vdashf)$ is the more intuitive one between the two. The process of constructing a derivation of $(\vdashf)$ bottom-up corresponds to what is done by users in ITP systems. This chapter demonstrates how it may be phrased as a proof system suitable for ATP purposes.

% \section{The tableau representation}
% \label{sec:the_tableau_representation}

% \newcommand{\tab}[2]{\begin{matrix}#1 & #2\end{matrix}}
% \newcommand{\tabb}[3]{\begin{matrix}#1 & \begin{matrix} #2 \\\hline #3 \end{matrix}\end{matrix}}

% \begin{definition}[Tableaux]
% \label{def:tableaux}
% The tableaux $S, T$ are structures generated by:
% $$
% \begin{aligned}
% F &::= \star \mid (a : A)^u_- \mid (a : A)^u_+ \\
% S, T &::= F \mid \tab{F}{T} \mid \tabb{F}{S}{T}
% \end{aligned}
% $$
% where every occurrence of $(a : A)^u_{\pm}$ is called a \emph{signed formula}. The subscript $-$ or $+$ is its \emph{sign}.
% \end{definition}

% The negative and positive signed formulas in a tableau intend to represent antecedent and succedent formulas, respectively.

% \begin{definition}[Paths]
% \label{def:tableau_paths}
% The paths $P, Q$ are ordered lists generated by:
% $$
% P, Q ::= \epsilon \mid P, (a : A)^u_- \mid P, (a : A)^u_+
% $$
% for which we define list concatenations $(PQ)$ as usual. Moreover:
% \begin{itemize}[noitemsep]
%     \item For any path $P$, the sublist $P[-]$ containing negative entries is an antecedent.
%     \item For any path $P$, the sublist $P[+]$ containing the last positive entry is a singleton succedent.
% \end{itemize}
% \end{definition}

% Paths are just $(\vdashf)$-sequents without the base signature $\Sigma$ part \emph{(which is constant in a derivation)}.

% \newcommand{\paths}{\operatorname{paths}}

% \begin{definition}[Paths in tableaux]
% \label{def:tableau_path_set}
% For any tableau $T$, its set of paths $(\paths T)$ is defined recursively by:
% $$
% \begin{aligned}
%     \paths F &\defeq \{F\} \\
%     \paths \left(\tab{F}{T}\right) &\defeq \{FP \mid P \in \paths T\} \\
%     \paths \left(\tabb{F}{S}{T}\right) &\defeq \{FP \mid P \in \paths S \cup \paths T\} \\
% \end{aligned}
% $$
% where the formulas are occurrences.
% \end{definition}

% Paths in a tableau intend to represent leaf sequents in some partial $(\vdashf)$-derivation.

% \begin{definition}[Connections in tableaux]
% \label{def:tableau_connections}
% Two formulas in a tableau with the same proof term and opposite signs, $(a : A)^u_-$ and $(a : B)^v_+$, are a \emph{connection} iff $u \leq v$ and $A \leq_v B$. The symbol $\star$ is a \emph{connection} by itself.
% \end{definition}

% Connections in a tableau intend to represent axioms in some partial $(\vdashf)$-derivation.

% \begin{definition}[Well-formed pairs]
% \label{def:tableau_well_formed_pairs}
% The pair $(\Sigma, T)$ of a signature and a tableau is \emph{well-formed} iff every term in $T$ is well-typed: for any $(a : A)^u_{\pm} \in T$ we have $\Sigma; u \vdashd a : A$.
% \end{definition}

% Unlike for sequents, here we do require positive (succedent) terms to be well-typed.

% \begin{definition}[Tableau expansion]
% \label{def:tableau_expansion}
% The relation $(\rightsquigarrow)$ between well-formed pairs can be one of the following transitions turning $(\Sigma, T)$ to $(\Sigma', T')$:

% \end{definition}

\section{States and transitions}
\label{sec:tableau_states_and_transitions}

In this report, the tableau representation will be presented directly as pruned $(\vdashf)$-derivations (\cref{def:sequent_calculi_pruned_derivations}).

\begin{definition}[Proof states]
\label{def:tableau_states}
A proof state is a pruned $(\vdashf)$-derivation $\mathcal D$ containing only well-formed sequents, with regularity conditions:
\begin{itemize}
    \item Every hole in $\mathcal D$ occurs exactly once as subject of some $(\mathsf{hole})$ axiom.
    \item For every $(\mathsf{hole})$ axiom in $\mathcal D$, let $(\Sigma; \Gamma \vdashf ({?x} : A)^u)$ be its sequent, then every hole in $\Gamma$ precedes ${?x}$ in $\Sigma$.
\end{itemize}
\end{definition}

\begin{definition}[Proof transitions]
\label{def:tableau_transitions}
A proof transition is a pair of a hole ${?x}$ and some parameters, which acts by turning in some proof state $\mathcal D$ a single $(\mathsf{hole})$ axiom
\begin{center}
    \AxiomC{$(\textcolor{gray}{?x}, u, A) \in \Sigma$}
    \RightLabel{$(\mathsf{hole})$}\UnaryInfC{$\Sigma; \Gamma \vdashf (\textcolor{gray}{?x} : A)^u$}
    \UnaryInfC{$\vdots$}
    \DisplayProof
\end{center}
into one of the following:
\begin{itemize}
    \item An $(\mathsf{init})$ axiom, if there is some $(\textcolor{yellow}{b} : B)^v \in \Gamma$ such that $(B \preceq_u A)$ is derivable. Since we assume that $\mathcal D$ is pruned, we already have $v \leq u$.\par
    In this case, we also replace all ${?x}$ by $\textcolor{yellow}{b}$ across $\mathcal D$.
    
    \item A $(\mathsf{univ})$ axiom, if $A = \mathcal U_l$ and there is some $k < l$.\par
    In this case, we replace all ${?x}$ by $\textcolor{blue}{\mathcal U_k}$ across $\mathcal D$.
    
    \item A $(\Pi\mathsf{form})$ inference with both branches closed by fresh holes $\textcolor{blue}{?a}$ and $\textcolor{blue}{?b}$, if $A = \mathcal U_l$.\par
    In this case, we replace all ${?x}$ by $\textcolor{blue}{(\ppi x {?a} {?b})}$ across $\mathcal D$, but the record $({?x}, u, A) \in \Sigma$ is replaced by two new records $(\textcolor{blue}{?a}, u, \mathcal U_l)$ and $(\textcolor{blue}{?b}, (u, x : \textcolor{blue}{?a}), \mathcal U_l)$.
    
    \item A $(\Sigma\mathsf{form})$ inference with both branches closed by fresh holes $\textcolor{blue}{?a}$ and $\textcolor{blue}{?b}$, if $A = \mathcal U_l$.\par
    In this case, we replace all ${?x}$ by $\textcolor{blue}{(\sig x {?a} {?b})}$ across $\mathcal D$, but the record $({?x}, u, A) \in \Sigma$ is replaced by two new records $(\textcolor{blue}{?a}, u, \mathcal U_l)$ and $(\textcolor{blue}{?b}, (u, x : \textcolor{blue}{?a}), \mathcal U_l)$.
    
    \item A $(\top\mathsf{form})$ axiom, if $A = \mathcal U_l$.\par
    In this case, we replace all ${?x}$ by $\textcolor{blue}{\top}$ across $\mathcal D$.
    
    \item A $(\Pi\mathsf{intro})$ inference with its branch closed by a fresh hole $\textcolor{green}{?b}$, if $A = (\ppi x {A'} {B'})$.\par
    In this case, we replace all ${?x}$ by $\textcolor{green}{(\lam x {?b})}$ across $\mathcal D$, but the record $({?x}, u, A) \in \Sigma$ is replaced by the new record $(\textcolor{green}{?b}, (u, x : {A'}), {B'})$.
    
    \item A $(\Sigma\mathsf{intro})$ inference with both branches closed by fresh holes $\textcolor{green}{?a}$ and $\textcolor{green}{?b}$, if $A = (\sig x {A'} {B'})$.\par
    In this case, we replace all ${?x}$ by $\textcolor{green}{\pair {?a} {?b}}$ across $\mathcal D$, but the record $({?x}, u, A) \in \Sigma$ is replaced by two new records $(\textcolor{green}{?a}, u, {A'})$ and $(\textcolor{green}{?b}, u, \llet x {\textcolor{green}{?a}} {B'})$.
    
    \item A $(\top\mathsf{intro})$ axiom, if $A = \top$.\par
    In this case, we replace all ${?x}$ by $\textcolor{green}{\sstar}$ across $\mathcal D$.
    
    \item A $(\Pi\mathsf{elim})$ inference with its branches closed by a fresh hole $\textcolor{red}{?a}$ and ${?x}$ respectively, if there exists some $(b : (\ppi x {A'} {B'})) \in \Gamma$.\par
    In this case, a new record $(\textcolor{red}{?a}, u, A')$ is inserted immediately preceding $({?x}, u, A) \in \Sigma$.
    
    \item A $(\Sigma\mathsf{elim})$ inference with its branch closed by ${?x}$, if exists $(b : (\sig x {A'} {B'})) \in \Gamma$.\par
    In this case, no other change is needed.
    
    \item A $(\mathsf{reduce})$ inference with its branch closed by ${?x}$.\par
    In this case, we update the record $({?x}, u, A) \in \Sigma$ by reducing $A$ accordingly.
\end{itemize}
The changes on $\mathcal D$ can be decomposed into the creation of zero or more holes, followed optionally by the filling of one hole. Using similar reasoning as in \cref{thm:focused_calculus_hole_filling,thm:focused_calculus_hole_filling_well_formed} we can check this indeed defines a relation between proof states.
\end{definition}

\begin{definition}[Initial states]
\label{def:tableau_initial_states}
For any well-formed full sequent $(\epsilon; \Gamma \vdashf (c : C)^w)$, its corresponding \emph{initial} proof state is defined as the single $(\mathsf{hole})$ axiom:
\begin{center}
    \AxiomC{$({?x}, w, C) \in \Sigma$}
    \RightLabel{$(\mathsf{hole})$}\UnaryInfC{$\Sigma; \Gamma \vdashf ({?x} : C)^w$}
    \DisplayProof
\end{center}
where $\Sigma \defeq ({?x}, w, C)$ is well-typed signature. This is indeed a proof state.
\end{definition}

\begin{definition}[Accepting states]
\label{def:tableau_accepting_states}
For any proof state $\mathcal D$, it is called an \emph{accepting} state iff it is \emph{perfect} (\cref{thm:classification}). Let $c$ be the proof term in the root sequent of some accepting state $\mathcal D$, then $\mathcal D$ is also called a \emph{$c$-accepting} state.
\end{definition}

The transitions listed above essentially restates all rules of $(\vdashf)$, but the application of each rule is now `motivated' by an existing $(\mathsf{hole})$ axiom. They should cover everything we needed to construct any perfect $(\vdashf)$-derivation for some well-formed sequent.

\begin{proposition}[Soundness and completeness]
\label{thm:tableau_soundness_and_completeness}
For well-formed full sequent $(\epsilon; \Gamma \vdashf (c : C)^w)$:
\begin{itemize}[noitemsep]
    \item If its corresponding initial proof state reaches a $c$-accepting state via a sequence of transitions, we have $\epsilon; w \vdashd c : C$.
    \item If $c \termd$ is in normal form and $\epsilon; w \vdashd c : C$, the initial proof state can reach a $c$-accepting state via a sequence of transitions.
\end{itemize}
\end{proposition}

\begin{proof}
Note that all transitions can only fill holes referenced in $(\mathsf{hole})$ axioms, and do not change the root sequent otherwise.
\begin{itemize}
    \item Therefore, any reachable $c$-accepting state would have a root sequent $(\Sigma; \Gamma \vdashf (c : C)^w)$ for some $\Sigma$. By \cref{thm:focused_calculus_soundness} this means $\Sigma; w \vdashd c : C$. Since there are no $(\mathsf{hole})$ axioms, $c$ cannot contain holes. So $\epsilon; w \vdashd c : C$ as desired.

    \item By \cref{thm:focused_calculus_completeness} we have a perfect derivation $\mathcal D$ for $(\epsilon; \Gamma \vdashf (c : C)^w)$. By \cref{thm:sequent_calculi_pruning} we may assume $\mathcal D$ is pruned. Starting from the initial proof state, we can take transitions until the proof state matches the structure of $\mathcal D$. This leads to a $d$-accepting state with root sequent $(\Sigma; \Gamma \vdashf (d : C)^w)$. Check that the proof term is uniquely determined by the structure of the derivation, so we have $d = c$ as desired. \qedhere
\end{itemize}
\end{proof}

Also, only the proof term in the root sequent and the remaining $(\mathsf{hole})$ axioms in the proof state are relevant to the search, so we do not need to store the whole derivation in an implementation:

\begin{proposition}[Equivalent states]
\label{thm:tableau_states_equivalence}
Two proof states $\mathcal D, \mathcal D'$ may be:
\begin{itemize}
    \item \emph{Equivalent:} for any term $c$ and sequence of transitions $t$, some $c$-accepting state is reachable from $\mathcal D$ via $t$ iff some $c$-accepting state is reachable from $\mathcal D'$ via $t$.

    \item \emph{Equi-constructible:} for any term $c$, some $c$-accepting state is reachable from $\mathcal D$ iff some $c$-accepting state is reachable from $\mathcal D'$.

    \item \emph{Equi-provable:} some accepting state is reachable from $\mathcal D$ iff some accepting state is reachable from $\mathcal D'$.
\end{itemize}
Then, a sufficient condition for equivalence is when $\mathcal D, \mathcal D'$ contain the same proof term in their root sequents, and have the same set of $(\mathsf{hole})$ axioms.
\end{proposition}

\begin{proof}
Note that two proof states satisfying the condition are either both $c$-accepting or both not. Also, they have the same set of outgoing transitions, all of which still lead to pairs of states satisfying the condition. Therefore they have the same set of sequences of transitions leading to $c$-accepting states.
\end{proof}

\begin{proposition}[Invertible transitions]
\label{thm:tableau_transitions_invertibility}
A proof transition from $\mathcal D$ to $\mathcal D'$ can be:
\begin{itemize}
    \item \emph{Invertible:} for any term $c$ and sequence of transitions $t$, if some $c$-accepting state is reachable from $\mathcal D$ via $t$, then some $c$-accepting state is reachable from $\mathcal D'$ via $t$.

    \item \emph{Invertible-for-constructions:} for any term $c$, if some $c$-accepting state is reachable from $\mathcal D$, then some $c$-accepting state is reachable from $\mathcal D'$.

    \item \emph{Invertible-for-proofs:} if some accepting state is reachable from $\mathcal D$, then some accepting state is reachable from $\mathcal D'$.
\end{itemize}
Then, the $(\Sigma\mathsf{elim})$ $(\mathsf{reduce})$ transitions are invertible-for-constructions.
\end{proposition}

\begin{proof}
Write $\mathcal D \leq \mathcal D'$ iff every leaf sequent in $\mathcal D'$ is no weaker than $\mathcal D$ (\cref{def:sequent_calculi_ordering}).

For any $\mathcal D \leq \mathcal D'$, if some $\mathcal E$ is reachable from $\mathcal D$ in one step, then some $\mathcal E' \geq \mathcal E$ is reachable from $\mathcal D'$ in at most one step. Both $(\Sigma\mathsf{elim})$ and $(\mathsf{reduce})$ do not weaken a proof state and are therefore invertible-for-constructions.
\end{proof}

Finally, we can optionally add one more kind of transitions to the system, after which all above discussions still apply:

\begin{definition}[The cut transition]
\label{def:tableau_transitions_with_cut}
\Cref{def:tableau_transitions} can be extended with the following clause:
\begin{itemize}
    \item \ldots A $(\mathsf{cut})$ inference with three branches closed by fresh ${?a}$, fresh ${?b}$ and ${?x}$ respectively.\par
    In this case, two new records $({?a}, u, \mathcal U_l)$ and $({?b}, u, {?a})$ are inserted immediately preceding $({?x}, u, A) \in \Sigma$.
\end{itemize}
\end{definition}

A cut transition can introduce a very large search space for the next few steps, since there is no restriction on what definition or lemma ${?b}$ we can construct with it. Therefore, it is more suitable for humans or machine-learning agents for laying out the high-level structure of a proof, rather than brute-force search procedures.

\section{Guiding search by unification}
\label{sec:tableau_unification_search}

So far, the application of the $(\mathsf{init})$ axiom requires a finished derivation of $(A \preceq_u B)$. This is important if we want to actually fill all holes ${?x}$ by $b$ in the proof state. Otherwise, we might later encounter ill-typed and non-normalising terms, potentially getting into an infinite loop.

\begin{example}
Consider removing the restriction on $(\mathsf{init})$. The cut and $(\Pi\mathsf{form})$ $(\Pi\mathsf{intro})$ $(\Pi\mathsf{elim})$ $(\mathsf{init})$ transitions then allow us to construct the formula
$$(\lam x {\app x x}) : (\ppi x {(\ppi y {?a} {?b})} {?c})$$
in the antecedent, since we are not type-checking whether $x$ can be its own argument. Similarly, $(\Pi\mathsf{elim})$ and then another $(\mathsf{init})$ without strict checking could allow us to apply this formula to itself, giving
$$\app {(\lam x {\app x x})} {(\lam x {\app x x})} : {?c}$$
in the antecedent. Finally, with a type family like $(B : (\ppi x A {\mathcal U_0}))$ we can make the formula
$${?x} : \app B (\app {(\lam x {\app x x})} {(\lam x {\app x x})})$$
and reducing on the type of ${?x}$ enters an infinite loop.
\end{example}

Since $(\preceq)$ is decidable for well-typed terms, the derivation $(A \preceq_u B)$ can be automatic. However, some types can contain holes, which must be filled with particular terms in order to make $(A \preceq_u B)$ derivable. For such holes, we want to see which particular terms are required by the $(\preceq)$-derivation, so that we can make a more informed choice when filling them. Such an optimisation is fundamental to every practical ATP system.

\begin{example}
The addition of natural numbers is commutative:
$$\forall x : \mathbb N.\ \forall y : \mathbb N.\ x + y = y + x$$
Now we need to prove that
$$1 + 2 = 2 + 1$$
Instead of brute-force enumerating through every natural number, putting them into $\forall x$ and $\forall y$, and hoping that the lemma instance will be useful for our proof, a better approach is to look at the pattern
$${?a} + {?b} = {?b} + {?a}$$
and fit it onto the goal
$$1 + 2 = 2 + 1$$
This suggests we need to use ${?a} \defeq 1$ and ${?b} \defeq 2$.
\end{example}

To this end, we should allow the construction of partial $(\preceq)$-derivations, but only actually fill the hole once the derivation is finished.

\begin{definition}[Unification constraints]
\label{def:unification_constraints}
A \emph{unification constraint} is a tuple $(a, b, \delta, t)$ consisting of $a, b \termh$ and $\delta \envh$ and a symbolic constant $t \in \{\cong, \preceq\}$. It is said to be \emph{well-formed} iff $a$ and $b$ are well-typed under some contexts $\Gamma_a$ and $\Gamma_b$, with $(\defn \Gamma_a) = (\defn \Gamma_b) = \delta$.
\end{definition}

\begin{definition}[Hypothetical conversion]
\label{def:conversion_calculus_with_hypotheses}
Given a set of unification constraints $C$, the relations $(C; a \cong_\delta b)$ and $(C; a \preceq_\delta b)$ are defined by the same rules as in \cref{def:conversion_calculus}, plus the hypothesis rules
\begin{center}
\AxiomC{$(a, b, \delta, \cong) \in C$}
\RightLabel{$(\mathsf{hyp})$}\UnaryInfC{$C; a \cong_\delta b$}
\DisplayProof
\qquad
\AxiomC{$(a, b, \delta, \preceq) \in C$}
\RightLabel{$(\mathsf{hyp})$}\UnaryInfC{$C; a \preceq_\delta b$}
\DisplayProof
\end{center}
It is clear that if $C = \emptyset$, these relations are still the same as \cref{def:conversion_calculus}.
\end{definition}

\begin{definition}[Assignments]
\label{def:conversion_assignments}
Given a $(\vdashf)$-derivation $\mathcal D$ and some $(\mathsf{hole})$ axiom $\mathcal I \in \mathcal D$ with sequent $(\Sigma; \Gamma \vdashf ({?x} : A)^u)$, an \emph{assignment} on $\mathcal I$ is a tuple of:
\begin{itemize}[noitemsep]
    \item The succedent hole ${?x}$;
    \item Some antecedent term $b$, for some $(b : B) \in \Gamma$;
    \item A set of well-formed unification constraints $C$ such that $(C; B \preceq_u A)$ is derivable.
\end{itemize}
\end{definition}

\begin{example}
For some $(\mathsf{hole})$ axiom $\mathcal I \in \mathcal D$ with sequent $(\Sigma; \Gamma \vdashf ({?x} : A)^u)$ and some antecedent formula $(b : B) \in \Gamma$, the tuple
$$({?x}, b, \{(B, A, u, \preceq)\})$$
is an assignment on $\mathcal I$. It reads:
$$\text{Substitute } {?x} \defeq b \text{ when all of } \{B \preceq_u A\} \text{ hold.}$$
\end{example}

\begin{definition}[Proof states]
\label{def:tableau_states_with_constraints}
A proof state is a pair $(\mathcal D, \mathcal A)$ of a derivation $\mathcal D$ in \cref{def:tableau_states} plus a set of assignments $\mathcal A$ on $\mathcal D$.
\end{definition}

\begin{definition}[Proof transitions]
\label{def:tableau_transitions_with_constraints}
A transition from a proof state $(\mathcal D, \mathcal A)$ can be one of those in \cref{def:tableau_transitions} applied on some $(\mathsf{hole})$ axiom \emph{without} an existing assignment, except that the $(\mathsf{init})$ transition is now replaced by the following ones:
\begin{itemize}
    \item If there is some $(\mathsf{hole})$ axiom \emph{without} an existing assignment, write its sequent as $(\Sigma; \Gamma \vdashf ({?x} : A)^u)$, then for any $(b : B)^v \in \Gamma$, we can add the new assignment to the proof state:
    $$({?x}, b, \{(B, A, u, \preceq)\})$$

    \item The $(\mathsf{reduce})$ rules can be applied: any unification constraint $(a, b, \delta, t)$ can be replaced by $(a', b', \delta, t)$, if $a \rightsquigarrow_\delta a'$ and $b \rightsquigarrow_\delta b'$.

    \item The $(\mathsf{split})$ rules can be applied: any unification constraint $(a, b, \delta, t)$ can be replaced by $(e_0, e'_0, \delta, \cong), \ldots, (e_{n-1}, e'_{n-1}, \delta, \cong)$, if $a = \form {h} {e_0, \ldots, e_{n-1}}$ and $b = \form {h'} {e'_0, \ldots, e'_{n'-1}}$ where $h = h'$ are rigid and $n = n'$. If $t = (\preceq)$ and $h = \mathcal U_k$ and $h' = \mathcal U_l$ where $k \leq l$, the constraint can be removed.

    \item If there is an assignment whose set of unification constraints is empty, this means $(B \preceq_\delta A)$ is derivable. The corresponding $(\mathsf{hole})$ axiom can now be replaced by an $(\mathsf{init})$ axiom.
\end{itemize}
By \cref{thm:conversion_calculus_stability}, the constraints sets in assignments remain valid under hole-filling transitions. Again we can check this indeed defines a relation between proof states.
\end{definition}

\begin{definition}[Initial states]
\label{def:tableau_initial_states_with_constraints}
Initial states are defined the same as in \cref{def:tableau_initial_states}, where the set of assignments is initially empty.
\end{definition}

\begin{definition}[Accepting states]
\label{def:tableau_accepting_states_with_constraints}
Accepting states are defined the same as in \cref{def:tableau_accepting_states}, where the set of assignments is again empty, as there is no more $(\mathsf{hole})$ axioms left.
\end{definition}

\begin{proposition}[Soundness and completeness]
\label{thm:tableau_soundness_and_completeness_with_constraints}
Soundness and completeness as stated in \cref{thm:tableau_soundness_and_completeness} are preserved in the modified system.
\end{proposition}

\begin{proof}
For soundness, we can translate any sequence of transitions in the modified system back to the original one by removing all $(\mathsf{reduce})$ and $(\mathsf{split})$ transitions. For completeness, the new rules allow us to construct any $(\cong)$- or $(\preceq)$-derivation for the application of an $(\mathsf{init})$ axiom.
\end{proof}

\begin{proposition}[Equivalent states]
\label{thm:tableau_states_equivalence_with_constraints}
A sufficient condition for equivalence between proof states $(\mathcal D, \mathcal A)$ and $(\mathcal D', \mathcal A')$ is when $\mathcal D, \mathcal D'$ satisfy the same condition in \cref{thm:tableau_states_equivalence} and $\mathcal A = \mathcal A'$.
\end{proposition}

\begin{proof}
By similar reasoning as in \cref{thm:tableau_states_equivalence}.
\end{proof}

\begin{proposition}[Invertible transitions]
\label{thm:tableau_transitions_invertibility_with_constraints}
The transitions $(\mathsf{reduce})$ $(\mathsf{split})$ and turning a fulfilled assignment into an $(\mathsf{init})$ axiom are invertible-for-constructions.
\end{proposition}

\begin{proof}
Write $(\mathcal D, \mathcal A) \leq (\mathcal D', \mathcal A')$ iff (i) $\mathcal D \leq \mathcal D'$ where both sides exclude $(\mathsf{hole})$ axioms with an assignment, and (ii) $\mathcal A, \mathcal A'$ contain assignments on the same set of holes where the unification constraints in $\mathcal A$ imply the ones in $\mathcal A'$. Then we check that the three kinds of transitions do not weaken the proof state; this involves using \cref{thm:conversion_calculus_inversion}.
\end{proof}

\begin{proposition}[Unprovable states]
\label{thm:tableau_states_unprovable_with_constraints}
For any proof state $(\mathcal D, \mathcal A)$, if there is some unification constraint $(a, b, \delta, t) \in A$ where $a = \form {h} {e_0, \ldots, e_{n-1}}$ and $b = \form {h'} {e'_0, \ldots, e'_{n'-1}}$ and both $h$ and $h'$ are rigid but:
\begin{itemize}[noitemsep]
    \item Either $h \neq h'$ or $n \neq n'$.
    \item It is not the case that $t = (\preceq)$ and $h = \mathcal U_k$ and $h' = \mathcal U_l$ where $k \leq l$.
\end{itemize}
Then no accepting state is reachable from $(\mathcal D, \mathcal A)$.
\end{proposition}

\begin{proof}
\Cref{def:tableau_transitions_with_constraints} requires that once an assignment is added on a $(\mathsf{hole})$ axiom, the axiom can no longer be replaced, until the derivation for type conversion in the assignment is finished. If this is not possible, the $(\mathsf{hole})$ will remain, making it impossible to reach an accepting state. This is the case when there is a rigid-rigid mismatch, again by \cref{thm:conversion_calculus_inversion}.
\end{proof}

Most of these properties either follow from that of the conversion calculus, or the basic transition system defined in the previous section; there should be nothing unexpected here. However, the last two corollaries are more useful than they initially seem to be. In particular, they would allow a simple brute-force prover to successfully prove $1 + 2 = 2 + 1$.

\begin{example}
Consider a $(\mathsf{hole})$ axiom for $(\Sigma; \Gamma \vdashf ({?x} : C)^w)$, where the antecedent $\Gamma$ is the context
$$
\begin{aligned}
\mathbb N &: \mathcal U_0 \\
1 &: \mathbb N \\
2 &: \mathbb N \\
+ &: \ppi x {\mathbb N} {\ppi y {\mathbb N} {\mathbb N}} \\
= &: \ppi x {\mathbb N} {\ppi y {\mathbb N} {\mathcal U_0}} \\
p &: \ppi x {\mathbb N} {\ppi y {\mathbb N} {\app {\app = {(\app {\app + x} y)}} {(\app {\app + y} x)}}}
\end{aligned}
$$
and the expected type $C$ is
$$
\app {\app = {(\app {\app + 1} 2)}} {(\app {\app + 2} 1)}
$$
A lucky prover might attempt $(\Pi\mathsf{elim})$ on $p$ twice, obtaining the new antecedent entries
$$
\begin{aligned}
\app p {?a} &: \ppi y {\mathbb N} {\app {\app = {(\app {\app + {?a}} y)}} {(\app {\app + y} {?a})}} \\
\app {\app p {?a}} {?b} &: \app {\app = {(\app {\app + {?a}} {?b})}} {(\app {\app + {?b}} {?a})} \\
\end{aligned}
$$
as well as two new $(\mathsf{hole})$ axioms for ${?a}$ and ${?b}$. Instead of filling them first, it then adds an assignment setting ${?x}$ to $(\app {\app p {?a}} {?b})$, generating the unification constraint
$$
(\app {\app = {(\app {\app + {?a}} {?b})}} {(\app {\app + {?b}} {?a})}, \app {\app = {(\app {\app + 1} 2)}} {(\app {\app + 2} 1)}, w, \preceq)
$$
which can be repeatedly $(\mathsf{split})$ without loss of generality, leading to the constraints
$$
\begin{aligned}
({?a}, 1, w, &\cong) \\
({?b}, 2, w, &\cong) \\
({?b}, 2, w, &\cong) \\
({?a}, 1, w, &\cong) \\
\end{aligned}
$$
Now we fill in the holes ${?a}$ and ${?b}$. Note that any choice other than setting ${?a}$ to $1$ and ${?b}$ to $2$ immediately leads to a rigid-rigid mismatch in one of the four constraints, which means they are easily filtered out.

As there is only one choice left for each hole, the choices are invertible, and can be applied without loss of generality. Then, all four constraints are removed by further $(\mathsf{split})$s, leading to the substitution ${?x} \defeq (\app {\app p 1} 2)$. This gives a $(\app {\app p 1} 2)$-accepting state.
\end{example}

Rigid-rigid matching is the easier case in Huet's unification algorithm \cite{huet1975unification}. Flexible-rigid matching is more involved. In our system, unification handling and hole filling are interleaved, which means we already have the necessary machinery to handle flexible-rigid matching in its full generality, which works in the same way as Huet's:

\begin{example}
Consider a $(\mathsf{hole})$ axiom for $(\Sigma; \Gamma \vdashf ({?x} : C)^w)$, where the antecedent $\Gamma$ is the context
$$
\begin{aligned}
A &: \mathcal U_0 \\
f &: \ppi {x_0} A {\ldots \ppi {x_{q-1}} A A} \\
\end{aligned}
$$
and the expected type $C$ is
$$
\ppi {x_0} A {\ldots \ppi {x_{p-1}} A A}
$$
When filling other holes in the same proof state, a unification constraint is generated:
$$
(\app {\app {\app {?x} {a_0}} {\ldots}} {a_{p-1}}, \app {\app {\app f {b_0}} {\ldots}} {b_{q-1}}, w, \cong)
$$
which cannot be $(\mathsf{split})$ further since ${?x}$ is a flexible head. To continue solving this constraint, the prover must fill something into ${?x}$ first. Since the expected type is a function type, the only possible transitions filling ${?x}$ are $(\mathsf{init})$ and $(\Pi\mathsf{intro})$.

As there are $p$ arguments in the expected type $C$, the prover can successively take up to $p$ $(\Pi\mathsf{intro})$ transitions, which substitute ${?x} \defeq (\lam {x_0} {?y})$, ${?y} \defeq (\lam {x_1} {?z})$ etc. Assuming it takes $(p - k)$ transitions, this will create a new $(\mathsf{hole})$ axiom for the last hole ${?w}$ with an extended context $\Gamma'$:
$$
\begin{aligned}
A &: \mathcal U_0 \\
f &: \ppi {x_0} A {\ldots \ppi {x_{q-1}} A A} \\
x_0 &: A \\
& \ldots \\
x_{p-k-1} &: A \\
\end{aligned}
$$
and the expected type
$$
C' \defeq \ppi {x_{p-k}} A {\ldots \ppi {x_{p-1}} A A}
$$
The unification constraint is also updated and reduced in this process:
$$
\begin{aligned}
&& (\app {\app {\app {(\llet {x_0} {a_0} ?y)} {a_1}} {\ldots}} {a_{p-1}}, \app {\app {\app f {b_0}} {\ldots}} {b_{q-1}}, w, &\cong) \\
\rightsquigarrow^\ast && (\app {\app {\app {(\llet {x_1} {a_1} {\llet {x_0} {a_0} ?z})} {a_2}} {\ldots}} {a_{p-1}}, \app {\app {\app f {b_0}} {\ldots}} {b_{q-1}}, w, &\cong) \\
\rightsquigarrow^\ast && \ldots \\
\rightsquigarrow^\ast && (\app {\app {\app {(\llet {x_{p-k-1}} {a_{p-k-1}} {\ldots \llet {x_1} {a_1} {\llet {x_0} {a_0} ?w}})} {a_{p-k}}} {\ldots}} {a_{p-1}}, \app {\app {\app f {b_0}} {\ldots}} {b_{q-1}}, w, &\cong) \\
\end{aligned}
$$

Then, the prover can take a single $(\mathsf{init})$ transition. This requires an antecedent formula with type $C'$, otherwise rigid-rigid matching can fail immediately. The only possibilities are:
\begin{itemize}
    \item If $k \leq q$, it can apply exactly $(q - k)$ $(\Pi\mathsf{elim})$ transitions on $f$, giving the antecedent formula
    $$
    \app {\app {\app f {?y_0}} {\ldots}} {?y_{q-k-1}} : \ppi {x_{q-k}} A {\ldots \ppi {x_{q-1}} A A}
    $$
    which can be used by $(\mathsf{init})$ to fill in ${?w}$.

    \item If $k = 0$, then $C' = A$, which means any $x_i : A$ can be used by $(\mathsf{init})$ to be filled into ${?w}$.
\end{itemize}
The two cases correspond to the $\mathsf{imitation}$ and $\mathsf{projection}$ actions in the example of \cref{sec:higher_order_unification}.
\end{example}

The last case, flexible-flexible matching, is very much under-constrained until at least one side becomes rigid. In a system where every type is inhabited, a unification problem always has a solution if only flexible-flexible pairs remain, therefore Huet's algorithm does not deal with them. In our system, this would correspond to continuing proof search as usual, ignoring flexible-flexible pairs until they become otherwise.

% \section{Exposition of the interface}
% \label{sec:tableau_exposition}

In an ITP like Lean, the user sees a list of remaining goals, which corresponds to the set of remaining $(\mathsf{hole})$ axioms. Every tactic invocation corresponds to a transition. You enter an initial state by writing down a theorem statement and entering tactic mode. Every `no goals' state is an accepting state.

Our tableau system has only $14$ `tactics' (\cref{def:tableau_transitions,def:tableau_transitions_with_constraints}) excluding cuts (\cref{def:tableau_transitions_with_cut}), out of which $5$ are invertible (one of the $\mathsf{elim}$s, two $\mathsf{reduce}$s, two of the other unification constraint handlers) and another $7$ require no additional parameters (the $\mathsf{intro}$s), but is already complete.

% \todo[inline]{Continue here (add implementation details if appropriate)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The connection system}
\label{sec:the_connection_system}

The defocused calculus $(\vdashs)$ enjoys better structural properties, allowing for more flexible proof search, and ultimately leading to a representation known as \emph{matrices} with minimal redundancies. The \emph{method of connections} commonly seen in state-of-the-art ATP systems performs proof search through matrices. In this chapter, we will generalise its main idea to our dependent type theory settings.

\section{States and transitions}
\label{sec:matrix_states_and_transitions}

It worth to emphasise that most rules in the transition system described before are \emph{not} invertible in any sense. In the left branch on the $(\Pi\mathsf{elim})$ rule, we are forced to focus on constructing the argument type $A$ in $(\ppi x A B)$, which may or may not be constructible, \emph{independent} of the provability of the goal $C$ in the conclusion. Therefore, an unfortunate choice of transition can lead us from a provable state to an unprovable one. This necessitates \emph{deep backtracking}, and it is easy to waste compute on searching from one of these unprovable states. On the other hand, $(\vdashs)$ does not suffer from the same issue due to monotonicity (\cref{thm:defocused_calculus_monotonicity}).

However, different branches of a $(\vdashs)$-derivation tend to contain a lot of repetition, especially when the context $\Gamma$ is large. Can we do `structural sharing' in an implementation, while correctly keeping track of the differing parts in each branch? Can we identify reusable parts of proofs across different branches? The matrix representation of $(\vdashs)$-sequents is the key.

\begin{definition}[Matrices]
\label{def:matrices}
The matrices $M, N$ are structures generated by:
$$
\begin{aligned}
F &::= \star \mid (a : A)^u_- \mid (a : A)^u_+ \\
M, N &::= F \mid \mat{M & N} \mid \mat{M \\ N}
\end{aligned}
$$
modulo the equivalence relation $(\sim)$ generated by associativity:
$$
\begin{aligned}
    && \mat{M_0 & \mat{M_1 & M_2}} && \sim &&& \mat{\mat{M_0 & M_1} & M_2} \\
    && \mat{M_0 \\ \mat{M_1 \\ M_2}} && \sim &&& \mat{\mat{M_0 \\ M_1} \\ M_2} \\
\end{aligned}
$$
\end{definition}

Due to associativity, we shall omit nested brackets in matrices whenever possible.

Every occurrence of $(a : A)^u_{\pm}$ is called a \emph{signed formula}. The subscript $-$ or $+$ is its \emph{sign}. They intend to represent antecedent and succedent formulas, respectively.

\begin{definition}[Paths]
\label{def:matrix_paths}
The paths $P, Q$ are lists generated by:
$$
\begin{aligned}
F &::= \star \mid (a : A)^u_- \mid (a : A)^u_+ \\
P, Q &::= \epsilon \mid P, F
\end{aligned}
$$
for which we define list concatenations $(PQ)$ as usual. Moreover:
\begin{itemize}[noitemsep]
    \item For any path $P$, the sublist $P[-]$ containing negative entries is an antecedent.
    \item For any path $P$, the sublist $P[+]$ containing positive entries is a succedent.
\end{itemize}
\end{definition}

Paths are just $(\vdashf)$-sequents without the base signature $\Sigma$ part \emph{(which is constant in a derivation)}.

\newcommand{\paths}{\operatorname{paths}}

\begin{definition}[Paths in matrices]
\label{def:matrix_path_set}
For any matrix $M$, its set of paths $(\paths M)$ is defined recursively by:
$$
\begin{aligned}
    \paths F &\defeq \{F\} \\
    \paths \mat{M & N} &\defeq \{PQ \mid P \in (\paths M), Q \in (\paths N)\} \\
    \paths \mat{M \\ N} &\defeq (\paths M) \cup (\paths N) \\
\end{aligned}
$$
where the formulas are occurrences. This definition respects associativity of matrices.
\end{definition}

Paths in a matrix intend to represent leaf sequents in some partial $(\vdashf)$-derivation.

\begin{definition}[Connections in matrices]
\label{def:matrix_connections}
For any matrix $M$, a connection is either:
\begin{itemize}[noitemsep]
    \item Two formulas $(a : A)^u_-, ({?x} : B)^v_+ \in M$ where $u \leq v$ and $A \preceq_v B$.
    \item Two formulas $(a : A)^u_-, (a : B)^v_+ \in M$ where $u \leq v$ and $A \preceq_v B$.
    \item A symbol $\star \in M$.
\end{itemize}
\end{definition}

Connections in a matrix intend to represent $(\vdashs)$-axioms. The binary connection means that we can use the $A$ that we have to fit into the $B$ that we want. Here in intuitionistic systems, connections are asymmetric, so we say e.g. $(a : A)^u_-$ is \emph{connected to} $({?x} : B)^v_+$.

\begin{definition}[Matrix expansions]
\label{def:matrix_expansions}
The matrix expansion relation $(\rightsquigarrow)$ between matrices is inductively defined by the rules listed in \cref{fig:matrix_expansion}.
\end{definition}

\begin{figure}
    \begin{prooftree}
    \AxiomC{$k < l$}
    \RightLabel{$(\mathsf{univ})$}\UnaryInfC{${({?x} : \mathcal U_l)}^u_+ \rightsquigarrow \mat{{(\textcolor{blue}{{?x}} : \mathcal U_l)}^u_+ & \star}$}
    \end{prooftree}
    
    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\Pi\mathsf{form})$}\UnaryInfC{${({?x} : \mathcal U_l)}^u_+ \rightsquigarrow \mat{{(\textcolor{blue}{{?x}} : \mathcal U_l)}^u_+ & \mat{{(\textcolor{blue}{?a} : \mathcal U_l)}^u_+ \\ \mat{(\textcolor{blue}{x} : {?a})^{u, (x : {?a})}_- & {(\textcolor{blue}{?b} : \mathcal U_l)}^{u, (x : {?a})}_+}}}$}
    \end{prooftree}
    
    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\Sigma\mathsf{form})$}\UnaryInfC{${({?x} : \mathcal U_l)}^u_+ \rightsquigarrow \mat{{(\textcolor{blue}{{?x}} : \mathcal U_l)}^u_+ & \mat{{(\textcolor{blue}{?a} : \mathcal U_l)}^u_+ \\ \mat{(\textcolor{blue}{x} : {?a})^{u, (x : {?a})}_- & {(\textcolor{blue}{?b} : \mathcal U_l)}^{u, (x : {?a})}_+}}}$}
    \end{prooftree}
    
    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\unit\mathsf{form})$}\UnaryInfC{${({?x} : \mathcal U_l)}^u_+ \rightsquigarrow \mat{{(\textcolor{blue}{{?x}} : \mathcal U_l)}^u_+ & \star}$}
    \end{prooftree}
    
    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\Pi\mathsf{intro})$}\UnaryInfC{$({?x} : (\ppi x A B))^u_+ \rightsquigarrow \mat{(\textcolor{green}{{?x}} : (\ppi x A B))^u_+ & (\textcolor{green}{x} : A)^{u, (x : A)}_- & (\textcolor{green}{?b} : B)^{u, (x : A)}_+}$}
    \end{prooftree}
    
    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\Sigma\mathsf{intro})$}\UnaryInfC{$({?x} : (\sig x A B))^u_+ \rightsquigarrow \mat{(\textcolor{green}{{?x}} : (\sig x A B))^u_+ & \mat{(\textcolor{green}{?a} : A)^u_+ \\ (\textcolor{green}{?b} : \llet x {?a} B)^u_+}}$}
    \end{prooftree}
    
    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\unit\mathsf{intro})$}\UnaryInfC{$({?x} : \unit)^u_+ \rightsquigarrow \mat{(\textcolor{green}{{?x}} : \unit)^u_+ & \star}$}
    \end{prooftree}
    
    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\Pi\mathsf{elim})$}\UnaryInfC{$(b : (\ppi x A B))^u_- \rightsquigarrow \mat{(\textcolor{red}{b} : (\ppi x A B))^u_- & \mat {(\textcolor{red}{?a} : A)^{uv}_+ \\ (\textcolor{red}{\app b {?a}} : \llet x {?a} B)^{uv}_-}}$}
    \end{prooftree}
    
    \begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\Sigma\mathsf{elim})$}\UnaryInfC{$(a : (\sig x A B))^u_- \rightsquigarrow \mat{(\textcolor{red}{a} : (\sig x A B))^u_- & (\textcolor{red}{\fst a} : A)^u_- & (\textcolor{red}{\snd a} : \llet x {\fst a} B)^u_-}$}
    \end{prooftree}
    
    \begin{prooftree}
    \AxiomC{$A \rightsquigarrow^\ast_u A'$}
    \RightLabel{$(\mathsf{reduce})$}\UnaryInfC{$(a : A)^u_{\pm} \rightsquigarrow (a : A')^u_{\pm}$}
    \end{prooftree}

    \vspace{1em}
    \begin{framed}
        $(\mathsf{cong})$
        
        \begin{prooftree}
        \AxiomC{$M \rightsquigarrow M'$}
        \UnaryInfC{$\mat{M & N} \rightsquigarrow \mat{M' & N}$}
        \DisplayProof
        \qquad
        \AxiomC{$N \rightsquigarrow N'$}
        \UnaryInfC{$\mat{M & N} \rightsquigarrow \mat{M & N'}$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
                
        \begin{prooftree}
        \AxiomC{$M \rightsquigarrow M'$}
        \UnaryInfC{$\mat{M \\ N} \rightsquigarrow \mat{M' \\ N}$}
        \DisplayProof
        \qquad
        \AxiomC{$N \rightsquigarrow N'$}
        \UnaryInfC{$\mat{M \\ N} \rightsquigarrow \mat{M \\ N'}$}
        \DisplayProof
        \AxiomC{}
        \end{prooftree}
    \end{framed}
    
\caption{Matrix expansion rules}
\label{fig:matrix_expansion}
\end{figure}

Matrix expansion rules intend to represent $(\vdashs)$-inferences. For constructor rules, a proof term may or may not be assigned to the subject hole ${?x}$ immediately:
\begin{itemize}
    \item If not, this means the constructor rule is applied on top of a $(\mathsf{hole})$ rule, and we may consider that a \emph{shared lock} is now put on ${?x}$.

    \item Otherwise, the corresponding proof term e.g. $\pair {?a} {?b}$ is substituted for ${?x}$, which we understand as an \emph{exclusive lock} since no further expansions are applicable to this formula.
\end{itemize}

For proof-irrelevant logics, no exclusive locking is necessary, and proof term assignment can be delayed until a matrix proof is finished. However, in dependent type theory, a term might appear relevant in types, potentially requiring a particular and immediate construction. The introduction of locks is our attempt to handle proof-relevance, while preserving the proof-sharing benefits of the connection method in proof-irrelevant situations as much as possible.

Since exclusive locks are already apparent \emph{(if a proof term is not a hole, it is exclusively locked)}, we only need to additionally track shared locks. In the following definition, the set $L$ intends to denote all shared locks:

\begin{definition}[Proof states]
\label{def:matrix_states}
A matrix proof state $\mathcal S$ is a tuple $(\Sigma; L; M; C)$ consisting of:
\begin{itemize}[noitemsep]
    \item A well-typed signature $\Sigma$.
    \item A set $L$ of holes declared in $\Sigma$.
    \item A matrix $M$ where every term is well-typed: for any $(a : A)^u_{\pm} \in M$ we have $\Sigma; u \vdashd a : A$.
    \item A set $C$ of connections in $M$.
\end{itemize}
\end{definition}

\begin{definition}[Proof transitions]
\label{def:matrix_transitions}
The matrix proof transition $(\rightsquigarrow)$ between proof states consists of the pairs $(\Sigma; L; M; C) \rightsquigarrow (\Sigma''; L''; M''; C'')$ where:
\begin{itemize}[noitemsep]
    \item $M \rightsquigarrow M'$ according to \cref{def:matrix_expansions}.
    \item $\Sigma', L', \sigma$ are chosen from \cref{tab:matrix_transitions}, subject to $\sigma = \mathrm{id}$ when ${?x} \in L$, and the holes in $\Sigma'$ are reordered so that $\Sigma'$ is well-typed and $\sigma$ is well-typed under $\Sigma$.
    \item $C'$ is $C$ plus any new occurrences of $(\star)$ in $M$.
    \item $(\Sigma''; L''; M''; C'') \defeq \sigma (\Sigma'; L'; M'; C')$.
\end{itemize}
and the pairs $(\Sigma; L; M; C) \rightsquigarrow (\Sigma''; L''; M''; C'')$, if there is a connection $(a : A)^u_-, ({?x} : B)^v_+$ in $M$:
\begin{itemize}[noitemsep]
    \item $\sigma$ is either $\mathrm{id}$ or $\subst {?x} {a} {}$, subject to $\sigma = \mathrm{id}$ when ${?x} \in L$.
    \item $\Sigma'$ is $\Sigma$ with holes reordered so that $\Sigma'$ is well-typed and $\sigma$ is well-typed under $\Sigma$.
    \item $L' \defeq L \cup \{?x\}$.
    \item $C' \defeq C \cup \{((a : A)^u_-, ({?x} : B)^v_+)\}$.
    \item $(\Sigma''; L''; M''; C'') \defeq \sigma (\Sigma'; L'; M; C')$.
\end{itemize}
We say that a transition is \emph{exclusive} iff it applies a substitution $\sigma \neq \mathsf{id}$, and \emph{shared} otherwise. The transitions are named in the same way as the underlying matrix expansions, e.g. $(\Pi\mathsf{form})$, and the transitions for binary connections are named $(\mathsf{init})$. In this way, every name corresponds to an exclusive and a shared transition, except $(\Pi\mathsf{elim})$ $(\Sigma\mathsf{elim})$ $(\mathsf{reduce})$ which only name a shared transition.
\end{definition}

\begin{table}
    \centering
    \begin{tabular}{lllll}\hline
    $M \rightsquigarrow M'$ & $\Sigma'$ & $L'$ & $\sigma$ \\
    \hline
    $(\mathsf{univ})$
        & $\Sigma$
        & $L \cup \{?x\}$
        & $\mathrm{id}$ or $\subst {?x} {\textcolor{blue}{\mathcal U_k}} {\cdot}$
        \\
    $(\Pi\mathsf{form})$
        & $\Sigma \cup \{(\textcolor{blue}{?a}, u, \mathcal U_l), (\textcolor{blue}{?b}, (u, x : \textcolor{blue}{?a}), \mathcal U_l)\}$
        & $L \cup \{?x\}$
        & $\mathrm{id}$ or $\subst {?x} {\textcolor{blue}{(\ppi x {?a} {?b})}} {\cdot}$
        \\
    $(\Sigma\mathsf{form})$
        & $\Sigma \cup \{(\textcolor{blue}{?a}, u, \mathcal U_l), (\textcolor{blue}{?b}, (u, x : \textcolor{blue}{?a}), \mathcal U_l)\}$
        & $L \cup \{?x\}$
        & $\mathrm{id}$ or $\subst {?x} {\textcolor{blue}{(\sig x {?a} {?b})}} {\cdot}$
        \\
    $(\unit\mathsf{form})$
        & $\Sigma$
        & $L \cup \{?x\}$
        & $\mathrm{id}$ or $\subst {?x} {\textcolor{blue}{\unit}} {\cdot}$
        \\
    $(\Pi\mathsf{intro})$
        & $\Sigma \cup \{(\textcolor{green}{?b}, (u, x : A), B)\}$
        & $L \cup \{?x\}$
        & $\mathrm{id}$ or $\subst {?x} {\textcolor{green}{(\lam x {?b})}} {\cdot}$
        \\
    $(\Sigma\mathsf{intro})$
        & $\Sigma \cup \{(\textcolor{green}{?a}, u, A), (\textcolor{green}{?b}, u, \llet x {\textcolor{green}{?a}} B)\}$
        & $L \cup \{?x\}$
        & $\mathrm{id}$ or $\subst {?x} {\textcolor{green}{\pair {?a} {?b}}} {\cdot}$
        \\
    $(\unit\mathsf{intro})$
        & $\Sigma$
        & $L \cup \{?x\}$
        & $\mathrm{id}$ or $\subst {?x} {\textcolor{green}{\sstar}} {\cdot}$
        \\
    $(\Pi\mathsf{elim})$
        & $\Sigma \cup \{(\textcolor{red}{?a}, uv, A)\}$
        & $L$
        & $\mathrm{id}$
        \\
    $(\Sigma\mathsf{elim})$
        & $\Sigma$
        & $L$
        & $\mathrm{id}$
        \\
    $(\mathsf{reduce})$
        & $\Sigma \setminus \{({?x}, u, A)\} \cup \{({?x}, u, A')\}$
        & $L$
        & $\mathrm{id}$
        \\
    \hline
    \end{tabular}
\caption{Matrix proof transition rules}
\label{tab:matrix_transitions}
\end{table}

Note the similarity between \cref{def:matrix_transitions} and \cref{def:tableau_transitions}. However, while in tableaux we have a fixed ordering of holes in $\Sigma$ which `always works', for matrices the holes may require on-the-fly reordering. This is mainly caused by the $(\Pi\mathsf{elim})$ and $(\mathsf{init})$ rules: in tableaux we maintained the invariant that every hole in an antecedent precedes the hole in the proof term of the succedent, but the same cannot be done for matrices, as we now have multiple succedents.

\begin{example}
Consider beginning with the context $\Gamma$ given by
$$
\begin{aligned}
A &: \mathcal U_0 \\
a &: A \\
f &: \ppi x A A \\
g &: \ppi y A A \\
\end{aligned}
$$
and constructing some hole ${?w}$ of the type $A$. This can be done in at least two ways, $(\app g {(\app f a)})$ and $(\app f {(\app g a)})$. Now let us try to write down this process as a matrix expansion sequence \emph{(labels omitted for brevity)}:
$$
\begin{aligned}
& && \mat{\Gamma^\ast_- & ({?w} : A)^+} \\
& \rightsquigarrow^\ast && \mat{\textcolor{blue}{\Gamma^\ast_-} & \textcolor{blue}{\mat{({?x} : A)_+ \\ (\app f {?x} : A)_-}} & ({?w} : A)^+} \\
& \rightsquigarrow^\ast && \mat{\textcolor{blue}{\Gamma^\ast_-} & \textcolor{blue}{\mat{({?y} : A)_+ \\ (\app g {?y} : A)_-}} & \mat{({?x} : A)_+ \\ (\app f {?x} : A)_-} & ({?w} : A)^+} \\
& \rightsquigarrow^\ast && \ldots \\
\end{aligned}
$$
Then the two constructions require different relative orderings of holes ${?x}$ and ${?y}$:
\begin{itemize}[noitemsep]
    \item For $(\app g {(\app f a)})$, we want to fill ${?y} \defeq \app f {?x}$, which is well-typed only if ${?x}$ precedes ${?y}$ in $\Sigma$.
    \item For $(\app f {(\app g a)})$, we want to fill ${?x} \defeq \app g {?y}$, which is well-typed only if ${?y}$ precedes ${?x}$ in $\Sigma$.
\end{itemize}
\end{example}

Finding a valid hole ordering is necessary: it prevents us to accidentally construct circular substitutions like $\subst {?x} {\app g {(\app f {?x})}} {}$. However, having to decide a particular hole ordering might lead to a significant number of choices in each transition. In an actual implementation, this may be avoided by maintaining a data structure recording hole dependencies during proof search, by which the existence of \emph{some} valid ordering can be ensured. We consider this as an analogue of \emph{occurs check} in first-order unification algorithms \cite{robinson1965machine}.\footnote{The concept of variable ordering was also made explicit in an optimised, linear-time algorithm \cite{martelli1982efficient}.}

\begin{definition}[Initial states]
\label{def:matrix_initial_states}
For any well-formed full sequent $(\epsilon; \Gamma \vdashs (\_ : C)^w)$, its corresponding \emph{initial} proof state is $(\Sigma; \emptyset; M; \emptyset)$ where $\Sigma \defeq ({?x}, w, C)$ and
$$
M \defeq \mat{\Gamma^\ast_- & ({?x} : C)^w_+}
$$
This is indeed a proof state.
\end{definition}

\begin{definition}[Accepting states]
\label{def:matrix_accepting_states}
For any proof state $(\Sigma; L; M; C)$, it is called an \emph{accepting} state iff every path in $M$ covers at least one connection in $C$.
\end{definition}

We can now translate matrix proofs into semi-perfect $(\vdashs)$-derivations, which leads to a soundness property. This translation creates a \emph{symmetric} $(\vdashs)$-derivation similar to the one in \cref{fig:example_symmetric_sequent_calculus}.

\begin{proposition}[Correspondence]
\label{thm:matrix_to_defocused_calculus}
For any well-formed full sequent $(\epsilon; \Gamma \vdashs (\_ : C)^w)$, if its corresponding initial proof state reaches an accepting state via a sequence of transitions, then some $(\Sigma; \Gamma \vdashs (c : C)^w)$ is semi-perfectly derivable.
\end{proposition}

\begin{proof}
By following \cref{def:matrix_transitions} from the initial state, we can construct bottom-up a partial $(\vdashs)$-derivation $\mathcal D$ for every reachable state $(\Sigma; L; M; C)$, such that every leaf sequent in $\mathcal D$ covers a path in $M$, and every hole ${?x}$ is the subject of some $(\mathsf{hole})$ inference in $\mathcal D$ iff ${?x} \in L$.

We show a few cases, in each case $(\Sigma; L; M; C) \rightsquigarrow (\Sigma''; L''; M''; C'')$ using some substitution $\sigma$, and we aim to construct the said partial $(\vdashs)$-derivation $\mathcal D''$ for the new state from $\mathcal D$ for the old state. First, consider cases where $\sigma \neq \mathrm{id}$:
\begin{itemize}
    \item For the $(\mathsf{univ})$ case, $\mathcal D'' \defeq \sigma \mathcal D'$, where $\mathcal D'$ is $\mathcal D$ with its base signature $\Sigma$ replaced by the reordered signature $\Sigma'$. In all following cases, $\mathcal D'$ is defined similarly.

    \item For the $(\Pi\mathsf{form})$ case, $\mathcal D''$ is obtained by applying the $(\Pi\mathsf{form})$ inference to \emph{every} leaf sequent in $\sigma \mathcal D'$ covering a path through the expansion point $({?x} : \mathcal U_l)^u_+ \in M$.

    \item For the $(\Pi\mathsf{elim})$ case, $\mathcal D''$ is obtained by applying the $(\Pi\mathsf{elim})$ inference to \emph{every} leaf sequent in $\sigma \mathcal D'$ covering a path through the expansion point $(b : (\ppi x A B))^u_- \in M$.\footnote{Since the label $(uv)$ in the matrix expansion rule $(\Pi\mathsf{elim})$ can reference any variable in the matrix, and some variables are only declared in a subset of paths, this particular case can create inaccessible entries in some branches of $\mathcal D''$. This is the reason why we have allowed inaccessible entries in the sequent calculi in the first place.}
\end{itemize}
All other cases are similarly straightforward. In every case, $\sigma \neq \mathrm{id}$ requires ${?x} \notin L$, which implies ${?x}$ does not occur as the subject of any $(\mathsf{hole})$ inference in $\mathcal D$, so the well-typed substitution $\sigma$ replacing ${?x}$ does not break the partial derivation $\mathcal D'$.

The cases where $\sigma = \mathrm{id}$ are similar, except that in every case we apply a $(\mathsf{hole})$ inference first, and then apply the corresponding inference on top of its premise. Since we always include a premise, the resulting partial derivation is semi-perfect.

Since the final state is an accepting state, for every leaf sequent in the final $\mathcal D$, there is a connection in its covered path in $M$, so we can close the leaf sequent with an $(\mathsf{init})$ or some other $(\vdashs)$-axiom, optionally layered above a $(\mathsf{hole})$ inference. This gives a semi-perfect derivation.
\end{proof}

\begin{proposition}[Soundness]
\label{thm:matrix_soundness}
For any well-formed full sequent $(\epsilon; \Gamma \vdashs (\_ : C)^w)$, if its corresponding initial proof state can reach an accepting state, we have $\epsilon; w \vdashd c : C$ for some $c \termd$.
\end{proposition}

\begin{proof}
This is a corollary from previous results:
\begin{itemize}
    \item By \cref{thm:matrix_to_defocused_calculus} we have a semi-perfect derivation of $(\Sigma; \Gamma \vdashs (c : C)^w)$ for some signature $\Sigma$ and term $c$.
    \item By \cref{thm:focusing}, this translates to a semi-perfect derivation of $(\Sigma; \Gamma \vdashf (c : C)^w)$.
    \item By \cref{thm:sequent_calculi_pruning}, this reduces to a semi-perfect pruned derivation of $(\Sigma; \Gamma' \vdashf (c : C)^w)$ where $\Gamma' \subseteq \Gamma$.
    \item By \cref{thm:focused_calculus_semi_perfect_term_assignment}, we have $\sigma \Sigma; \sigma w \vdashd \sigma c : \sigma C$ for some substitution $\sigma$, where $(\sigma c)$ contains holes only in $\Gamma, w, C$. However, since $(\epsilon; \Gamma \vdashs (c : C)^w)$ is well-formed, there are no holes in $\Gamma, w, c$, so there are no holes in $(\sigma c)$ as well. This gives $\epsilon; w \vdashd \sigma c : C$ as desired. \qedhere
\end{itemize}
\end{proof}

Translating the other way is also possible. Unlike \cref{thm:matrix_to_defocused_calculus}, the following lemma does not require a symmetric $(\vdashs)$-derivation: in fact, it works for any perfect $(\vdashf)$-derivation.

\begin{proposition}[Simulation]
\label{thm:focused_calculus_to_matrix}
For any well-formed full sequent $(\epsilon; \Gamma \vdashf (c : C)^w)$, if it is perfectly derivable, then its corresponding initial proof state can reach an accepting state via a sequence of transitions.
\end{proposition}

\begin{proof}
The matrix transitions can simulate the tableau transitions (\cref{def:tableau_transitions}) by constructing a matrix $M$, such that every path in $M$ covers a leaf sequent in the tableau state $\mathcal D$\footnote{In particular, it is possible for multiple paths to cover the same leaf sequent.}, and every closed leaf sequent in $\mathcal D$ leads to a connection in every path covering it. Therefore, we can apply similar reasoning as in \cref{thm:tableau_soundness_and_completeness}.
\end{proof}

\begin{proposition}[Completeness]
\label{thm:matrix_completeness}
For any well-formed full sequent $(\epsilon; \Gamma \vdashf (c : C)^w)$, if $c \termd$ is in normal form and $\epsilon; w \vdashd c : C$, then the corresponding initial proof state can reach an accepting state via a sequence of transitions.
\end{proposition}

\begin{proof}
By \cref{thm:focused_calculus_completeness} we have a perfect derivation $\mathcal D$ for $(\epsilon; \Gamma \vdashf (c : C)^w)$. The conclusion then follows from \cref{thm:focused_calculus_to_matrix}.
\end{proof}

Actually, in order to simulate tableau transitions, only exclusive transitions are needed \emph{(except for eliminations and reductions, which are never exclusive)}. So why do we still include the other shared transitions? Our motivation here is to preserve proof-sharing opportunities from the original connection method \cite{wallen1987automated} as much as possible, but exclusive transitions prevent the use of the expanded or connected formula in \emph{another} expansion or connection, which is unseen in the original method.

\begin{proposition}[Invertible transitions]
\label{thm:matrix_transitions_invertibility}
All shared transitions are invertible-for-proofs, whenever they do not expand the shared lock set $L$.
\end{proposition}

\begin{proof}
Write $(\Sigma; L; M; C) \leq (\Sigma'; L'; M'; C')$ iff:
\begin{itemize}[noitemsep]
    \item $\Sigma'$ contains every record in $\Sigma$ up to conversion in type.
    \item $L' \subseteq L$.
    \item Every path in $M'$ covers some path in $M$, up to conversion in types.
    \item $C'$ contains every connection in $C$ up to conversion in types.
\end{itemize}

For any proof states $\mathcal S \leq \mathcal S'$, if some $\mathcal T$ is reachable from $\mathcal S$ in one step, then some $\mathcal T' \geq \mathcal T$ is reachable from $\mathcal S'$ in at most one step. The shared transitions do not weaken a proof state, and are therefore invertible-for-proofs, whenever they do not expand $L$.
\end{proof}

Still, not every transition is invertible; in short, they are invertible iff they do not acquire new exclusive or shared locks. We argue this is analogous to the original matrix method, if by default shared locks are put on formulas deemed to be \emph{proof-irrelevant}:
\begin{itemize}
    \item In the original method, matrix expansions and connections are invertible, but term substitutions \emph{(e.g. as a result of unification)} are not.
    \item In our generalisation, shared expansions on and connections to proof-irrelevant formulas are invertible, but exclusive expansions on and connections to proof-relevant formulas \emph{(e.g. as a result of unification)} are not.
\end{itemize}

Finally, we can optionally add one more kind of transitions to the system, after which all above discussions still apply:

\begin{definition}[The cut transition]
\label{def:matrix_transitions_with_cut}
\Cref{def:matrix_expansions,def:matrix_transitions} can be extended with:
\begin{prooftree}
    \AxiomC{}
    \RightLabel{$(\mathsf{cut})$}\UnaryInfC{$M \rightsquigarrow \mat{M & \mat{({?a} : \mathcal U_l)^u_+ \\ ({?b} : {?a})^u_+ \\ ({?b} : {?a})^u_-}}$}
\end{prooftree}
\begin{center}
    \begin{tabular}{lllll}\hline
    $M \rightsquigarrow M'$ & $\Sigma'$ & $L'$ & $\sigma$ \\
    \hline
    \ldots
        & \ldots
        & \ldots
        & \ldots
        \\
    $(\mathsf{cut})$
        & $\Sigma \cup \{({?a}, u, \mathcal U_l), ({?b}, u, {?a})\}$
        & $L$
        & $\mathrm{id}$
        \\
    \hline
    \end{tabular}
\end{center}
This gives a new shared transition $(\mathsf{cut})$, which is also invertible. 
\end{definition}

\section{Future work}
\label{sec:matrix_unification_search}

The discussion of \cref{sec:tableau_unification_search} can be generalised to matrices quite easily. However, compared to the tableau method, the connection method introduces additional degrees of freedom:
\begin{itemize}[noitemsep]
    \item The hole ordering in $\Sigma$ no longer admit a unique best choice.
    \item The labels $(uv)$ of $(\Pi\mathsf{elim})$ transitions no longer admit a unique best choice.
    \item We need to choose between exclusive and shared transitions.
\end{itemize}
To mitigate some of these, existing research on the connection method like \cite{wallen1987automated} has considered more flexible search strategies: we \emph{first} identify what could be an accepting state, and \emph{then} try to find a sequence of transitions leading to it. This involves the introduction of:
\begin{itemize}
    \item \textbf{Expansion ordering.} Define a partial order on formulas in a matrix $M$ based on variable freshness conditions \emph{(i.e. the implicit requirement that newly introduced variables have new names)}, after which we can identify a possible ordering of expansions given an expanded matrix, thereby finding a sequence of transitions leading to the state.

    \item \textbf{Label unification.} Introduce `label holes' in labels, to be determined later by unifying labels in candidate connections. This requires the extra flexibility provided by expansion-ordering-based matrix proof search.
\end{itemize}
The ideas are not new, but we may face new problems in dependent type theory:
\begin{itemize}
    \item \textbf{Search strategies.} In the original connection method, we may invertibly expand the matrix first, and proceed to non-invertible unification search only afterwards. However, in dependent type theory, this search strategy is not complete:
    \begin{example}
    Given the context
    $$
    \begin{aligned}
    \bot &: (\ppi A {\mathcal U_0} A) \\
    B &: \mathcal U_0 \\
    b &: B \\
    \end{aligned}
    $$
    we want to construct exactly the term
    $$
    \app {\app \bot (\ppi x B B)} b : B
    $$
    but all we can get is the antecedent formula $(\app \bot {?x} : {?x})$ if we do not choose to \emph{(non-invertibly)} fill ${?x} \defeq (\ppi x B B)$, so we never get a chance to apply the $(\Pi\mathsf{elim})$ expansion again to get $(\app {\app \bot {?x}} {?y} : B)$.
    \end{example}
    This problem happens whenever some function has a return type that is partially determined by its previous arguments. Less trivial examples are the eliminators of positively presented types, including the identity type \texttt{Eq}. The discussion is equally applicable to the tableau method. An efficient search strategy that works with such cases still seems elusive.

    \item \textbf{Proof irrelevance detection.} Proof relevance not only force us to use exclusive transitions, but also makes shared transitions non-invertible in general. However, if we can identify `certainly' proof irrelevant holes, shared transitions on them would be invertible. An easy case is when the type theory has a definitional proof-irrelevant universe. Another case is when some hole no longer appears in any type after reductions. More transitions being invertible results in fewer deep backtracking, which could improve the efficiency of proof search.
\end{itemize}

% \begin{example}
% A previous iteration of this report formulated the sequent calculi \emph{without} including variables types in labels, which lead to issues in the matrix representation. Consider beginning with the context $\Gamma$ given by
% $$
% \begin{aligned}
% A &: \mathcal U_0 \\
% B &: \mathcal U_0 \\
% C &: \mathcal U_0 \\
% f &: \ppi x A B \\
% g &: \ppi y B C \\
% \end{aligned}
% $$
% and constructing something of the type $\sig w \unit {(\ppi z A C)}$. This can be done by simply using $\sstar : \top$ to fill in the first element of type $\unit$, and the composition of the functions $f$ and $g$ for the second element of type $(\ppi z A C)$. Now let us try to write down this process as a matrix expansion sequence (labels omitted for brevity):
% $$
% \begin{aligned}
% & && \mat{\Gamma^\ast_- & {\sig w \unit {(\ppi z A C)}}_+} \\
% & \rightsquigarrow && \mat{\Gamma^\ast_- & \textcolor{red}{{\sig w \unit {(\ppi z A C)}}_+} & \textcolor{blue}{\mat{\unit_+ \\ (\ppi z A C)_+}}} \\
% & \rightsquigarrow && \mat{\Gamma^\ast_- & {\sig w \unit {(\ppi z A C)}}_+ & \mat{\unit_+ \\ \textcolor{red}{(\ppi z A C)_+} & \textcolor{blue}{(z : A)_-} & \textcolor{blue}{C_+}}} \\
% & \rightsquigarrow && \mat{\textcolor{red}{\Gamma^\ast_-} & \textcolor{blue}{\mat{A_+ \\ (\app f z : B)_-}} & {\sig w \unit {(\ppi z A C)}}_+ & \mat{\unit_+ \\ {\ppi z A C}_+ & (z : A)_- & C_+}} \\
% & \rightsquigarrow && \ldots \\ 
% \end{aligned}
% $$
% where there is already a problem: in the path
% $$\Gamma^\ast_-, (\app f z : B)_-, (\sig w \unit {(\ppi z A C)})_+, \unit_+$$
% the variable $z$ occurs free in $(\app f z : B)_-$ but its type is not declared anywhere in this path. It is only declared in paths that pass through the lower row in the rightmost column. The problem occurs when we use $(\Pi\mathsf{elim})$ to bring something in the scope $u$ to a narrower scope $(uv)$, where the narrower scope $(uv)$ is only recognised by a subset of paths passing through.

% In the current formulation, by using contexts as labels, the variable $z$ is now declared in the label of the entry $(\app f z : B)_-$, despite not shown here.
% \end{example}

% \section{Elaboration}
% \label{sec:elaboration}

% The process of initialising a valid proof state from some $\mathsf{term}_?$ is called \emph{elaboration}. It serves to establish the \emph{problem} to be later solved via the transition system interface, which then works towards filling in holes.

% Elaboration can be understood as `type checking under the presence of holes', the main difference being the possible side effect of creating constraints:

% \begin{itemize}
%     \item During type checking, two terms $a, b$ containing holes may participate in the conversion rule, where they `should satisfy' the premise $a \equiv_\Delta b$ under some environment $\Delta$. These `should-bes' become unification constraints between terms containing holes.

%     \item During type checking, the unknown subterm at some hole $?x$ `should satisfy' the judgment $\Gamma \vdash {?x} : A$ under some given context $\Gamma$ and with some expected type $A$. These `should-bes' become typing constraints for holes.
% \end{itemize}

% In most ITPs, the user typically do not directly deal with unification constraints, and libraries are structured so that most unification constraints which come up `during normal use' can be automatically solved by heuristics built into the ITP. However, as part of a transition system, complex heuristics makes its behaviour less predictable. Therefore, the Zenith elaborator does not attempt heuristic solutions and instead delegates all unification constraints to the transition system interface.

% \clearpage
% \chapter{Shortcuts}
% \label{sec:shortcuts}

% \clearpage
% \chapter{The discrimination tree}
% \label{sec:index}

% An index for quickly filtering variables with expected return types, to be used in refinement steps. Precise requirements will need clarification (after the transition system interface is completely specified), but overall it works by classifying weakly normalised terms according to their head variables. This likely requires an immutable tree structure for structural sharing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
\label{sec:evaluation}

This chapter will qualitatively evaluate the main components of our work, and describe methods for a quantitative evaluation in the future.

\section{The dependent type theory}
\label{sec:evaluation_type_theory}

The fragment of ECC, $(\vdasho)$, as specified in \cref{sec:logical_framework} is fairly standard, except we did not include the $(\eta)$-conversion rules. Their addition is left as a future improvement.

For the purposes of efficient type checking and proof search, we extended the type theory in \cref{sec:kernel,sec:holes_and_signatures} to $(\vdashd)$ containing let-expressions and holes. This is a combination of ideas in \cite{severi1994pure,mcbride2000dependently,norell2007towards}, but in particular it has allowed sensible reductions on terms containing holes (\cref{thm:hole_filling}), which is a crucial feature for later.

\begin{example}
If we directly add holes on top of $(\vdasho)$ instead of $(\vdashd)$, substitutions for holes no longer permute with term reductions. Consider the term $(\app {(\lam x {\app f {?b}})} a)$ and the substitution $\sigma \defeq \subst {?b} {x} {\cdot}$, we have:
$$
\app {(\lam x {\app f {?b}})} a \rightsquigarrow \subst x a {(\app f {?b})} = \app f {?b}
$$
but applying $\sigma$ to both sides gives:
$$
\begin{aligned}
\sigma(\app {(\lam x {\app f {?b}})} a) &= \app {(\lam x {\app f x})} a \rightsquigarrow \app f a \\
\sigma(\app f {?b}) &= \app f x \\
\end{aligned}
$$
where $\app f a \neq \app f x$. The problem is in the equation $\subst x a {(\app f {?b})} = \app f {?b}$, where we neglected the possibility that $x$ may occur free in a substitution for ${?b}$. Because of this, \cite{mcbride2000dependently} disallowed all term reductions around holes, which is overly restrictive for our purpose.

Instead, we use let-expressions for term reductions, and consider \emph{every} variable as potentially free in any hole. This only restricts $(\zeta)$ reductions. The same example now looks like:
$$
\app {(\lam x {\app f {?b}})} a \rightsquigarrow \llet x a {(\app f {?b})} \equiv \llet x a {(\app f {(\llet x a {?b})})} \rightsquigarrow \app f {(\llet x a {?b})}
$$
where the $\equiv$ in the middle uses the $(\mathsf{attach})$ rule as mentioned in \cref{sec:type_theory_def}, and:
$$
\begin{aligned}
\sigma(\app {(\lam x {\app f {?b}})} a) &= \app {(\lam x {\app f x})} a \rightsquigarrow \app f a \\
\sigma(\app f {(\llet x a {?b})}) &= \app f {(\llet x a x)} \rightsquigarrow \app f a \\
\end{aligned}
$$
\end{example}
Note that in order to move the let-binding $\llet x a {\cdot}$ around the term $(\app f {?b})$, we have to first shadow the variable name $x$ in an inner, duplicate let-binding, so that $x$ becomes bound in the inner let-binding, allowing the outer one to be removed by a $(\zeta)$-reduction. Such a detour is not necessary in the implementation, which suggests a future direction to give a presentation of the type theory that better captures the implementation.

In the respective chapters, we have proved the extensions from $(\vdasho)$ to $(\vdashd)$ without holes, and to $(\vdashd)$ with holes are all conservative, under the names of soundness and completeness. We have shown that all three systems admit subject reduction, and the latter two $(\vdashd)$ both preserve the strong normalisation property of $(\vdash)$.

Finally, we note that the type theory is still bare-bones. For example, it does not contain rules for inductive types. Such extensions are left as future work, though the main ideas described in this report can be equally applicable.

\section{The tableau system}
\label{sec:evaluation_tableau_system}

The tableau method refers to bottom-up proof search using a sequent calculus. We note that this is not new for dependent type theory: it is essentially the same as proof construction by refinements \emph{(hole filling)} in ITP user interfaces, with fully automated procedures proposed in \cite{pym1990proofs,dowek1993complete} and very recently implemented in \cite{norman2025canonical}. Our main contribution here is an independent sequent calculus representation $(\vdashf)$, as well as a transition system presentation in \cref{sec:the_tableau_system}, suitable as a common ground for both brute-force search procedures and reinforcement learning methods.

It worth noting that the original initiative of this project was to identify a simplified but complete subset of ITP interfaces for better interaction by reinforcement learning, and in particular LLM-based provers. The tableau system has already achieved this goal: the transition system it generates looks exactly like \cref{fig:search_tree_improved}, where `make function' refers to the $(\Pi\mathsf{intro})$ transition, `use entry' refers to an assignment transition, and `automatic post-processing' refers to the application of invertible unification transitions. This is a clear improvement over \cref{fig:search_tree_wasted}.

\section{The connection system}
\label{sec:evaluation_connection_system}

The connection method refers to bidirectional proof search in a compact matrix representation, corresponding to a multi-succedent sequent calculus. To our knowledge, this has not been investigated previously for dependent type theory. Our main contribution here is the novel multi-succedent sequent calculus $(\vdashs)$, a term assignment method for proof-irrelevant holes after translating back to $(\vdashf)$, and a transition system presentation in \cref{sec:the_connection_system}.

The connection method has advantages over the tableau method:

\begin{itemize}
    \item \textbf{Proof sharing} across different branches in a sequent calculus derivation. Every matrix transition corresponds to a simultaneous application of some sequent calculus inference rule in \emph{all} applicable branches. We note that the term assignment process requires focusing back to $(\vdashf)$, which duplicates the shared parts anyway, but it is only needed once after a matrix proof has been found.

    \item \textbf{Invertibility} of more transitions. However, due to proof relevance in dependent type theory, sometimes we need to \emph{fill holes} across the matrix, or \emph{acquire a shared lock} preventing such filling, which are both non-invertible. This can be improved by identifying proof-irrelevant holes that never require immediate filling, so transitions acquiring shared locks on them become invertible.\footnote{Applying invertible transitions is also known as \emph{saturation} in ATP literature.}

    \item \textbf{Permutability} of more transitions. We have yet to give a formal account, but even non-invertible transitions permute with each other on many occasions, subject only to expansion ordering constraints. This implies we can organise our search in an ordered way, avoiding unnecessary exploration into different permutation variants of the same search path.\footnote{Such an optimisation leads to the introduction of \emph{multiplicities} in connection method research.}
\end{itemize}

These properties can be exploited for a potentially \emph{exponential} reduction of the memory requirement and the local search space.

\begin{example}
Suppose we have an antecedent $\Gamma$ under which a lemma $A$ is derivable, but finding a proof is hard. Now we need to show the goal $(\sig {x_0} {A_0} {\ldots \sig {x_{n-1}} {A_{n-1}} {\unit}})$, where each $A_i$ requires a proof of the lemma $A$.

\begin{itemize}
    \item Using the tableau method, a search strategy is not immediately motivated to construct a proof of $A$, and is likely to apply $(\Sigma\mathsf{intro})$ transitions first. Then, we have to prove $A_0, \ldots, A_{n-1}$ in $n$ branches. Since all of them depends on a proof of $A$, this proof is duplicated $n$ times.

    \item Using the connection method, we can similarly apply $(\Sigma\mathsf{intro})$ transitions first. The matrix then looks like:
    $$
    \mat{\Gamma_- & \ldots & \mat{({?x_0} : A_0)^u_+ \\ \vdots \\ ({?x_{n-1}} : A_{n-1})^u_+}}
    $$
    Then, in finding a connection to the first conjunct $({?x_0} : A_0)^u_+$, we prove \emph{(i.e. find a connection from formulas in $\Gamma_-$ to)} the lemma $A$ as a side effect:
    $$
    \mat{\Gamma_- & \ldots & ({?a} : A)^u_- & \ldots & \mat{({?x_0} : A_0)^u_+ \\ \vdots \\ ({?x_{n-1}} : A_{n-1})^u_+}}
    $$
    which is equally accessible for reuse in the proofs of the other conjuncts $({?x_i} : A_i)^u_+$.
\end{itemize}

Intuitively, we may understand that the connection method `remembers everything it has done' by not having to completely branch like the tableau method.
\end{example}

\begin{example}
Suppose we have an antecedent $\Gamma$ which include the two functions $f : (\ppi y B A)$ and $g : (\ppi z C A)$, and we need to show the goal $A$. However, a proof of $B$ turns out to be much more difficult than a proof of $C$.

\begin{itemize}
    \item Using the tableau method, a search strategy must be smart enough to avoid the application of $(\Pi\mathsf{elim})$ on $f$, which may or may not be the case. If not, such an application can result in long and fruitless proof search for an argument to $f$, until either succeeding or backtracking.

    \item Using the connection method, we can apply $(\Pi\mathsf{elim})$ on both $f$ and $g$, resulting in a matrix like:
    $$
    \mat{\Gamma_- & \mat{({?y} : B)^u_+ \\ (\app f {?y} : A)^u_-} & \mat{({?z} : C)^u_+ \\ (\app g {?z} : A)^u_-} & ({?x} : A)^u_+}
    $$
    Note that both $(\app f {?y} : A)^u_-$ and $(\app g {?z} : A)^u_-$ can be invertibly connected to $({?x} : A)^u_+$, so it remains to find some connection to either $({?y} : B)^u_+$ or $({?z} : C)^u_+$. In this process, any proved lemma not depending on a particular one of $B, C$ can be shared, like in the previous example. A search procedure is likely to find a proof of $C$ much more easily, which immediately results in a spanning set of connections, and therefore a finished matrix proof. 
\end{itemize}

Intuitively, we may understand that the connection method is `always open to different possibilities', instead of the relatively `focused but stubborn' tableau method.
\end{example}

\begin{example}
Suppose a proof for some goal $A$ requires $n$ premises $A_0, \ldots, A_{n-1}$ in the antecedent $\Gamma$. All of the premises are functions, meaning we need to apply $(\Pi\mathsf{elim})$ on them \emph{(which is quite common in mathematics, where theorems almost always begin with a universal quantification)}.

\begin{itemize}
    \item Using the tableau method, it is questionable to apply all $(\Pi\mathsf{elim})$ transitions upfront, since $(\Pi\mathsf{elim})$ is a branching transition, and their relative ordering can make a difference, leading to a huge search space with at least $O(n!)$ possibilities.
    
    A more sensible search strategy is to delay the application of $(\Pi\mathsf{elim})$ as much as possible, which nevertheless creates additional work in all branches.

    \item Using the connection method, every $(\Pi\mathsf{elim})$ expands to a column matrix with two rows. The resulting matrix would contain $2^n$ paths, meaning that its corresponding $(\vdashs)$-derivation contains $2^n$ branches. But the matrix representation stores them in $O(n)$ space.

    Moreover, an optimised search strategy would stick to some arbitrary but fixed ordering for $(\Pi\mathsf{elim})$ transitions, because different orderings of them do not make an essential difference. This reduces $(n!)$ choices of expansion ordering to just $1$.
\end{itemize}

Intuitively, we may understand that the expansions in different locations across a matrix are largely independent of each other, allowing us to treat different expansion orderings equally. This is much less obvious for the tableau method.
\end{example}

\section{Future work}
\label{sec:evaluation_future_works}

Further optimisations to the the connection system (\cref{sec:matrix_unification_search}), as well as their implementation in Zenith, are still unfinished by now due to project time constraints. After this is done, we plan to experimentally evaluate our work. Although this project concerns only the proof system, we will need to pair it up with a search strategy for evaluation. Possible choices include:

\begin{itemize}
    \item \textbf{Brute-force search strategy.} This involves the enumeration of multiplicities \emph{(the number of times every location in a matrix is expanded)} and the search for a spanning set of connections. We will consider two variants:
    \begin{itemize}
        \item \textbf{The incomplete strategy.} This always expands and connects to the lower rows of column matrices first, so that unification constraints are generated for the upper rows, restricting the search as much as possible. It splits into the two phases of invertible matrix expansions and non-invertible unification search. As mentioned in \cref{sec:matrix_unification_search}, this strategy is incomplete.

        \item \textbf{The complete strategy.} When a type has a flexible head, we enumerate different ways to fill in the hole \emph{(as part of multiplicity enumeration)}, using the non-invertible transitions. This can either lead to an improvement or a performance drop, due to the extra, explosive search space of type enumeration.
    \end{itemize}

    \item \textbf{Reinforcement learning.} This involves fine-tuning a small language model to suggest next transitions based on a text representation of the current proof state.
\end{itemize}

Finally, a variety of existing datasets are available for evaluation:

\begin{itemize}
    \item \textbf{The Natural Number Game}.\footnote{\url{https://github.com/leanprover-community/NNG4/}} This contains problems in dependent type theory that are mostly self-contained, serving as a good starting point. However, few ATPs have been benchmarked there.

    \item \textbf{The TPTP dataset} \cite{sut17tptp}. This contains problems in first-order and higher-order logics, which may be fully embedded in ECC. However, even a shallow embedding introduces significant overhead, so we do not expect our prover to outperform state-of-the-art ATPs and SMT solvers. Instead, our goal is to demonstrate reasonable performance and establish a baseline for future improvements.

    \item \textbf{The miniF2F dataset} \cite{zheng2021minif2f}. This contains problems in dependent type theory, but require many external theorems. Evaluation will be contingent on having a sufficiently developed mathematical library in the prover. If our solutions are successfully implemented on top of Lean, we may conduct evaluations from there.
\end{itemize}

\section{Ethical considerations}
\label{sec:ethical_considerations}

As a project on formal systems, it is unlikely that this project will result in any immediate real-world impact. In particular, it does not involve third-parties, including humans and animals, neither does it collect user data in any form. However, works of this kind might drive the development of artificial intelligence, which has some consequences:

\begin{itemize}
    \item AI systems may gain improved reasoning abilities.

    \item AI systems which formalise their reasoning can become practical. They will not suffer from the problem of hallucination, so long as the correctness of their logical conclusions have been validated independently by a trusted proof checking kernel by the time they reach the user.
\end{itemize}

People working in the fields of mathematics and computer science will likely to be affected first, although it is difficult to say if this will be more positive \emph{(due to e.g. increased productivity)} or negative \emph{(due to e.g. decreased job market demands)} at this point. Nevertheless, the initial motivation for this project is to ultimately reduce the computational budget required in training and running reinforcement learning provers, which could allow more people to benefit from them, while reducing the overall energy cost and environmental impact.

\chapter{Conclusion}
\label{sec:conclusion}

In this project, we started from a fragment of the dependent type theory ECC, extended it with let-expression and holes, and established basic properties including confluence, subject reduction and the preservation of strong normalisation. We gave a characterisation of weak head normal forms in the type theory, which is intimately related to the problem of higher-order unification.

Then, we formulated two labelled sequent calculi for the purpose of efficient proof search, one single-succedent $(\vdashf)$ and one multi-succedent $(\vdashs)$, both including a $(\mathsf{hole})$ inference rule for marking out proof-irrelevant sub-derivations for delayed term assignment. We showed how delayed term assignment works on $(\vdashf)$, if every $(\mathsf{hole})$ inference comes with a sub-derivation that constructs a term to fill the hole. We then showed how to translate between $(\vdashf)$ and $(\vdashs)$, proved their soundness and completeness, and discussed the addition of a cut rule.

We defined a \emph{tableau} proof search method as a transition system that constructs a $(\vdashf)$-derivation step-by-step, whose soundness and completeness follow from that of $(\vdashf)$. Then, we extended the system with the ability to keep track of and simplify unification constraints, allowing for more informed choices when instantiating quantified (proof-relevant) terms.

We also defined a \emph{connection} proof search method as a transitions system that construct a matrix proof step-by-step, where a \emph{matrix} is a compact representation of a $(\vdashs)$-derivation. To maximise the sharing of intermediate proof steps while allowing unification steps, we distinguish between delayed and immediate term assignment. Since both cannot target the same hole, we introduced \emph{locks} on holes. Soundness and completeness of this system follows from that of $(\vdashs)$ and the tableau system. Such a transition system has several efficiency advantages over the tableau system, with more inference steps being shared and more inference rules being invertible.

The original objective of this project was to formulate \emph{some} machine-oriented proof system for a dependent type theory. Despite having met that goal, our investigations initiated by this project are still far from finished:

\begin{itemize}
    \item Our syntax rules of the type theory still suffer from minor issues, which might be improved with a better formulation of substitutions that more closely mirror an NbE type checking algorithm.

    \item We have yet to consider the $(\eta)$-conversion rule, inductive types, etc.

    \item Our connection system has yet to incorporate many common optimisation techniques seen in existing ATP research on connection methods.

    \item We have so far restricted our attention to proof systems, but a good search strategy, either hard-coded heuristic or statistically learned, is equally important.

    \item Our demo implementation, the Zenith theorem prover, only performs type checking by now. Its proof search component is not yet finished.

    \item If possible, we may implement some of our solutions in mainstream ITPs like Lean.
\end{itemize}

All of the above will be considered for future work.

\bibliographystyle{vancouver}
\bibliography{cites}

\end{document}
